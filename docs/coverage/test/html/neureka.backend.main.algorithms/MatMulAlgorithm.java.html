<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>MatMulAlgorithm.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.backend.main.algorithms</a> &gt; <span class="el_source">MatMulAlgorithm.java</span></div><h1>MatMulAlgorithm.java</h1><pre class="source lang-java linenums">package neureka.backend.main.algorithms;

import neureka.Neureka;
import neureka.Tensor;
import neureka.autograd.ADAction;
import neureka.backend.api.AutoDiffMode;
import neureka.backend.api.ExecutionCall;
import neureka.backend.api.Result;
import neureka.backend.api.template.algorithms.AbstractDeviceAlgorithm;
import neureka.backend.api.template.algorithms.AbstractFunDeviceAlgorithm;
import neureka.devices.Device;
import neureka.math.Function;
import neureka.math.args.Arg;
import neureka.ndim.config.NDConfiguration;
import neureka.ndim.config.types.simple.Simple2DConfiguration;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MatMulAlgorithm extends AbstractFunDeviceAlgorithm&lt;MatMulAlgorithm&gt;
{
<span class="fc" id="L21">    private static final Logger _LOG = LoggerFactory.getLogger(MatMulAlgorithm.class);</span>

    public MatMulAlgorithm() {
<span class="fc" id="L24">        super(&quot;simple_matmul&quot;);</span>
<span class="fc" id="L25">        setIsSuitableFor(</span>
<span class="fc" id="L26">                call -&gt; call.validate()</span>
<span class="fc" id="L27">                        .allNotNull( t -&gt; Number.class.isAssignableFrom(t.getItemType()) )</span>
<span class="fc" id="L28">                        .getEstimator()</span>
<span class="fc" id="L29">                        .goodIfAnyNonNull( t -&gt; t.getNDConf() instanceof Simple2DConfiguration)</span>
<span class="fc bfc" id="L30" title="All 2 branches covered.">                        .badIfAnyNonNull( t -&gt; !( t.getNDConf() instanceof Simple2DConfiguration) )</span>
<span class="fc" id="L31">                        .getEstimation()</span>
        );
<span class="fc" id="L33">        setAutogradModeFor( call -&gt; AutoDiffMode.BACKWARD_ONLY );</span>
<span class="fc" id="L34">        setExecution(</span>
            (outerCaller, outerCall) -&gt;
<span class="fc" id="L36">                Result.of(AbstractDeviceAlgorithm.executeDeviceAlgorithm(_prepare(outerCall)))</span>
<span class="fc" id="L37">                .withAutoDiff( (Function f, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; adCall ) -&gt;</span>
                {
<span class="pc bpc" id="L39" title="1 of 2 branches missed.">                    if ( adCall.autogradMode().allowsForward() )</span>
<span class="nc" id="L40">                        throw new IllegalArgumentException(&quot;Matrix multiplication does not support forward-AD!&quot;);</span>
<span class="fc" id="L41">                    Function matMul = Neureka.get().backend().getFunction().matMul();</span>
<span class="fc" id="L42">                    int d = ( 1 + adCall.getValOf( Arg.DerivIdx.class ) ) % 2;</span>
<span class="fc" id="L43">                    Tensor&lt;?&gt; derivative = Util.transpose(adCall.input( d )).deepCopy().mut().setIsIntermediate( true ); // We need to clone it to make it have a simple nd configuration...</span>
<span class="fc" id="L44">                    derivative.to(adCall.getDevice());</span>
<span class="fc" id="L45">                    return ADAction.of(target -&gt; {</span>
                        Tensor&lt;?&gt; result;
<span class="pc bpc" id="L47" title="1 of 3 branches missed.">                        switch ( d ) {</span>
                            case 0:
<span class="fc" id="L49">                                result = matMul.execute(derivative, target.error());</span>
<span class="fc" id="L50">                                break;</span>
                            case 1:
<span class="fc" id="L52">                                result = matMul.execute(target.error(), derivative);</span>
<span class="fc" id="L53">                                break;</span>
                            default:
<span class="nc" id="L55">                                throw new IllegalStateException(&quot;This should never happen!&quot;);</span>
                        }
<span class="fc" id="L57">                        return result;</span>
                    });
                })
        );
<span class="fc" id="L61">        setCallPreparation(MatMulAlgorithm::_prepare);</span>
<span class="fc" id="L62">    }</span>

    private static ExecutionCall&lt;Device&lt;Object&gt;&gt; _prepare( ExecutionCall&lt;?&gt; call )
    {
<span class="pc bpc" id="L66" title="1 of 2 branches missed.">        assert call.arity() &lt;= 3;</span>
<span class="pc bpc" id="L67" title="1 of 2 branches missed.">        if ( call.arity() == 2 ) call = call.withAddedInputAt(0, null);</span>
<span class="pc bpc" id="L68" title="1 of 2 branches missed.">        if ( call.input( 0 ) == null ) // Creating a new tensor:</span>
<span class="fc" id="L69">            call = _withNewOutput( call );</span>

<span class="fc" id="L71">        return (ExecutionCall&lt;Device&lt;Object&gt;&gt;) _autoClone( call );</span>
    }

    private static ExecutionCall&lt;?&gt; _withNewOutput( ExecutionCall&lt;?&gt; call )
    {
<span class="fc" id="L76">        Class&lt;Number&gt; type = (Class&lt;Number&gt;) call.input(  1 ).getDataType().getItemTypeClass();</span>

<span class="fc" id="L78">        int[] shp = new int[]{ call.input( 1 ).shape(0), call.input( 2 ).shape(1) };</span>
<span class="fc" id="L79">        Tensor&lt;Number&gt; output = Tensor.of( type ).withShape( shp ).all( 0 ).mut().setIsIntermediate( true );</span>

<span class="fc" id="L81">        call = _checkAndPrepareLayout( call, output );</span>

<span class="fc" id="L83">        call.getDeviceFor(Number.class).store( output );</span>
<span class="fc" id="L84">        return call.withInputAt( 0, output );</span>
    }

    private static ExecutionCall&lt;?&gt; _checkAndPrepareLayout( ExecutionCall&lt;?&gt; call, Tensor&lt;?&gt; c )
    {
<span class="fc" id="L89">        Tensor&lt;?&gt; a = call.input( 1 );</span>
<span class="fc" id="L90">        Tensor&lt;?&gt; b = call.input( 2 );</span>
        // We need to make sure that the matrices have a common/compatible layout,
        // ..before we can before the actual a @ b = c matrix multiplication!
<span class="fc" id="L93">        NDConfiguration.Layout layoutA = a.getNDConf().getLayout();</span>
<span class="fc" id="L94">        NDConfiguration.Layout layoutB = b.getNDConf().getLayout();</span>
<span class="fc" id="L95">        NDConfiguration.Layout layoutC = c.getNDConf().getLayout();</span>

<span class="fc" id="L97">        boolean aIsCompatible = isRMOrCM( layoutA );</span>
<span class="fc" id="L98">        boolean bIsCompatible = isRMOrCM( layoutB );</span>

<span class="fc" id="L100">        Function relayout = Neureka.get().backend().getFunction().relayout();</span>

<span class="fc bfc" id="L102" title="All 2 branches covered.">        if ( aIsCompatible ) {</span>
<span class="fc bfc" id="L103" title="All 2 branches covered.">            if ( layoutB != NDConfiguration.Layout.SYMMETRIC )</span>
<span class="fc" id="L104">                b = relayout.with(Arg.Layout.of(layoutA)).call(b); // We choose a valid layout based on a</span>
<span class="fc" id="L105">            layoutC = layoutA;</span>
<span class="fc bfc" id="L106" title="All 2 branches covered.">        } else if ( bIsCompatible ) {</span>
<span class="fc bfc" id="L107" title="All 2 branches covered.">            if ( layoutA != NDConfiguration.Layout.SYMMETRIC )</span>
<span class="fc" id="L108">                a = relayout.with(Arg.Layout.of(layoutB)).call(a); // We choose a valid layout based on b</span>
<span class="fc" id="L109">            layoutC = layoutB;</span>
        } else {
            // Ok so the inputs are unspecific/symmetric/ (not RM or CM)
            // So we just need to decide on any valid layout really:
<span class="pc bpc" id="L113" title="1 of 2 branches missed.">            layoutC = isRMOrCM(layoutC) ? layoutC : NDConfiguration.Layout.ROW_MAJOR;</span>
<span class="fc" id="L114">            a = relayout.with(Arg.Layout.of(layoutC)).call(a);</span>
<span class="fc" id="L115">            b = relayout.with(Arg.Layout.of(layoutC)).call(b);</span>
        }

<span class="fc" id="L118">        c.mut().toLayout( layoutC );</span>
<span class="fc" id="L119">        c.mut().setIsVirtual( false ); // This statement is after the layout conversion for performance reasons (virtual tensors barely need copying).</span>

<span class="fc" id="L121">        return call.withInputAt( 1, a ).withInputAt( 2, b );</span>
    }

    private static boolean isRMOrCM(NDConfiguration.Layout layout ) {
<span class="fc bfc" id="L125" title="All 4 branches covered.">        return layout == NDConfiguration.Layout.ROW_MAJOR ||</span>
               layout == NDConfiguration.Layout.COLUMN_MAJOR;
    }

    /**
     *  This method will clone {@link Tensor} instances if they do not
     *  possess a simple {@link neureka.ndim.config.NDConfiguration}.
     *  This is usually the case when they are slices or permuted views on data...
     *  The reason for this is simply that we need inline data for the OpenCL kernels!
     *
     *
     * @param call The execution call whose tensors ought to be cloned based on the complexity of their access patterns.
     */
    private static ExecutionCall&lt;?&gt; _autoClone( ExecutionCall&lt;?&gt; call ) {
<span class="fc bfc" id="L139" title="All 2 branches covered.">        for ( int i = 0; i &lt; call.arity(); i++ )</span>
<span class="fc" id="L140">            if (</span>
<span class="fc bfc" id="L141" title="All 4 branches covered.">                (!_isSimpleRowMajorMatrix( call.input( i ) ) &amp;&amp; !_isSimpleColumnMajorMatrix( call.input( i ) ))</span>
                        ||
<span class="pc bpc" id="L143" title="1 of 2 branches missed.">                call.input( i ).isPartialSlice()</span>
            ) {
<span class="fc" id="L145">                _LOG.debug(&quot;Auto cloning a tensor which does not have a simple ND configuration...&quot;);</span>
<span class="fc" id="L146">                call = call.withInputAt( i, call.input( i ).deepCopy().mut().setIsIntermediate( true ) );</span>
                /*
                    The user should do cloning explicitly because using slices
                    will cause the backend to perform auto cloning every time the
                    slice is being used for operations like this one...
                 */
            }

<span class="fc" id="L154">        return call;</span>
    }

    private static boolean _isSimpleColumnMajorMatrix( Tensor&lt;?&gt; t ) {
<span class="pc bpc" id="L158" title="1 of 4 branches missed.">        return t.rank() == 2 &amp;&amp; t.getNDConf().getLayout() == NDConfiguration.Layout.COLUMN_MAJOR;</span>
    }

    private static boolean _isSimpleRowMajorMatrix( Tensor&lt;?&gt; t ) {
<span class="pc bpc" id="L162" title="1 of 4 branches missed.">        return t.rank() == 2 &amp;&amp; t.getNDConf().getLayout() == NDConfiguration.Layout.ROW_MAJOR;</span>
    }

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.11.202310140853</span></div></body></html>