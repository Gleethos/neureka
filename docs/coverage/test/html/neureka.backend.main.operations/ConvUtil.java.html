<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ConvUtil.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.backend.main.operations</a> &gt; <span class="el_source">ConvUtil.java</span></div><h1>ConvUtil.java</h1><pre class="source lang-java linenums">package neureka.backend.main.operations;

import neureka.Neureka;
import neureka.Tsr;
import neureka.autograd.ADAgent;
import neureka.backend.api.ExecutionCall;
import neureka.backend.api.fun.ADAgentSupplier;
import neureka.backend.api.AutoDiffMode;
import neureka.backend.api.Result;
import neureka.backend.main.algorithms.Convolution;
import neureka.backend.main.operations.other.Reshape;
import neureka.calculus.Function;
import neureka.calculus.args.Arg;
import neureka.calculus.assembly.FunctionParser;
import neureka.calculus.internal.CalcUtil;
import neureka.devices.Device;
import org.jetbrains.annotations.Contract;

<span class="pc bpc" id="L19" title="1 of 2 branches missed.">public class ConvUtil {</span>

    /**
     *  There will always only be a single convolution instance
     *  shared among all 3 convolution operations.
     */
<span class="fc" id="L25">    private static Convolution conv = null;</span>

    public static Convolution createDeconvolutionFor( String operator ) {
<span class="fc" id="L28">        return new Convolution()</span>
<span class="fc" id="L29">                .setAutogradModeFor( call -&gt; {</span>
<span class="fc bfc" id="L30" title="All 2 branches covered.">                    if ( call.getOperation().supports( Convolution.class ) ) return AutoDiffMode.BACKWARD_ONLY;</span>
<span class="fc" id="L31">                    Tsr&lt;?&gt; last = null;</span>
<span class="pc bpc" id="L32" title="1 of 2 branches missed.">                    for ( Tsr&lt;?&gt; t : call.inputs() ) {</span>
<span class="nc bnc" id="L33" title="All 4 branches missed.">                        if ( last != null &amp;&amp; !last.shape().equals(t.shape()) ) return AutoDiffMode.BACKWARD_ONLY;</span>
<span class="nc" id="L34">                        last = t; // Note: shapes are cached!</span>
                    }
<span class="fc" id="L36">                    return AutoDiffMode.FORWARD_AND_BACKWARD;</span>
                })
<span class="fc" id="L38">                .setExecution(</span>
                    ( caller, call ) -&gt; {
<span class="fc" id="L40">                        ADAgentSupplier autoDiff = ( Function f, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; adCall ) -&gt;</span>
                        {
<span class="fc" id="L42">                            int d = adCall.getDerivativeIndex();</span>
<span class="fc" id="L43">                            Function deConv = new FunctionParser( Neureka.get().backend() ).parse(</span>
                                    &quot;I[ 0 ]&quot; + operator + &quot;&gt;&gt;I[ 1 ]&quot; + operator + &quot;&gt;&gt;I[ 2 ]&quot;,
                                    false
                            );
<span class="fc" id="L47">                            Tsr&lt;?&gt; derivative = f.derive( (Tsr[]) adCall.inputs(), d );</span>
<span class="pc bpc" id="L48" title="3 of 6 branches missed.">                            assert d &gt;= 0 &amp;&amp; d &lt;= 1;</span>
<span class="pc bpc" id="L49" title="2 of 4 branches missed.">                            assert derivative != null;</span>
<span class="pc bpc" id="L50" title="2 of 4 branches missed.">                            assert deConv != null;</span>
<span class="pc bpc" id="L51" title="3 of 6 branches missed.">                            assert adCall.arity() &gt;= 2 &amp;&amp; adCall.arity() &lt;= 3;</span>
                            // Now we need to remember the shape of the input which is targeted for back prop.
<span class="fc bfc" id="L53" title="All 2 branches covered.">                            int[] shape = adCall.input( adCall.arity() &gt; 2 ? d + 1 : d ).getNDConf().shape();</span>
                            // This is because it will be the shape of the output to the de-convolution!
<span class="fc" id="L55">                            return ADAgent.of( derivative )</span>
<span class="fc" id="L56">                                    .withAD(</span>
                                            target -&gt;
<span class="fc" id="L58">                                                    deConv.execute(</span>
<span class="fc" id="L59">                                                            target.error(),</span>
                                                            derivative,
<span class="fc" id="L61">                                                            Tsr.of(shape, 0).getUnsafe().setIsIntermediate( false )</span>
                                                    )
                                    );
                        };
<span class="fc bfc" id="L65" title="All 2 branches covered.">                        if ( !caller.isFlat() ) return Result.of(CalcUtil.defaultRecursiveExecution( caller, call )).withAutoDiff(autoDiff);</span>
<span class="fc bfc" id="L66" title="All 2 branches covered.">                        if ( call.getOperation().getOperator().equals(&quot;x&quot;) ) {</span>
<span class="fc" id="L67">                            Tsr&lt;?&gt;[] tensors = new Tsr[]{null, call.input( 0 ), call.input( 1 )};</span>
<span class="fc" id="L68">                            tensors[ 0 ] =</span>
<span class="fc bfc" id="L69" title="All 2 branches covered.">                                (call.getValOf( Arg.DerivIdx.class ) &lt; 0)</span>
<span class="fc" id="L70">                                    ? Tsr.of(</span>
<span class="fc" id="L71">                                        call.input(0).getValueClass(),</span>
<span class="fc" id="L72">                                            _shapeOfCon( tensors[ 1 ].getNDConf().shape(), tensors[ 2 ].getNDConf().shape() ),</span>
<span class="fc" id="L73">                                            0</span>
                                        )
<span class="fc" id="L75">                                        .getUnsafe()</span>
<span class="fc" id="L76">                                        .setIsIntermediate( true )</span>
<span class="fc" id="L77">                                    : null;</span>

<span class="fc bfc" id="L79" title="All 4 branches covered.">                            for ( Tsr&lt;?&gt; t : tensors ) if ( t != null ) t.setIsVirtual( false );</span>
<span class="fc" id="L80">                            tensors[ 0 ] = CalcUtil.recursiveExecution( call.withInputs(tensors), JunctionUtil::forConvolution );</span>
<span class="pc bpc" id="L81" title="1 of 2 branches missed.">                            if ( tensors[ 0 ] == null )</span>
<span class="nc" id="L82">                                throw new IllegalStateException(&quot;Failed to execute convolution!&quot;);</span>
<span class="fc" id="L83">                            return Result.of(tensors[ 0 ]).withAutoDiff(autoDiff);</span>
                        } else {
<span class="pc bpc" id="L85" title="1 of 2 branches missed.">                            if ( call.getValOf( Arg.DerivIdx.class ) &lt; 0 ) {</span>
<span class="fc" id="L86">                                Tsr&lt;?&gt;[] tensors = CalcUtil.srcActivation(call.inputs(), call.getValOf( Arg.VarIdx.class ), -1, 0, caller.getSubFunctions().toArray(new Function[0]));</span>
<span class="fc" id="L87">                                Reshape.makeFit(tensors, caller.isDoingAD()); // This might not fit here... (fitting should probably be a setup thing...)</span>
<span class="fc bfc" id="L88" title="All 2 branches covered.">                                for ( Tsr&lt;?&gt; t : tensors ) t.setIsVirtual( false );</span>
<span class="fc" id="L89">                                tensors[ 0 ] = CalcUtil.recursiveExecution(</span>
<span class="fc" id="L90">                                                            ExecutionCall.of( tensors )</span>
<span class="fc" id="L91">                                                                            .andArgs( Arg.DerivIdx.of(0) )</span>
<span class="fc" id="L92">                                                                            .running( call.getOperation() )</span>
<span class="fc" id="L93">                                                                            .on( call.getDevice() ),</span>
                                                            JunctionUtil::forConvolution
                                                        );
<span class="fc bfc" id="L96" title="All 2 branches covered.">                                if ( call.getOperation() == Neureka.get().backend().getOperation(&quot;x&gt;&gt;&quot;) )</span>
<span class="fc" id="L97">                                    return Result.of(tensors[ 2 ]).withAutoDiff(autoDiff);</span>
                                else
<span class="fc" id="L99">                                    return Result.of(tensors[ 0 ]).withAutoDiff(autoDiff);</span>
                            }
                        }
<span class="nc" id="L102">                        return Result.of(CalcUtil.defaultRecursiveExecution( caller, call )).withAutoDiff(autoDiff);</span>
                    }
                )
<span class="fc" id="L105">                .setCallPreparation(</span>
                     call -&gt; {
<span class="fc" id="L107">                         Device&lt;Number&gt; device = call.getDeviceFor(Number.class);</span>
<span class="fc bfc" id="L108" title="All 2 branches covered.">                         if ( call.input( 0 ) == null ) // Creating a new tensor:</span>
                         {
<span class="fc" id="L110">                             int[] shp = _shapeOfCon(call.input( 1 ).getNDConf().shape(), call.input( 2 ).getNDConf().shape());</span>
<span class="fc" id="L111">                             Tsr&lt;Double&gt; output = Tsr.of( shp, 0.0 ).getUnsafe().setIsIntermediate( true );</span>
<span class="fc" id="L112">                             output.setIsVirtual( false );</span>
                             try {
<span class="fc" id="L114">                                 device.store( output );</span>
<span class="nc" id="L115">                             } catch ( Exception e ) {</span>
<span class="nc" id="L116">                                 e.printStackTrace();</span>
<span class="fc" id="L117">                             }</span>
<span class="fc" id="L118">                             call.setInput( 0, output );</span>
                         }
<span class="fc" id="L120">                         return call;</span>
                     }
                )
<span class="fc" id="L123">                .buildFunAlgorithm();</span>
    }

    public static Convolution getConv() {
<span class="fc bfc" id="L127" title="All 2 branches covered.">        if ( conv == null )</span>
<span class="fc" id="L128">            conv = createDeconvolutionFor(&quot;x&quot;);</span>
<span class="fc" id="L129">        return ConvUtil.conv;</span>
    }

    @Contract(pure = true)
    private static int[] _shapeOfCon( int[] shape1, int[] shape2 ) {
<span class="fc" id="L134">        int[] shape = new int[ ( shape1.length + shape2.length ) / 2 ];</span>
<span class="pc bpc" id="L135" title="1 of 4 branches missed.">        for ( int i = 0; i &lt; shape1.length &amp;&amp; i &lt; shape2.length; i++ )</span>
<span class="fc" id="L136">            shape[ i ] = Math.abs( shape1[ i ] - shape2[ i ] ) + 1;</span>
<span class="fc" id="L137">        return shape;</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>