<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>RMSProp.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.optimization.implementations</a> &gt; <span class="el_source">RMSProp.java</span></div><h1>RMSProp.java</h1><pre class="source lang-java linenums">package neureka.optimization.implementations;

import neureka.Tensor;
import neureka.common.utility.LogUtil;
import neureka.optimization.Optimizer;

/**
 * Root Mean Squared Propagation, or RMSProp,
 * is an extension of gradient descent and the AdaGrad version of gradient
 * descent that uses a decaying average of partial gradients in the adaptation of the
 * step size for each parameter.
 * It is similar to {@link AdaGrad} in that it uses a moving average of
 * the squared gradients to scale the learning rate.
 *
 * @param &lt;V&gt; The super type of the value item type for the tensors whose gradients can be optimized by this.
 */
public class RMSProp&lt;V extends Number&gt; implements Optimizer&lt;V&gt;
{
    private final double lr; // learning rate
    private final double decay; // decay rate
    private final Tensor&lt;Number&gt; h; // sum of squared gradients:

<span class="fc" id="L23">    RMSProp(Tensor&lt;Number&gt; target, double learningRate, double decay ) {</span>
<span class="fc" id="L24">        LogUtil.nullArgCheck( target, &quot;target&quot;, Tensor.class );</span>
<span class="fc" id="L25">        h = Tensor.of(target.getItemType(), target.shape(), 0);</span>
<span class="fc" id="L26">        lr = learningRate; // Step size/learning rate is 0.001 by default!</span>
<span class="fc" id="L27">        this.decay = decay; // Decay rate is 0.9 by default!</span>
<span class="fc" id="L28">    }</span>

    @Override
    public Tensor&lt;V&gt; optimize(Tensor&lt;V&gt; w ) {
<span class="fc" id="L32">        LogUtil.nullArgCheck( w, &quot;w&quot;, Tensor.class ); // The input must not be null!</span>
<span class="fc" id="L33">        Tensor&lt;Number&gt; g = w.gradient().get().mut().upcast(Number.class);</span>
<span class="fc" id="L34">        h.getMut().timesAssign(decay);</span>
<span class="fc" id="L35">        h.getMut().plusAssign(g.power(2).times(1 - decay));</span>
<span class="fc" id="L36">        return Tensor.of(&quot;-&quot; + lr + &quot; * &quot;, g, &quot; / ( ( &quot;, h, &quot; ** 0.5 ) + 1e-8 )&quot;);</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.11.202310140853</span></div></body></html>