<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>AdaGrad.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.optimization.implementations</a> &gt; <span class="el_source">AdaGrad.java</span></div><h1>AdaGrad.java</h1><pre class="source lang-java linenums">package neureka.optimization.implementations;

import neureka.Shape;
import neureka.Tsr;
import neureka.common.utility.LogUtil;
import neureka.optimization.Optimizer;

import java.util.List;

/**
 * Adaptive Gradients, or AdaGrad for short, is an extension of the gradient descent optimization
 * algorithm that adjusts the step size for each parameter based on the squared gradients
 * seen over the course of previous optimization steps.
 *
 * @param &lt;V&gt; The super type of the value item type for the tensors whose gradients can be optimized by this.
 */
public class AdaGrad&lt;V extends Number&gt; implements Optimizer&lt;V&gt;
{
    private static final double E = 1e-8;

    private final double lr; // learning rate
    private final Tsr&lt;Number&gt; h; // sum of squared gradients:

<span class="fc" id="L24">    AdaGrad( Tsr&lt;V&gt; target, double learningRate ) {</span>
<span class="fc" id="L25">        LogUtil.nullArgCheck( target, &quot;target&quot;, Tsr.class );</span>
<span class="fc" id="L26">        Shape shape = target.shape();</span>
<span class="fc" id="L27">        h = Tsr.of(target.getItemType(), shape, 0).getMut().upcast(Number.class);</span>
<span class="fc" id="L28">        lr = learningRate; // Step size/learning rate is 0.01 by default!</span>
<span class="fc" id="L29">    }</span>

    @Override
    public Tsr&lt;V&gt; optimize( Tsr&lt;V&gt; w ) {
<span class="fc" id="L33">        LogUtil.nullArgCheck( w, &quot;w&quot;, Tsr.class ); // The input must not be null!</span>
<span class="fc" id="L34">        Tsr&lt;Number&gt; g = w.gradient().get().mut().upcast(Number.class);</span>
<span class="fc" id="L35">        h.getMut().plusAssign(g.power(2));</span>
<span class="fc" id="L36">        return Tsr.of(&quot;-&quot;+ lr +&quot; * &quot;, g, &quot; / ( ( &quot;, h, &quot; ** 0.5 ) + &quot;+E+&quot; )&quot;);</span>
    }

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.9.202303310957</span></div></body></html>