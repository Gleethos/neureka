<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>Convolution.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.backend.main.operations.linear</a> &gt; <span class="el_source">Convolution.java</span></div><h1>Convolution.java</h1><pre class="source lang-java linenums">package neureka.backend.main.operations.linear;

import neureka.Neureka;
import neureka.Shape;
import neureka.Tensor;
import neureka.autograd.ADAction;
import neureka.backend.api.AutoDiffMode;
import neureka.backend.api.ExecutionCall;
import neureka.backend.api.Result;
import neureka.backend.api.template.algorithms.AbstractDeviceAlgorithm;
import neureka.backend.api.template.operations.AbstractOperation;
import neureka.backend.api.template.operations.OperationBuilder;
import neureka.backend.main.algorithms.NDConvolution;
import neureka.backend.main.operations.ConvUtil;
import neureka.math.Function;
import neureka.math.args.Arg;
import neureka.math.parsing.FunctionParser;
import neureka.devices.Device;

<span class="fc" id="L20">public class Convolution extends AbstractOperation</span>
{
    public Convolution()
    {
<span class="fc" id="L24">        super(</span>
            new OperationBuilder()
<span class="fc" id="L26">                .identifier(       &quot;mul_conv&quot;  )</span>
<span class="fc" id="L27">                .operator(         &quot;x&quot;         )</span>
<span class="fc" id="L28">                .arity(            2           )</span>
<span class="fc" id="L29">                .isOperator(       true        )</span>
<span class="fc" id="L30">                .isIndexer(        false       )</span>
<span class="fc" id="L31">                .isDifferentiable( true        )</span>
<span class="fc" id="L32">                .isInline(         false       )</span>
        );

<span class="fc" id="L35">        setAlgorithm(</span>
            NDConvolution.class,
            new NDConvolution()
<span class="fc" id="L38">            .setAutogradModeFor( call -&gt; {</span>
<span class="fc bfc" id="L39" title="All 2 branches covered.">                if ( call.getOperation().supports( NDConvolution.class ) ) return AutoDiffMode.BACKWARD_ONLY;</span>
<span class="fc" id="L40">                Tensor&lt;?&gt; last = null;</span>
<span class="pc bpc" id="L41" title="1 of 2 branches missed.">                for ( Tensor&lt;?&gt; t : call.inputs() ) {</span>
<span class="nc bnc" id="L42" title="All 4 branches missed.">                    if ( last != null &amp;&amp; !last.shape().equals(t.shape()) ) return AutoDiffMode.BACKWARD_ONLY;</span>
<span class="nc" id="L43">                    last = t; // Note: shapes are cached!</span>
                }
<span class="fc" id="L45">                return AutoDiffMode.FORWARD_AND_BACKWARD;</span>
            })
<span class="fc" id="L47">            .setExecution(</span>
                (outerCaller, outerCall) -&gt;
<span class="fc" id="L49">                    Result.of(AbstractDeviceAlgorithm.prepareAndExecute(</span>
                        outerCall,
                        call -&gt;
<span class="fc" id="L52">                                AbstractDeviceAlgorithm.executeDeviceAlgorithm(</span>
                                        call
                                )
                    ))
<span class="fc" id="L56">                    .withAutoDiff(( Function f, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; adCall ) -&gt;</span>
                    {
<span class="fc" id="L58">                        int d = adCall.getDerivativeIndex();</span>
<span class="fc" id="L59">                        Function deConv = new FunctionParser( Neureka.get().backend() ).parse(</span>
                                &quot;I[ 0 ] x&gt;&gt; I[ 1 ] x&gt;&gt; I[ 2 ]&quot;,
                                false
                        );
<span class="fc" id="L63">                        Tensor&lt;?&gt; derivative = f.derive( (Tensor[]) adCall.inputs(), d );</span>
<span class="pc bpc" id="L64" title="2 of 4 branches missed.">                        assert d &gt;= 0 &amp;&amp; d &lt;= 1;</span>
<span class="pc bpc" id="L65" title="1 of 2 branches missed.">                        assert derivative != null;</span>
<span class="pc bpc" id="L66" title="1 of 2 branches missed.">                        assert deConv != null;</span>
<span class="pc bpc" id="L67" title="2 of 4 branches missed.">                        assert adCall.arity() &gt;= 2 &amp;&amp; adCall.arity() &lt;= 3;</span>
                        // Now we need to remember the shape of the input which is targeted for back prop.
<span class="pc bpc" id="L69" title="1 of 2 branches missed.">                        Shape shape = Shape.of(adCall.input( adCall.arity() &gt; 2 ? d + 1 : d ).getNDConf().shape());</span>
                        Number zero;
<span class="fc bfc" id="L71" title="All 2 branches covered.">                        if ( derivative.getItemType() == Double.class         ) zero = 0d;</span>
<span class="fc bfc" id="L72" title="All 2 branches covered.">                        else if ( derivative.getItemType() == Float.class     ) zero = 0f;</span>
<span class="pc bpc" id="L73" title="1 of 2 branches missed.">                        else if ( derivative.getItemType() == Integer.class   ) zero = 0;</span>
<span class="nc bnc" id="L74" title="All 2 branches missed.">                        else if ( derivative.getItemType() == Long.class      ) zero = 0L;</span>
<span class="nc bnc" id="L75" title="All 2 branches missed.">                        else if ( derivative.getItemType() == Short.class     ) zero = (short) 0;</span>
<span class="nc bnc" id="L76" title="All 2 branches missed.">                        else if ( derivative.getItemType() == Byte.class      ) zero = (byte) 0;</span>
                        else {
<span class="nc" id="L78">                            zero = null;</span>
<span class="nc" id="L79">                            throw new IllegalArgumentException(&quot;Unsupported item type for convolution derivative: &quot; + derivative.getItemType());</span>
                        }
                        // This is because it will be the shape of the output to the de-convolution!
<span class="fc" id="L82">                        return ADAction.of( target -&gt;</span>
<span class="fc" id="L83">                                deConv.execute(</span>
<span class="fc" id="L84">                                        target.error(),</span>
                                        derivative,
<span class="fc" id="L86">                                        Tensor.of(shape, zero).mut().setIsIntermediate( false )</span>
                                )
                        );
                    })
            )
<span class="fc" id="L91">            .setCallPreparation(</span>
                 call -&gt; {
<span class="pc bpc" id="L93" title="1 of 2 branches missed.">                     if ( call.arity() &lt;= 2 ) call = call.withAddedInputAt( 0, null );</span>
<span class="fc" id="L94">                     Device&lt;Number&gt; device = call.getDeviceFor(Number.class);</span>
<span class="fc" id="L95">                     Shape shp = ConvUtil.shapeOfCon(call.input( 1 ).getNDConf().shape(), call.input( 2 ).getNDConf().shape());</span>
<span class="fc" id="L96">                     Tensor&lt;Number&gt; output = (Tensor&lt;Number&gt;) Tensor.of( call.input(1).getItemType(), shp, 0 )</span>
<span class="fc" id="L97">                                                             .mut()</span>
<span class="fc" id="L98">                                                             .setIsIntermediate( true );</span>
<span class="fc" id="L99">                     output.mut().setIsVirtual( false );</span>
                     //device.store( output );//Todo: find out why this causes problems
<span class="fc" id="L101">                     return call.withInputAt( 0, output );</span>
                 }
            )
<span class="fc" id="L104">            .buildFunAlgorithm()</span>
        );

<span class="fc" id="L107">    }</span>


    @Override
    public Result execute( final Function caller, final ExecutionCall&lt;?&gt; call )
    {
<span class="fc bfc" id="L113" title="All 2 branches covered.">        if ( !caller.isFlat() ) {</span>
<span class="fc" id="L114">            Function reducedCaller = reducePairwise(caller);</span>
<span class="fc" id="L115">            ExecutionCall&lt;?&gt; flatCall = AbstractDeviceAlgorithm.flatten( reducedCaller, call.withArgs(Arg.DerivIdx.of(-1)) );</span>
<span class="fc" id="L116">            Function flat = new FunctionParser(Neureka.get().backend()).parse( flatCall.getOperation(), flatCall.arity(), true );</span>
<span class="pc bpc" id="L117" title="1 of 4 branches missed.">            for ( Tensor&lt;?&gt; t : flatCall.inputs() ) if ( t != null ) t.mut().setIsIntermediate(false);</span>
<span class="fc" id="L118">            return this.execute( flat, flatCall );</span>
        }
<span class="fc bfc" id="L120" title="All 2 branches covered.">        if ( call.getDerivativeIndex() &gt;= 0 ) {</span>
<span class="fc" id="L121">            int d = call.getDerivativeIndex();</span>
            /*
                In autograd convolution is similar to matrix multiplication.
                If the derivative index is 0 then the second operand is used for backward broadcasting.
                If the derivative index is 1 then the first operand is used for backward broadcasting.
             */
<span class="fc bfc" id="L127" title="All 2 branches covered.">            return Result.of( call.input( d == 0 ? 1 : 0 ) );</span>
        }
<span class="fc" id="L129">        Function reducedCaller = reducePairwise(caller);</span>
<span class="fc" id="L130">        ExecutionCall&lt;?&gt; flatCall = AbstractDeviceAlgorithm.flatten( reducedCaller, call.withArgs(Arg.DerivIdx.of(-1)) );</span>
<span class="fc" id="L131">        Function flat = new FunctionParser(Neureka.get().backend()).parse( flatCall.getOperation(), flatCall.arity(), true );</span>
<span class="pc bpc" id="L132" title="1 of 4 branches missed.">        for ( Tensor&lt;?&gt; t : flatCall.inputs() ) if ( t != null ) t.mut().setIsIntermediate(false);</span>
<span class="fc" id="L133">        return super.execute( flat, flatCall );</span>
    }

    private Function reducePairwise( final Function fun ) {
<span class="fc" id="L137">        Function reduced = fun;</span>
<span class="pc bpc" id="L138" title="1 of 2 branches missed.">        if ( reduced.getSubFunctions().size() &gt; 2 ) {</span>
            /*
                So currently we have something like this: a x b x c x d...
                However, this is how it is really executed:  ((((a x b) x c) x d)..)
                ...so let's create a function that is nested like the above:
            */
<span class="nc" id="L144">            Function nested = reduced.getSubFunctions().get(0);</span>
<span class="nc bnc" id="L145" title="All 2 branches missed.">            for ( int i = 1; i &lt; reduced.getSubFunctions().size(); i++ )</span>
<span class="nc" id="L146">                nested = Function.of( nested + &quot; x &quot; + reduced.getSubFunctions().get(i), true );</span>

<span class="nc" id="L148">            reduced = nested;</span>
        }
<span class="fc" id="L150">        return reduced;</span>
    }

    @Override
<span class="nc" id="L154">    public double calculate( double[] inputs, int j, int d, Function[] src ) { return src[ 0 ].call( inputs, j ); }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.9.202303310957</span></div></body></html>