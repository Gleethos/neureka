<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>XConv.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.backend.main.operations.linear</a> &gt; <span class="el_source">XConv.java</span></div><h1>XConv.java</h1><pre class="source lang-java linenums">package neureka.backend.main.operations.linear;

import neureka.Neureka;
import neureka.Tsr;
import neureka.autograd.ADAction;
import neureka.backend.api.AutoDiffMode;
import neureka.backend.api.ExecutionCall;
import neureka.backend.api.template.algorithms.AbstractDeviceAlgorithm;
import neureka.backend.api.template.operations.AbstractOperation;
import neureka.backend.api.template.operations.OperationBuilder;
import neureka.backend.main.algorithms.Convolution;
import neureka.backend.main.implementations.CLImplementation;
import neureka.backend.main.operations.ConvUtil;
import neureka.backend.main.implementations.convolution.CPUXConv;
import neureka.calculus.Function;
import neureka.calculus.args.Arg;
import neureka.calculus.assembly.FunctionParser;
import neureka.devices.Device;
import neureka.devices.host.CPU;
import neureka.devices.opencl.OpenCLDevice;

<span class="pc bpc" id="L22" title="1 of 2 branches missed.">public class XConv extends AbstractOperation</span>
{
    public XConv()
    {
<span class="fc" id="L26">        super(</span>
                new OperationBuilder()
<span class="fc" id="L28">                        .identifier(       &quot;mul_conv&quot;  )</span>
<span class="fc" id="L29">                        .operator(         &quot;x&quot;         )</span>
<span class="fc" id="L30">                        .arity(            2           )</span>
<span class="fc" id="L31">                        .isOperator(       true        )</span>
<span class="fc" id="L32">                        .isIndexer(        false       )</span>
<span class="fc" id="L33">                        .isDifferentiable( true        )</span>
<span class="fc" id="L34">                        .isInline(         false       )</span>
        );

<span class="fc" id="L37">        setAlgorithm(</span>
            Convolution.class,
            new Convolution()
<span class="fc" id="L40">                .setAutogradModeFor( call -&gt; {</span>
<span class="fc bfc" id="L41" title="All 2 branches covered.">                    if ( call.getOperation().supports( Convolution.class ) ) return AutoDiffMode.BACKWARD_ONLY;</span>
<span class="fc" id="L42">                    Tsr&lt;?&gt; last = null;</span>
<span class="pc bpc" id="L43" title="1 of 2 branches missed.">                    for ( Tsr&lt;?&gt; t : call.inputs() ) {</span>
<span class="nc bnc" id="L44" title="All 4 branches missed.">                        if ( last != null &amp;&amp; !last.shape().equals(t.shape()) ) return AutoDiffMode.BACKWARD_ONLY;</span>
<span class="nc" id="L45">                        last = t; // Note: shapes are cached!</span>
                    }
<span class="fc" id="L47">                    return AutoDiffMode.FORWARD_AND_BACKWARD;</span>
                })
<span class="fc" id="L49">                .setDeviceExecution(</span>
                    (call, executor) -&gt;
                    {
<span class="fc" id="L52">                        Tsr&lt;?&gt;[] tensors = call.inputs();</span>
<span class="pc bpc" id="L53" title="1 of 4 branches missed.">                        for ( Tsr&lt;?&gt; t : tensors ) if ( t != null ) t.setIsVirtual( false );</span>

<span class="fc" id="L55">                        ExecutionCall&lt;?&gt; prepared = AbstractDeviceAlgorithm._prepareForExecution( call.withInputs(tensors) );</span>
<span class="fc" id="L56">                        return AbstractDeviceAlgorithm.executeOnCommonDevice(</span>
<span class="fc" id="L57">                                prepared,()-&gt;ConvUtil.executeRecursively( &quot;x&quot;, prepared, null/*recursion is not expected to happen here*/ )</span>
                        );
                    },
                    ( Function f, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; adCall ) -&gt;
                    {
<span class="fc" id="L62">                        int d = adCall.getDerivativeIndex();</span>
<span class="fc" id="L63">                        Function deConv = new FunctionParser( Neureka.get().backend() ).parse(</span>
                                &quot;I[ 0 ] x&gt;&gt; I[ 1 ] x&gt;&gt; I[ 2 ]&quot;,
                                false
                        );
<span class="fc" id="L67">                        Tsr&lt;?&gt; derivative = f.derive( (Tsr[]) adCall.inputs(), d );</span>
<span class="pc bpc" id="L68" title="3 of 6 branches missed.">                        assert d &gt;= 0 &amp;&amp; d &lt;= 1;</span>
<span class="pc bpc" id="L69" title="2 of 4 branches missed.">                        assert derivative != null;</span>
<span class="pc bpc" id="L70" title="2 of 4 branches missed.">                        assert deConv != null;</span>
<span class="pc bpc" id="L71" title="3 of 6 branches missed.">                        assert adCall.arity() &gt;= 2 &amp;&amp; adCall.arity() &lt;= 3;</span>
                        // Now we need to remember the shape of the input which is targeted for back prop.
<span class="fc bfc" id="L73" title="All 2 branches covered.">                        int[] shape = adCall.input( adCall.arity() &gt; 2 ? d + 1 : d ).getNDConf().shape();</span>
                        // This is because it will be the shape of the output to the de-convolution!
<span class="fc" id="L75">                        return ADAction.of( target -&gt;</span>
<span class="fc" id="L76">                                        deConv.execute(</span>
<span class="fc" id="L77">                                                target.error(),</span>
                                                derivative,
<span class="fc" id="L79">                                                Tsr.of(shape, 0).getUnsafe().setIsIntermediate( false )</span>
                                        )
                                );
                    }
                )
<span class="fc" id="L84">                .setCallPreparation(</span>
                     call -&gt; {
<span class="fc" id="L86">                         Device&lt;Number&gt; device = call.getDeviceFor(Number.class);</span>
<span class="fc" id="L87">                         int[] shp = ConvUtil.shapeOfCon(call.input( 1 ).getNDConf().shape(), call.input( 2 ).getNDConf().shape());</span>
<span class="fc" id="L88">                         Tsr&lt;Number&gt; output = (Tsr&lt;Number&gt;) Tsr.of( call.input(1).getItemType(), shp, 0 )</span>
<span class="fc" id="L89">                                                 .getUnsafe()</span>
<span class="fc" id="L90">                                                 .setIsIntermediate( true );</span>
<span class="fc" id="L91">                         output.setIsVirtual( false );</span>
                         //device.store( output );//Todo: find out why this causes problems
<span class="fc" id="L93">                         return call.withInputAt( 0, output );</span>
                     }
                )
<span class="fc" id="L96">                .buildFunAlgorithm()</span>
<span class="fc" id="L97">                .setImplementationFor(</span>
                    CPU.class,
                    new CPUXConv()
                )
<span class="fc" id="L101">                .setImplementationFor(</span>
                    OpenCLDevice.class,
<span class="fc" id="L103">                    CLImplementation.compiler()</span>
<span class="fc" id="L104">                            .arity( 3 )</span>
<span class="fc" id="L105">                            .kernelSource( Neureka.get().utility().readResource(&quot;kernels/convolution_template.cl&quot;) )</span>
<span class="fc" id="L106">                            .activationSource( &quot;value = src1 * src2;\n&quot; )</span>
<span class="fc" id="L107">                            .differentiationSource( &quot;value += handle * drain;\n&quot; )</span>
<span class="fc" id="L108">                            .kernelPostfix( this.getIdentifier() )</span>
<span class="fc" id="L109">                            .execution(</span>
                                call -&gt; {
<span class="pc bpc" id="L111" title="1 of 2 branches missed.">                                    int offset = ( call.input( Number.class, 0 ) != null ) ? 0 : 1;</span>
<span class="pc bpc" id="L112" title="1 of 2 branches missed.">                                    int gwz = ( call.input( Number.class, 0 ) != null ) ? call.input( Number.class, 0 ).size() : call.input( Number.class, 1 ).size();</span>
<span class="fc" id="L113">                                    call.getDevice()</span>
<span class="fc" id="L114">                                        .getKernel(call)</span>
<span class="fc" id="L115">                                        .passAllOf( call.input( Number.class, offset ) )</span>
<span class="fc" id="L116">                                        .passAllOf( call.input( Number.class, offset + 1 ) )</span>
<span class="fc" id="L117">                                        .passAllOf( call.input( Number.class, offset + 2 ) )</span>
<span class="fc" id="L118">                                        .pass( call.input( Number.class, 0 ).rank() )</span>
<span class="fc" id="L119">                                        .pass( call.getValOf( Arg.DerivIdx.class ) )</span>
<span class="fc" id="L120">                                        .call( gwz );</span>

<span class="fc" id="L122">                                    return call.input( 0 );</span>
                                }
                            )
<span class="fc" id="L125">                            .build()</span>
                )
        );

<span class="fc" id="L129">    }</span>

    @Override
<span class="nc" id="L132">    public double calculate( double[] inputs, int j, int d, Function[] src ) { return src[ 0 ].call( inputs, j ); }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>