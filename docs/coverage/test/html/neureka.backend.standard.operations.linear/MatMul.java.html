<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>MatMul.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.backend.standard.operations.linear</a> &gt; <span class="el_source">MatMul.java</span></div><h1>MatMul.java</h1><pre class="source lang-java linenums">package neureka.backend.standard.operations.linear;

import neureka.Neureka;
import neureka.Tsr;
import neureka.autograd.DefaultADAgent;
import neureka.backend.api.Algorithm;
import neureka.backend.api.ExecutionCall;
import neureka.backend.api.Operation;
import neureka.backend.api.operations.AbstractOperation;
import neureka.backend.api.operations.OperationBuilder;
import neureka.backend.standard.algorithms.Convolution;
import neureka.backend.standard.algorithms.GenericAlgorithm;
import neureka.backend.standard.implementations.CLImplementation;
import neureka.backend.standard.implementations.HostImplementation;
import neureka.calculus.Function;
import neureka.calculus.assembly.FunctionBuilder;
import neureka.devices.Device;
import neureka.devices.host.HostCPU;
import neureka.devices.opencl.OpenCLDevice;

public class MatMul extends AbstractOperation
{

    public MatMul()
    {
<span class="nc" id="L26">        super(</span>
                new OperationBuilder()
<span class="nc" id="L28">                        .setFunction(         &quot;matmul&quot;    )</span>
<span class="nc" id="L29">                        .setOperator(         &quot;@&quot;         )</span>
<span class="nc" id="L30">                        .setArity(            2           )</span>
<span class="nc" id="L31">                        .setIsOperator(       true        )</span>
<span class="nc" id="L32">                        .setIsIndexer(        false       )</span>
<span class="nc" id="L33">                        .setIsDifferentiable( true        )</span>
<span class="nc" id="L34">                        .setIsInline(         false       )</span>
        );

<span class="nc" id="L37">        Algorithm.RecursiveJunctor rja = (call, goDeeperWith)-&gt;</span>
        {
<span class="nc" id="L39">            Tsr[] tsrs = call.getTensors();</span>
<span class="nc" id="L40">            Device device = call.getDevice();</span>
<span class="nc" id="L41">            int d = call.getDerivativeIndex();</span>
<span class="nc" id="L42">            Operation type = call.getOperation();</span>

<span class="nc" id="L44">            Tsr alternative = null;</span>
<span class="nc bnc" id="L45" title="All 2 branches missed.">            if (tsrs.length &gt; 3) {</span>
<span class="nc bnc" id="L46" title="All 2 branches missed.">                if ( d &lt; 0 ) {</span>
<span class="nc" id="L47">                    Tsr[] reduction = new Tsr[]{tsrs[ 0 ], tsrs[ 1 ], tsrs[ 2 ]};</span>
<span class="nc" id="L48">                    alternative = goDeeperWith.apply(</span>
<span class="nc" id="L49">                            ExecutionCall.builder()</span>
<span class="nc" id="L50">                                    .device(device)</span>
<span class="nc" id="L51">                                    .tensors(reduction)</span>
<span class="nc" id="L52">                                    .derivativeIndex(d)</span>
<span class="nc" id="L53">                                    .operation(type)</span>
<span class="nc" id="L54">                                    .build()</span>
                    );
<span class="nc" id="L56">                    tsrs[ 0 ] = reduction[ 0 ];</span>

<span class="nc" id="L58">                    reduction = Utility.offsetted(tsrs, 1);</span>
<span class="nc" id="L59">                    alternative = goDeeperWith.apply(</span>
<span class="nc" id="L60">                            ExecutionCall.builder()</span>
<span class="nc" id="L61">                                    .device(device)</span>
<span class="nc" id="L62">                                    .tensors(reduction)</span>
<span class="nc" id="L63">                                    .derivativeIndex(d)</span>
<span class="nc" id="L64">                                    .operation(type)</span>
<span class="nc" id="L65">                                    .build()</span>
                    );
<span class="nc" id="L67">                    tsrs[ 0 ] = reduction[ 0 ];</span>
                }
<span class="nc" id="L69">                return alternative;</span>
            } else
<span class="nc" id="L71">                return alternative;</span>

        };

<span class="nc" id="L75">        DefaultOperatorCreator&lt;TertiaryNDIConsumer&gt; convolutionNDICreator =</span>
                ( inputs, d ) -&gt; {
<span class="nc" id="L77">                    double[] t1_val = inputs[ 1 ].value64();</span>
<span class="nc" id="L78">                    double[] t2_val = inputs[ 2 ].value64();</span>
<span class="nc bnc" id="L79" title="All 2 branches missed.">                    if ( d &lt; 0 ) {</span>
<span class="nc" id="L80">                        return ( t0Idx, t1Idx, t2Idx ) -&gt; t1_val[ t1Idx.i() ] * t2_val[t2Idx.i()];</span>
                    } else {
<span class="nc" id="L82">                        return ( t0Idx, t1Idx, t2Idx ) -&gt; {</span>
<span class="nc bnc" id="L83" title="All 2 branches missed.">                            if (d == 0) return t2_val[t2Idx.i()];</span>
<span class="nc" id="L84">                            else return t1_val[ t1Idx.i() ];</span>
                        };
                    }
                };

<span class="nc" id="L89">        DefaultOperatorCreator&lt;TertiaryNDAConsumer&gt; convolutionCreator =</span>
                ( inputs, d ) -&gt; {
<span class="nc" id="L91">                    double[] t1_val = inputs[ 1 ].value64();</span>
<span class="nc" id="L92">                    double[] t2_val = inputs[ 2 ].value64();</span>
<span class="nc bnc" id="L93" title="All 2 branches missed.">                    if ( d &lt; 0 ) {</span>
<span class="nc" id="L94">                        return ( t0Idx, t1Idx, t2Idx ) -&gt; t1_val[inputs[ 1 ].indexOfIndices( t1Idx )] * t2_val[inputs[ 2 ].indexOfIndices(t2Idx)];</span>
                    } else {
<span class="nc" id="L96">                        return ( t0Idx, t1Idx, t2Idx ) -&gt; {</span>
<span class="nc bnc" id="L97" title="All 2 branches missed.">                            if (d == 0) return t2_val[inputs[ 2 ].indexOfIndices(t2Idx)];</span>
<span class="nc" id="L98">                            else return t1_val[inputs[ 1 ].indexOfIndices( t1Idx )];</span>
                        };
                    }
                };


<span class="nc" id="L104">        GenericAlgorithm convolution = new GenericAlgorithm(&quot;matmul&quot;)</span>
<span class="nc" id="L105">                .setCanPerformBackwardADFor( call -&gt; true )</span>
<span class="nc" id="L106">                .setCanPerformForwardADFor(</span>
                        call -&gt; {
<span class="nc bnc" id="L108" title="All 2 branches missed.">                            if ( call.getOperation().supports(Convolution.class) ) return false;</span>
<span class="nc bnc" id="L109" title="All 2 branches missed.">                            if ( call.getOperation().getOperator().equals(&quot;,&quot;) ) return false; //Reshape</span>
<span class="nc" id="L110">                            Tsr&lt;?&gt; last = null;</span>
<span class="nc bnc" id="L111" title="All 2 branches missed.">                            for ( Tsr&lt;?&gt; t : call.getTensors() ) {</span>
<span class="nc bnc" id="L112" title="All 4 branches missed.">                                if ( last != null &amp;&amp; !last.shape().equals(t.shape()) ) return false;</span>
<span class="nc" id="L113">                                last = t; // Note: shapes are cached!</span>
                            }
<span class="nc" id="L115">                            return true;</span>
                        }
                )
<span class="nc" id="L118">                .setSupplyADAgentFor(</span>
                        ( Function f, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call, boolean forward ) -&gt;
                        {
                            //Tsr ctxDerivative = (Tsr) call.getAt(&quot;derivative&quot;);
<span class="nc bnc" id="L122" title="All 2 branches missed.">                            if ( forward ) throw new IllegalArgumentException(&quot;Matrix multiplication of does not support forward-AD!&quot;);</span>

<span class="nc" id="L124">                            Function invX = new FunctionBuilder(Neureka.get().context()).build( &quot;I[ 0 ] @ I[ 1 ]&quot;, false );</span>
<span class="nc" id="L125">                            Tsr[] inputs = call.getTensors();</span>
<span class="nc" id="L126">                            int d = call.getDerivativeIndex();</span>
<span class="nc" id="L127">                            Tsr deriv = inputs[1+d].T();//f.derive( inputs, d );</span>
<span class="nc" id="L128">                            return new DefaultADAgent( deriv )</span>
<span class="nc" id="L129">                                    .setForward( (node, forwardDerivative ) -&gt; null )</span>
<span class="nc" id="L130">                                    .setBackward( (t, error) -&gt; invX.call(new Tsr[]{ error, deriv }) );</span>
                        }
                )
<span class="nc" id="L133">                .setHandleInsteadOfDevice(</span>
                        ( caller, call ) -&gt; {
<span class="nc bnc" id="L135" title="All 2 branches missed.">                            if ( !caller.isFlat() ) return null;</span>
<span class="nc bnc" id="L136" title="All 2 branches missed.">                            if ( call.getOperation().getOperator().equals(&quot;x&quot;) ) {</span>

<span class="nc" id="L138">                                Tsr[] inputs = call.getTensors();</span>
<span class="nc" id="L139">                                Tsr[] tsrs = new Tsr[]{null, inputs[ 0 ], inputs[ 1 ]};</span>
<span class="nc bnc" id="L140" title="All 2 branches missed.">                                tsrs[ 0 ] = (call.getDerivativeIndex() &lt; 0)</span>
<span class="nc" id="L141">                                        ? Tsr.ofShape( Tsr.Utility.Indexing.shpOfCon(tsrs[ 1 ].getNDConf().shape(), tsrs[ 2 ].getNDConf().shape()) )</span>
<span class="nc" id="L142">                                        : null;</span>

<span class="nc bnc" id="L144" title="All 4 branches missed.">                                for (Tsr t : tsrs) if (t != null) t.setIsVirtual( false );</span>
<span class="nc" id="L145">                                call.getDevice().execute((ExecutionCall&lt;Device&lt;?&gt;&gt;) call.withTensors(tsrs));</span>
<span class="nc" id="L146">                                return tsrs[ 0 ];</span>
                            } else {
<span class="nc bnc" id="L148" title="All 2 branches missed.">                                if (call.getDerivativeIndex() &lt; 0) {</span>
<span class="nc" id="L149">                                    Tsr[] tsrs = caller.srcActivation(call.getTensors(), call.getJ(), -1, 0);</span>
<span class="nc" id="L150">                                    Tsr.makeFit(tsrs, caller.isDoingAD()); // This might not fit here... (fitting should probably be a setup thing...)</span>
<span class="nc bnc" id="L151" title="All 2 branches missed.">                                    for ( Tsr t : tsrs ) t.setIsVirtual( false );</span>
<span class="nc" id="L152">                                    call.getDevice().execute(</span>
<span class="nc" id="L153">                                            ExecutionCall.builder()</span>
<span class="nc" id="L154">                                                    .device(call.getDevice())</span>
<span class="nc" id="L155">                                                    .tensors( tsrs )</span>
<span class="nc" id="L156">                                                    .derivativeIndex( 0 )</span>
<span class="nc" id="L157">                                                    .operation( call.getOperation() )</span>
<span class="nc" id="L158">                                                    .build()</span>
                                    );
<span class="nc bnc" id="L160" title="All 2 branches missed.">                                    if ( call.getOperation() == Neureka.get().context().instance(&quot;x&gt;&gt;&quot;) )</span>
<span class="nc" id="L161">                                        return tsrs[ 2 ];</span>
                                    else
<span class="nc" id="L163">                                        return tsrs[ 0 ];</span>
                                }
                            }
<span class="nc" id="L166">                            return null;</span>
                        }
                )
<span class="nc" id="L169">                .setHandleRecursivelyAccordingToArity( rja )</span>
<span class="nc" id="L170">                .setInstantiateNewTensorsForExecutionIn(</span>
                        call -&gt; {
<span class="nc" id="L172">                            Tsr[] tsrs = call.getTensors();</span>
<span class="nc" id="L173">                            Device device = call.getDevice();</span>
<span class="nc bnc" id="L174" title="All 2 branches missed.">                            if ( tsrs[ 0 ] == null ) // Creating a new tensor:</span>
                            {
<span class="nc" id="L176">                                int[] shp = Tsr.Utility.Indexing.shpOfCon(tsrs[ 1 ].getNDConf().shape(), tsrs[ 2 ].getNDConf().shape());</span>
<span class="nc" id="L177">                                Tsr output = Tsr.of( shp, 0.0 );</span>
<span class="nc" id="L178">                                output.setIsVirtual( false );</span>
                                try {
<span class="nc" id="L180">                                    device.store(output);</span>
<span class="nc" id="L181">                                } catch ( Exception e ) {</span>
<span class="nc" id="L182">                                    e.printStackTrace();</span>
<span class="nc" id="L183">                                }</span>
<span class="nc" id="L184">                                tsrs[ 0 ] = output;</span>
                            }
<span class="nc" id="L186">                            return call;</span>
                        }
                )
<span class="nc" id="L189">                .build();</span>

<span class="nc" id="L191">        setAlgorithm(</span>
                GenericAlgorithm.class,
                convolution
<span class="nc" id="L194">                        .setImplementationFor(</span>
                                HostCPU.class,
                                new HostImplementation(
                                        call -&gt;
<span class="nc" id="L198">                                                call.getDevice().getExecutor()</span>
<span class="nc" id="L199">                                                        .threaded (</span>
<span class="nc" id="L200">                                                                call.getTsrOfType( Number.class, 0 ).size(),</span>
<span class="nc bnc" id="L201" title="All 2 branches missed.">                                                                (Neureka.get().settings().indexing().isUsingArrayBasedIndexing())</span>
<span class="nc" id="L202">                                                                        ? ( start, end ) -&gt;</span>
<span class="nc" id="L203">                                                                        Convolution.convolve (</span>
<span class="nc" id="L204">                                                                                call.getTsrOfType( Number.class, 0 ), call.getTsrOfType( Number.class, 1 ), call.getTsrOfType( Number.class, 2 ),</span>
<span class="nc" id="L205">                                                                                call.getDerivativeIndex(), start, end,</span>
<span class="nc" id="L206">                                                                                convolutionCreator.create(</span>
<span class="nc" id="L207">                                                                                        call.getTensors(),</span>
                                                                                        -1//call.getDerivativeIndex()
                                                                                )
                                                                        )
<span class="nc" id="L211">                                                                        :  ( start, end ) -&gt;</span>
<span class="nc" id="L212">                                                                        Convolution.convolve (</span>
<span class="nc" id="L213">                                                                                call.getTsrOfType( Number.class, 0 ), call.getTsrOfType( Number.class, 1 ), call.getTsrOfType( Number.class, 2 ),</span>
<span class="nc" id="L214">                                                                                call.getDerivativeIndex(), start, end,</span>
<span class="nc" id="L215">                                                                                convolutionNDICreator.create(</span>
<span class="nc" id="L216">                                                                                        call.getTensors(),</span>
                                                                                        -1//call.getDerivativeIndex()
                                                                                )
                                                                        )
                                                        ),
                                        3
                                )
                        )
<span class="nc" id="L224">                        .setImplementationFor(</span>
                                OpenCLDevice.class,
<span class="nc" id="L226">                                CLImplementation.fromSource()</span>
<span class="nc" id="L227">                                        .arity( 3 )</span>
<span class="nc" id="L228">                                        .kernelName( &quot;&quot; )</span>
<span class="nc" id="L229">                                        .kernelSource(</span>
                                                &quot;_kernel void simpleMatMul(   &quot; +
                                                        &quot;   int widthA,                                     &quot; +
                                                        &quot;   int heightA,                                    &quot; +
                                                        &quot;   int widthB,                                     &quot; +
                                                        &quot;   int heightB,                                    &quot; +
                                                        &quot;   __global float* outputC,                        &quot; +
                                                        &quot;   __global float* inputA,                         &quot; +
                                                        &quot;   __global float* inputB                          &quot; +
                                                        &quot;) {                                                &quot; +
                                                        &quot;   int row = get_global_id( 1 );                     &quot; +
                                                        &quot;   int col = get_global_id(0);                     &quot; +
                                                        &quot;   float sum = 0.0f;                               &quot; +
                                                        &quot;   for ( int i = 0; i &lt; widthA; i++ ) {            &quot; +
                                                        &quot;      sum += inputA[ row * widthA + i ] * inputB[ i * widthB + col ];&quot; +
                                                        &quot;   }                                               &quot; +
                                                        &quot;   outputC[ row * widthB * col ] = sum;&quot; +
                                                        &quot;}&quot;
                                        )
<span class="nc" id="L248">                                        .build()</span>
                        )
        );


<span class="nc" id="L253">    }</span>


    @Override
    public String stringify( String[] children ) {
<span class="nc" id="L258">        StringBuilder reconstructed = new StringBuilder();</span>
<span class="nc bnc" id="L259" title="All 2 branches missed.">        for ( int i = 0; i &lt; children.length; ++i ) {</span>
<span class="nc" id="L260">            reconstructed.append( children[ i ] );</span>
<span class="nc bnc" id="L261" title="All 2 branches missed.">            if ( i &lt; children.length - 1 ) {</span>
<span class="nc" id="L262">                reconstructed.append(&quot; @ &quot;);</span>
            }
        }
<span class="nc" id="L265">        return &quot;(&quot; + reconstructed + &quot;)&quot;;</span>
    }

    @Override
    public String asDerivative( Function[] children, int derivationIndex) {
<span class="nc" id="L270">        throw new IllegalStateException(&quot;Operation does not support dynamic derivation!&quot;);</span>
    }

    @Override
    public double calculate( double[] inputs, int j, int d, Function[] src ) {
<span class="nc" id="L275">        return src[ 0 ].call( inputs, j );</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>