<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ConvUtil.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.backend.standard.operations</a> &gt; <span class="el_source">ConvUtil.java</span></div><h1>ConvUtil.java</h1><pre class="source lang-java linenums">package neureka.backend.standard.operations;

import neureka.Neureka;
import neureka.Tsr;
import neureka.autograd.ADAgent;
import neureka.backend.api.ExecutionCall;
import neureka.backend.standard.algorithms.Convolution;
import neureka.calculus.CalcUtil;
import neureka.calculus.Function;
import neureka.calculus.args.Arg;
import neureka.calculus.assembly.FunctionBuilder;
import neureka.devices.Device;

<span class="pc bpc" id="L14" title="1 of 2 branches missed.">public class ConvUtil {</span>

<span class="fc" id="L16">    private static Convolution conv = null;</span>

    public static Convolution createDeconvolutionFor( String operator ) {
<span class="fc" id="L19">        return new Convolution()</span>
<span class="fc" id="L20">                .setCanPerformBackwardADFor( call -&gt; true )</span>
<span class="fc" id="L21">                .setCanPerformForwardADFor(</span>
                        call -&gt; {
<span class="pc bpc" id="L23" title="1 of 2 branches missed.">                            if ( call.getOperation().supports( Convolution.class ) ) return false;</span>
<span class="nc bnc" id="L24" title="All 2 branches missed.">                            if ( call.getOperation().getOperator().equals(&quot;,&quot;) ) return false; //Reshape</span>
<span class="nc" id="L25">                            Tsr&lt;?&gt; last = null;</span>
<span class="nc bnc" id="L26" title="All 2 branches missed.">                            for ( Tsr&lt;?&gt; t : call.getTensors() ) {</span>
<span class="nc bnc" id="L27" title="All 4 branches missed.">                                if ( last != null &amp;&amp; !last.shape().equals(t.shape()) ) return false;</span>
<span class="nc" id="L28">                                last = t; // Note: shapes are cached!</span>
                            }
<span class="nc" id="L30">                            return true;</span>
                        }
                )
<span class="fc" id="L33">                .setSupplyADAgentFor(</span>
                        ( Function f, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call, boolean forward ) -&gt;
                        {
<span class="fc" id="L36">                            Tsr&lt;?&gt; ctxDerivative = (Tsr&lt;?&gt;) call.getValOf(Arg.Derivative.class);</span>
<span class="fc bfc" id="L37" title="All 2 branches covered.">                            if ( forward )</span>
<span class="fc" id="L38">                                throw new IllegalArgumentException(&quot;Convolution does not support forward-AD!&quot;);</span>

<span class="fc" id="L40">                            Function mul = Neureka.get().backend().getFunction().mul();</span>
<span class="fc" id="L41">                            Tsr[] inputs = call.getTensors();</span>
<span class="fc" id="L42">                            int d = call.getDerivativeIndex();</span>

<span class="fc" id="L44">                            Function deConv = new FunctionBuilder( Neureka.get().backend() ).build(</span>
                                    &quot;I[ 0 ]&quot; + operator + &quot;&gt;&gt;I[ 1 ]&quot; + operator + &quot;&gt;&gt;I[ 2 ]&quot;,
                                    false
                            );
<span class="fc" id="L48">                            Tsr&lt;?&gt; derivative = f.derive( inputs, d );</span>
<span class="pc bpc" id="L49" title="3 of 6 branches missed.">                            assert d &gt;= 0 &amp;&amp; d &lt;= 1;</span>
<span class="pc bpc" id="L50" title="2 of 4 branches missed.">                            assert mul != null;</span>
<span class="pc bpc" id="L51" title="2 of 4 branches missed.">                            assert derivative != null;</span>
<span class="pc bpc" id="L52" title="2 of 4 branches missed.">                            assert deConv != null;</span>
<span class="pc bpc" id="L53" title="3 of 6 branches missed.">                            assert inputs.length &gt;= 2 &amp;&amp; inputs.length &lt;= 3;</span>
                            // Now we need to remember the shape of the input which is targeted for back prop.
<span class="fc bfc" id="L55" title="All 2 branches covered.">                            int[] shape = inputs[ inputs.length &gt; 2 ? d + 1 : d ].getNDConf().shape();</span>
                            // This is because it will be the shape of the output to the de-convolution!
<span class="fc" id="L57">                            return ADAgent.of( derivative )</span>
<span class="fc" id="L58">                                    .setForward(</span>
<span class="nc" id="L59">                                        (node, forwardDerivative ) -&gt; mul.execute( forwardDerivative, derivative )</span>
                                    )
<span class="fc" id="L61">                                    .setBackward(</span>
<span class="fc" id="L62">                                        (node, error) -&gt; deConv.execute( error, derivative, Tsr.of(shape, 0) )</span>
                                    );
                        }
                )
<span class="fc" id="L66">                .setExecutionDispatcher(</span>
                        ( caller, call ) -&gt; {
<span class="fc bfc" id="L68" title="All 2 branches covered.">                            if ( !caller.isFlat() ) return CalcUtil.defaultRecursiveExecution( caller, call );</span>
<span class="fc bfc" id="L69" title="All 2 branches covered.">                            if ( call.getOperation().getOperator().equals(&quot;x&quot;) ) {</span>

<span class="fc" id="L71">                                Tsr&lt;?&gt;[] inputs = call.getTensors();</span>
<span class="fc" id="L72">                                Tsr&lt;?&gt;[] tsrs = new Tsr[]{null, inputs[ 0 ], inputs[ 1 ]};</span>
<span class="fc bfc" id="L73" title="All 2 branches covered.">                                tsrs[ 0 ] = (call.getValOf( Arg.DerivIdx.class ) &lt; 0)</span>
<span class="fc" id="L74">                                        ? Tsr.ofShape(Tsr.Utility.Indexing.shpOfCon(tsrs[ 1 ].getNDConf().shape(), tsrs[ 2 ].getNDConf().shape()))</span>
<span class="fc" id="L75">                                        : null;</span>

<span class="fc bfc" id="L77" title="All 4 branches covered.">                                for (Tsr&lt;?&gt; t : tsrs) if ( t != null ) t.setIsVirtual( false );</span>
<span class="fc" id="L78">                                CalcUtil.recursiveExecution( call.withTensors(tsrs), JunctionUtil::forConvolution );</span>
<span class="fc" id="L79">                                return tsrs[ 0 ];</span>
                            } else {
<span class="pc bpc" id="L81" title="1 of 2 branches missed.">                                if ( call.getValOf( Arg.DerivIdx.class ) &lt; 0 ) {</span>
<span class="fc" id="L82">                                    Tsr&lt;?&gt;[] tsrs = CalcUtil.srcActivation(call.getTensors(), call.getJ(), -1, 0, caller.getSubFunctions().toArray(new Function[0]));</span>
<span class="fc" id="L83">                                    Tsr.makeFit(tsrs, caller.isDoingAD()); // This might not fit here... (fitting should probably be a setup thing...)</span>
<span class="fc bfc" id="L84" title="All 2 branches covered.">                                    for ( Tsr&lt;?&gt; t : tsrs ) t.setIsVirtual( false );</span>
<span class="fc" id="L85">                                    CalcUtil.recursiveExecution(</span>
<span class="fc" id="L86">                                                    ExecutionCall.of(tsrs)</span>
<span class="fc" id="L87">                                                                    .andArgs(Arg.DerivIdx.of(0))</span>
<span class="fc" id="L88">                                                                    .running(call.getOperation())</span>
<span class="fc" id="L89">                                                                    .on(call.getDevice()),</span>
                                                    JunctionUtil::forConvolution
                                                );
<span class="fc bfc" id="L92" title="All 2 branches covered.">                                    if ( call.getOperation() == Neureka.get().backend().getOperation(&quot;x&gt;&gt;&quot;) )</span>
<span class="fc" id="L93">                                        return tsrs[ 2 ];</span>
                                    else
<span class="fc" id="L95">                                        return tsrs[ 0 ];</span>
                                }
                            }
<span class="nc" id="L98">                            return CalcUtil.defaultRecursiveExecution( caller, call );</span>
                        }
                )
<span class="fc" id="L101">                .setCallPreparation(</span>
                        call -&gt; {
<span class="fc" id="L103">                            Tsr&lt;?&gt;[] tensors = call.getTensors();</span>
<span class="fc" id="L104">                            Device&lt;Number&gt; device = call.getDeviceFor(Number.class);</span>
<span class="fc bfc" id="L105" title="All 2 branches covered.">                            if ( tensors[ 0 ] == null ) // Creating a new tensor:</span>
                            {
<span class="fc" id="L107">                                int[] shp = Tsr.Utility.Indexing.shpOfCon(tensors[ 1 ].getNDConf().shape(), tensors[ 2 ].getNDConf().shape());</span>
<span class="fc" id="L108">                                Tsr&lt;Double&gt; output = Tsr.of( shp, 0.0 );</span>
<span class="fc" id="L109">                                output.setIsVirtual( false );</span>
                                try {
<span class="fc" id="L111">                                    device.store( output );</span>
<span class="nc" id="L112">                                } catch ( Exception e ) {</span>
<span class="nc" id="L113">                                    e.printStackTrace();</span>
<span class="fc" id="L114">                                }</span>
<span class="fc" id="L115">                                tensors[ 0 ] = output;</span>
                            }
<span class="fc" id="L117">                            return call;</span>
                        }
                )
<span class="fc" id="L120">                .buildFunAlgorithm();</span>
    }

    public static Convolution getConv() {
<span class="fc bfc" id="L124" title="All 2 branches covered.">        if ( conv == null ) conv = createDeconvolutionFor(&quot;x&quot;);</span>
<span class="fc" id="L125">        return ConvUtil.conv;</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>