<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ConvUtil.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.backend.standard.operations</a> &gt; <span class="el_source">ConvUtil.java</span></div><h1>ConvUtil.java</h1><pre class="source lang-java linenums">package neureka.backend.standard.operations;

import neureka.Neureka;
import neureka.Tsr;
import neureka.autograd.ADAgent;
import neureka.backend.api.ExecutionCall;
import neureka.backend.standard.algorithms.Convolution;
import neureka.backend.standard.operations.other.Reshape;
import neureka.calculus.internal.CalcUtil;
import neureka.calculus.Function;
import neureka.calculus.args.Arg;
import neureka.calculus.assembly.FunctionBuilder;
import neureka.devices.Device;
import org.jetbrains.annotations.Contract;

<span class="pc bpc" id="L16" title="1 of 2 branches missed.">public class ConvUtil {</span>

    /**
     *  There will always only be a single convolution instance
     *  shared among all 3 convolution operations.
     */
<span class="fc" id="L22">    private static Convolution conv = null;</span>

    public static Convolution createDeconvolutionFor( String operator ) {
<span class="fc" id="L25">        return new Convolution()</span>
<span class="fc" id="L26">                .setCanPerformBackwardADFor( call -&gt; true )</span>
<span class="fc" id="L27">                .setCanPerformForwardADFor(</span>
                    call -&gt; {
<span class="pc bpc" id="L29" title="1 of 2 branches missed.">                        if ( call.getOperation().supports( Convolution.class ) ) return false;</span>
<span class="nc bnc" id="L30" title="All 2 branches missed.">                        if ( call.getOperation().getOperator().equals(&quot;,&quot;) ) return false; //Reshape</span>
<span class="nc" id="L31">                        Tsr&lt;?&gt; last = null;</span>
<span class="nc bnc" id="L32" title="All 2 branches missed.">                        for ( Tsr&lt;?&gt; t : call.getTensors() ) {</span>
<span class="nc bnc" id="L33" title="All 4 branches missed.">                            if ( last != null &amp;&amp; !last.shape().equals(t.shape()) ) return false;</span>
<span class="nc" id="L34">                            last = t; // Note: shapes are cached!</span>
                        }
<span class="nc" id="L36">                        return true;</span>
                    }
                )
<span class="fc" id="L39">                .setSupplyADAgentFor(</span>
                    ( Function f, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call, boolean forward ) -&gt;
                    {
<span class="fc" id="L42">                        Tsr&lt;?&gt; ctxDerivative = (Tsr&lt;?&gt;) call.getValOf(Arg.Derivative.class);</span>
<span class="fc bfc" id="L43" title="All 2 branches covered.">                        if ( forward )</span>
<span class="fc" id="L44">                            throw new IllegalArgumentException(&quot;Convolution does not support forward-AD!&quot;);</span>

<span class="fc" id="L46">                        Tsr[] inputs = call.getTensors();</span>
<span class="fc" id="L47">                        int d = call.getDerivativeIndex();</span>

<span class="fc" id="L49">                        Function deConv = new FunctionBuilder( Neureka.get().backend() ).build(</span>
                                &quot;I[ 0 ]&quot; + operator + &quot;&gt;&gt;I[ 1 ]&quot; + operator + &quot;&gt;&gt;I[ 2 ]&quot;,
                                false
                        );
<span class="fc" id="L53">                        Tsr&lt;?&gt; derivative = f.derive( inputs, d );</span>
<span class="pc bpc" id="L54" title="3 of 6 branches missed.">                        assert d &gt;= 0 &amp;&amp; d &lt;= 1;</span>
<span class="pc bpc" id="L55" title="2 of 4 branches missed.">                        assert derivative != null;</span>
<span class="pc bpc" id="L56" title="2 of 4 branches missed.">                        assert deConv != null;</span>
<span class="pc bpc" id="L57" title="3 of 6 branches missed.">                        assert inputs.length &gt;= 2 &amp;&amp; inputs.length &lt;= 3;</span>
                        // Now we need to remember the shape of the input which is targeted for back prop.
<span class="fc bfc" id="L59" title="All 2 branches covered.">                        int[] shape = inputs[ inputs.length &gt; 2 ? d + 1 : d ].getNDConf().shape();</span>
                        // This is because it will be the shape of the output to the de-convolution!
<span class="fc" id="L61">                        return ADAgent.of( derivative )</span>
<span class="fc" id="L62">                                .setForward( null )</span>
<span class="fc" id="L63">                                .setBackward(</span>
                                    (node, error) -&gt;
<span class="fc" id="L65">                                            deConv.execute(</span>
                                                    error,
                                                    derivative,
<span class="fc" id="L68">                                                    Tsr.of(shape, 0).getUnsafe().setIsIntermediate( false )</span>
                                            )
                                );
                    }
                )
<span class="fc" id="L73">                .setExecutionDispatcher(</span>
                    ( caller, call ) -&gt; {
<span class="fc bfc" id="L75" title="All 2 branches covered.">                        if ( !caller.isFlat() ) return CalcUtil.defaultRecursiveExecution( caller, call );</span>
<span class="fc bfc" id="L76" title="All 2 branches covered.">                        if ( call.getOperation().getOperator().equals(&quot;x&quot;) ) {</span>

<span class="fc" id="L78">                            Tsr&lt;?&gt;[] inputs = call.getTensors();</span>
<span class="fc" id="L79">                            Tsr&lt;?&gt;[] tensors = new Tsr[]{null, inputs[ 0 ], inputs[ 1 ]};</span>
<span class="fc" id="L80">                            tensors[ 0 ] =</span>
<span class="fc bfc" id="L81" title="All 2 branches covered.">                                (call.getValOf( Arg.DerivIdx.class ) &lt; 0)</span>
<span class="fc" id="L82">                                    ? Tsr.of(</span>
<span class="fc" id="L83">                                            inputs[0].getValueClass(),</span>
<span class="fc" id="L84">                                            _shpOfCon(tensors[ 1 ].getNDConf().shape(), tensors[ 2 ].getNDConf().shape()),</span>
<span class="fc" id="L85">                                            0</span>
                                        )
<span class="fc" id="L87">                                        .getUnsafe()</span>
<span class="fc" id="L88">                                        .setIsIntermediate( true )</span>
<span class="fc" id="L89">                                    : null;</span>

<span class="fc bfc" id="L91" title="All 4 branches covered.">                            for ( Tsr&lt;?&gt; t : tensors ) if ( t != null ) t.setIsVirtual( false );</span>
<span class="fc" id="L92">                            CalcUtil.recursiveExecution( call.withTensors(tensors), JunctionUtil::forConvolution );</span>
<span class="pc bpc" id="L93" title="1 of 2 branches missed.">                            if (tensors[ 0 ] == null)</span>
<span class="nc" id="L94">                                throw new IllegalStateException(&quot;Failed to execute convolution!&quot;);</span>
<span class="fc" id="L95">                            return tensors[ 0 ];</span>
                        } else {
<span class="pc bpc" id="L97" title="1 of 2 branches missed.">                            if ( call.getValOf( Arg.DerivIdx.class ) &lt; 0 ) {</span>
<span class="fc" id="L98">                                Tsr&lt;?&gt;[] tsrs = CalcUtil.srcActivation(call.getTensors(), call.getJ(), -1, 0, caller.getSubFunctions().toArray(new Function[0]));</span>
<span class="fc" id="L99">                                Reshape.makeFit(tsrs, caller.isDoingAD()); // This might not fit here... (fitting should probably be a setup thing...)</span>
<span class="fc bfc" id="L100" title="All 2 branches covered.">                                for ( Tsr&lt;?&gt; t : tsrs ) t.setIsVirtual( false );</span>
<span class="fc" id="L101">                                CalcUtil.recursiveExecution(</span>
<span class="fc" id="L102">                                                ExecutionCall.of(tsrs)</span>
<span class="fc" id="L103">                                                                .andArgs(Arg.DerivIdx.of(0))</span>
<span class="fc" id="L104">                                                                .running(call.getOperation())</span>
<span class="fc" id="L105">                                                                .on(call.getDevice()),</span>
                                                JunctionUtil::forConvolution
                                            );
<span class="fc bfc" id="L108" title="All 2 branches covered.">                                if ( call.getOperation() == Neureka.get().backend().getOperation(&quot;x&gt;&gt;&quot;) )</span>
<span class="fc" id="L109">                                    return tsrs[ 2 ];</span>
                                else
<span class="fc" id="L111">                                    return tsrs[ 0 ];</span>
                            }
                        }
<span class="nc" id="L114">                        return CalcUtil.defaultRecursiveExecution( caller, call );</span>
                    }
                )
<span class="fc" id="L117">                .setCallPreparation(</span>
                     call -&gt; {
<span class="fc" id="L119">                         Tsr&lt;?&gt;[] tensors = call.getTensors();</span>
<span class="fc" id="L120">                         Device&lt;Number&gt; device = call.getDeviceFor(Number.class);</span>
<span class="fc bfc" id="L121" title="All 2 branches covered.">                         if ( tensors[ 0 ] == null ) // Creating a new tensor:</span>
                         {
<span class="fc" id="L123">                             int[] shp = _shpOfCon(tensors[ 1 ].getNDConf().shape(), tensors[ 2 ].getNDConf().shape());</span>
<span class="fc" id="L124">                             Tsr&lt;Double&gt; output = Tsr.of( shp, 0.0 ).getUnsafe().setIsIntermediate( true );</span>
<span class="fc" id="L125">                             output.setIsVirtual( false );</span>
                             try {
<span class="fc" id="L127">                                 device.store( output );</span>
<span class="nc" id="L128">                             } catch ( Exception e ) {</span>
<span class="nc" id="L129">                                 e.printStackTrace();</span>
<span class="fc" id="L130">                             }</span>
<span class="fc" id="L131">                             tensors[ 0 ] = output;</span>
                         }
<span class="fc" id="L133">                         return call;</span>
                     }
                )
<span class="fc" id="L136">                .buildFunAlgorithm();</span>
    }

    public static Convolution getConv() {
<span class="fc bfc" id="L140" title="All 2 branches covered.">        if ( conv == null )</span>
<span class="fc" id="L141">            conv = createDeconvolutionFor(&quot;x&quot;);</span>
<span class="fc" id="L142">        return ConvUtil.conv;</span>
    }

    @Contract(pure = true)
    private static int[] _shpOfCon( int[] shp1, int[] shp2 ) {
<span class="fc" id="L147">        int[] shape = new int[ ( shp1.length + shp2.length ) / 2 ];</span>
<span class="pc bpc" id="L148" title="1 of 4 branches missed.">        for ( int i = 0; i &lt; shp1.length &amp;&amp; i &lt; shp2.length; i++ )</span>
<span class="fc" id="L149">            shape[ i ] = Math.abs( shp1[ i ] - shp2[ i ] ) + 1;</span>
<span class="fc" id="L150">        return shape;</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>