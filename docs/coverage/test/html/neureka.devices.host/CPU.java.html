<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>CPU.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.devices.host</a> &gt; <span class="el_source">CPU.java</span></div><h1>CPU.java</h1><pre class="source lang-java linenums">package neureka.devices.host;

import neureka.Neureka;
import neureka.Tsr;
import neureka.backend.api.Operation;
import neureka.calculus.Function;
import neureka.devices.AbstractDevice;
import neureka.devices.Device;
import neureka.devices.host.concurrent.Parallelism;
import neureka.devices.host.concurrent.WorkScheduler;
import neureka.devices.host.machine.ConcreteMachine;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Collection;
import java.util.Collections;
import java.util.Set;
import java.util.WeakHashMap;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.function.IntSupplier;

/**
 *  The CPU class, one of many implementations of the {@link Device} interface,
 *  is simply supposed to be an API for dispatching threaded workloads onto the CPU.
 *  Contrary to other types of devices, the CPU will host tensor data by default, simply
 *  because the tensors will be stored in RAM (JVM heap) by default if no device was specified.
 *  This means that they are implicitly &quot;stored&quot; on the {@link CPU} device.
 *  The class is also a singleton instead of being part of a {@link neureka.backend.api.BackendExtension}.
 */
public class CPU extends AbstractDevice&lt;Number&gt;
{
<span class="fc" id="L33">    private static final Logger _LOG = LoggerFactory.getLogger( CPU.class );</span>
    private static final CPU _INSTANCE;

    private static final WorkScheduler.Divider _DIVIDER;
    private static final IntSupplier _PARALLELISM;

<span class="fc" id="L39">    public static int PARALLELIZATION_THRESHOLD = 32;</span>

    static {
<span class="fc" id="L42">        _INSTANCE = new CPU();</span>
<span class="fc" id="L43">        _DIVIDER = new WorkScheduler.Divider(CPU.get().getExecutor().getPool());</span>
<span class="fc" id="L44">        _PARALLELISM = Parallelism.THREADS;</span>
<span class="fc" id="L45">    }</span>

<span class="fc" id="L47">    private final JVMExecutor _executor = new JVMExecutor();</span>
<span class="fc" id="L48">    private final Set&lt;Tsr&lt;Number&gt;&gt; _tensors = Collections.newSetFromMap(new WeakHashMap&lt;&gt;());</span>

<span class="fc" id="L50">    private CPU() { super(); }</span>

    /**
     *  Use this method to access the singleton instance of this {@link CPU} class,
     *  which is a {@link Device} type and default location for freshly instantiated {@link Tsr} instances.
     *  {@link Tsr} instances located on the {@link CPU} device will reside in regular RAM
     *  causing operations to run on the JVM and thereby the CPU.
     *
     * @return The singleton instance of this {@link CPU} class.
     */
<span class="fc" id="L60">    public static CPU get() { return _INSTANCE; }</span>

    /**
     *  The {@link JVMExecutor} offers a similar functionality as the parallel stream API,
     *  however it differs in that the {@link JVMExecutor} is processing {@link RangeWorkload} lambdas
     *  instead of simply exposing a single index or concrete elements for a given workload size.
     *
     * @return A parallel range based execution API running on the JVM.
     */
<span class="fc" id="L69">    public JVMExecutor getExecutor() { return _executor; }</span>

    @Override
<span class="fc" id="L72">    protected boolean _approveExecutionOf( Tsr&lt;?&gt;[] tensors, int d, Operation operation ) { return true; }</span>

    /**
     *  This method will shut down the internal thread-pool used by this
     *  class to execute JVM/CPU based operations in parallel.
     */
    @Override
    public void dispose() {
<span class="nc" id="L80">        _executor.getPool().shutdown();</span>
<span class="nc" id="L81">        _tensors.clear();</span>
<span class="nc" id="L82">        _LOG.warn(</span>
<span class="nc" id="L83">                &quot;Main thread pool in '&quot;+this.getClass()+&quot;' shutting down! &quot; +</span>
                &quot;Newly incoming operations will not be executed in parallel.&quot;
        );
<span class="nc" id="L86">    }</span>

    @Override
<span class="nc" id="L89">    public &lt;T extends Number&gt; Device&lt;Number&gt; write( Tsr&lt;T&gt; tensor, Object value ) { return this; }</span>

    @Override
<span class="nc" id="L92">    public &lt;T extends Number&gt; Object valueFor( Tsr&lt;T&gt; tensor ) { return tensor.getValue(); }</span>

    @Override
<span class="fc" id="L95">    public &lt;T extends Number&gt; Number valueFor( Tsr&lt;T&gt; tensor, int index ) { return tensor.getValueAt( index ); }</span>

    @Override
<span class="fc" id="L98">    public CPU restore( Tsr&lt;Number&gt; tensor ) { return this; }</span>

    @Override
    public &lt;T extends Number&gt; CPU store( Tsr&lt;T&gt; tensor ) {
        //super.store(tensor);
<span class="fc" id="L103">        _tensors.add( (Tsr&lt;Number&gt;) tensor);</span>
<span class="fc" id="L104">        return this;</span>
    }

    @Override
    public &lt;T extends Number&gt; CPU store( Tsr&lt;T&gt; tensor, Tsr&lt;T&gt; parent ) {
<span class="nc" id="L109">        _tensors.add( (Tsr&lt;Number&gt;) tensor);</span>
<span class="nc" id="L110">        _tensors.add( (Tsr&lt;Number&gt;) parent);</span>
<span class="nc" id="L111">        return this;</span>
    }

    @Override
<span class="fc" id="L115">    public &lt;T extends Number&gt; boolean has( Tsr&lt;T&gt; tensor ) { return _tensors.contains( tensor ); }</span>

    @Override
    public &lt;T extends Number&gt; CPU free( Tsr&lt;T&gt; tensor ) {
<span class="fc" id="L119">        _tensors.remove( tensor );</span>
<span class="fc" id="L120">        return this;</span>
    }

    @Override
<span class="nc" id="L124">    public &lt;T extends Number&gt; CPU swap( Tsr&lt;T&gt; former, Tsr&lt;T&gt; replacement ) { return this; }</span>

    @Override
    public &lt;T extends Number&gt; Device&lt;Number&gt; updateNDConf( Tsr&lt;T&gt; tensor ) {
<span class="nc" id="L128">        return this;</span>
    }

    @Override
<span class="fc" id="L132">    public Collection&lt;Tsr&lt;Number&gt;&gt; getTensors() { return _tensors; }</span>

    @Override
<span class="nc" id="L135">    public Operation optimizedOperationOf( Function function, String name ) { throw new IllegalStateException(); }</span>

    /**
     *  This method is part of the component system built into the {@link Tsr} class.
     *  Do not use this as part of anything but said component system.
     *
     * @param changeRequest An API which describes the type of update and a method for executing said update.
     * @return The truth value determining if this {@link Device} ought to be added to a tensor (Here always false!).
     */
    @Override
    public boolean update( OwnerChangeRequest&lt;Tsr&lt;Number&gt;&gt; changeRequest ) {
<span class="fc" id="L146">        super.update( changeRequest );</span>
<span class="fc" id="L147">        return false; // This type of device can not be a component simply because it is the default device</span>
    }

    /**
     * Returns the number of CPU cores available to the Java virtual machine.
     * This value may change during a particular invocation of the virtual machine.
     * Applications that are sensitive to the number of available processors should
     * therefore occasionally poll this property and adjust their resource usage appropriately.
     *
     * @return The maximum number of CPU cores available to the JVM.
     *         This number is never smaller than one!
     */
<span class="fc" id="L159">    public int getCoreCount() { return Runtime.getRuntime().availableProcessors(); }</span>

    @Override
    public String toString() {
<span class="fc" id="L163">        return this.getClass().getSimpleName()+&quot;[coreCount=&quot;+getCoreCount()+&quot;]&quot;;</span>
    }

    /**
     *  A simple functional interface for executing a range whose implementations will
     *  either be executed sequentially or they are being dispatched to
     *  a thread-pool, given that the provided workload is large enough.
     */
    @FunctionalInterface
    public interface RangeWorkload {
        void execute( int start, int end );
    }

    /**
     *  Takes the provided range and divides it into multi-threaded workloads.
     */
    public void divide(
            final int first,
            final int limit,
            final WorkScheduler.Worker worker
    ) {
<span class="fc" id="L184">        _DIVIDER.parallelism( _PARALLELISM )</span>
<span class="fc" id="L185">                .threshold( PARALLELIZATION_THRESHOLD )</span>
<span class="fc" id="L186">                .divide( first, limit, worker );</span>
<span class="fc" id="L187">    }</span>


    /**
     *  The {@link JVMExecutor} offers a similar functionality as the parallel stream API,
     *  however it differs in that the {@link JVMExecutor} is processing {@link RangeWorkload} lambdas
     *  instead of simply exposing a single index or concrete elements for a given workload size.
     *  This means that a {@link RangeWorkload} lambda will be called with the work range of a single worker thread
     *  processing its current workload.
     *  This range is dependent on the number of available threads as well as the size of the workload.
     *  If the workload is very small, then the current main thread will process the entire workload range
     *  whereas the underlying {@link ThreadPoolExecutor} will not be used to avoid unnecessary overhead.
     */
<span class="fc" id="L200">    public static class JVMExecutor</span>
    {
<span class="fc" id="L202">        private static final AtomicInteger _COUNTER = new AtomicInteger();</span>
<span class="fc" id="L203">        private static final ThreadGroup   _GROUP   = new ThreadGroup(&quot;neureka-daemon-group&quot;);</span>

        /*
            The following 2 constants determine if any given workload size will be parallelize or not...
            We might want to adjust this some more for better performance...
         */
        private static final int _MIN_THREADED_WORKLOAD_SIZE = 32;
        private static final int _MIN_WORKLOAD_PER_THREAD    = 8;

<span class="fc" id="L212">        private final ThreadPoolExecutor _pool =</span>
                                            new ThreadPoolExecutor(
                                                    ConcreteMachine.ENVIRONMENT.units,
                                                    Integer.MAX_VALUE,
                                                    5L,
                                                    TimeUnit.SECONDS,
                                                    new SynchronousQueue&lt;Runnable&gt;(),
<span class="fc" id="L219">                                                    _newThreadFactory(&quot;neureka-daemon-&quot;)</span>
                                            );

        private static ThreadFactory _newThreadFactory( final String name ) {
<span class="fc" id="L223">            return _newThreadFactory( _GROUP, name );</span>
        }

        private static ThreadFactory _newThreadFactory( final ThreadGroup group, final String name) {

<span class="pc bpc" id="L228" title="1 of 2 branches missed.">            String prefix = name.endsWith(&quot;-&quot;) ? name : name + &quot;-&quot;;</span>

<span class="fc" id="L230">            return target -&gt; {</span>
<span class="fc" id="L231">                Thread thread = new Thread(group, target, prefix + _COUNTER.incrementAndGet());</span>
<span class="fc" id="L232">                thread.setDaemon(true);</span>
<span class="fc" id="L233">                return thread;</span>
            };
        }

<span class="fc" id="L237">        public ThreadPoolExecutor getPool() { return _pool; }</span>

        /**
         *  This method slices the provided workload size into multiple ranges which can be executed in parallel.
         *
         * @param workloadSize The total workload size which ought to be split into multiple ranges.
         * @param workload The range lambda which ought to be executed across multiple threads.
         */
        public void threaded( int workloadSize, RangeWorkload workload )
        {
<span class="fc" id="L247">            int cores = _pool.getCorePoolSize() - _pool.getActiveCount();</span>
<span class="pc bpc" id="L248" title="1 of 2 branches missed.">            cores = ( cores == 0 ) ? 1 : cores;</span>
<span class="pc bpc" id="L249" title="1 of 4 branches missed.">            if ( workloadSize &gt;= _MIN_THREADED_WORKLOAD_SIZE &amp;&amp; ( ( workloadSize / cores ) &gt;= _MIN_WORKLOAD_PER_THREAD) ) {</span>
<span class="fc" id="L250">                final int chunk = workloadSize / cores;</span>
<span class="fc" id="L251">                Future&lt;?&gt;[] futures = new Future[ cores ];</span>
<span class="fc bfc" id="L252" title="All 2 branches covered.">                for ( int i = 0; i &lt; cores; i++ ) {</span>
<span class="fc" id="L253">                    final int start = i * chunk;</span>
<span class="fc bfc" id="L254" title="All 2 branches covered.">                    final int end = ( i == cores - 1 ) ? workloadSize : ( (i + 1) * chunk );</span>
<span class="fc" id="L255">                    Neureka neureka = Neureka.get();</span>
<span class="fc" id="L256">                    futures[ i ] = _pool.submit(() -&gt; {</span>
<span class="fc" id="L257">                        Neureka.set( neureka ); // This ensures that the threads in the pool have the same settings!</span>
<span class="fc" id="L258">                        workload.execute( start, end );</span>
<span class="fc" id="L259">                    });</span>
                }
<span class="fc bfc" id="L261" title="All 2 branches covered.">                for ( Future&lt;?&gt; f : futures ) {</span>
                    try {
<span class="fc" id="L263">                        f.get(); // Return value is null because we submitted merely a simple Runnable</span>
<span class="nc" id="L264">                    } catch ( InterruptedException | ExecutionException e ) {</span>
<span class="nc" id="L265">                        e.printStackTrace();</span>
<span class="fc" id="L266">                    }</span>
                }
<span class="fc" id="L268">            }</span>
<span class="fc" id="L269">            else sequential( workloadSize, workload );</span>
<span class="fc" id="L270">        }</span>

        /**
         *  This method will simply execute the provided {@link RangeWorkload} lambda sequentially
         *  with 0 as the start index and {@code workloadSize} as the exclusive range.       &lt;br&gt;&lt;br&gt;
         *
         * @param workloadSize The workload size which will be passed to the provided {@link RangeWorkload} as second argument.
         * @param workload The {@link RangeWorkload} which will be executed sequentially.
         */
        public void sequential( int workloadSize, RangeWorkload workload ) {
<span class="fc" id="L280">            workload.execute( 0, workloadSize );</span>
<span class="fc" id="L281">        }</span>

    }

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>