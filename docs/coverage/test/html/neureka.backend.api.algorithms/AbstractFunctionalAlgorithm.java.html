<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>AbstractFunctionalAlgorithm.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka.backend.api.algorithms</a> &gt; <span class="el_source">AbstractFunctionalAlgorithm.java</span></div><h1>AbstractFunctionalAlgorithm.java</h1><pre class="source lang-java linenums">package neureka.backend.api.algorithms;


import neureka.Tsr;
import neureka.autograd.ADAgent;
import neureka.backend.api.Algorithm;
import neureka.backend.api.ExecutionCall;
import neureka.backend.api.Operation;
import neureka.backend.api.algorithms.fun.*;
import neureka.backend.standard.memory.MemValidator;
import neureka.calculus.Function;
import neureka.calculus.internal.RecursiveExecutor;
import neureka.calculus.implementations.FunctionNode;
import neureka.devices.Device;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Arrays;

/**
 *  This is the base class for implementations of the {@link Algorithm} interface.
 *  The class implements a basic component system, as is implicitly expected by said interface.
 *  Additionally, it contains useful methods used to process passed arguments of {@link ExecutionCall}
 *  as well as an implementation of the {@link Algorithm} interface which allows its methods to
 *  be implemented in a functional programming style, meaning that instances of concrete implementations
 *  extending this abstract class have setters for lambdas representing the {@link Algorithm} methods.
 *  It is being used by the standard backend of Neureka as abstract base class for various algorithms.
 *
 *  Conceptually an implementation of the {@link Algorithm} interface represents &quot;a sub-kind of operation&quot; for
 *  an instance of an implementation of the {@link Operation} interface. &lt;br&gt;
 *  The &quot;+&quot; operator for example has different {@link Algorithm} instances tailored to specific requirements
 *  originating from different {@link ExecutionCall} instances with unique arguments.
 *  {@link Tsr} instances within an execution call having the same shape would
 *  cause the {@link Operation} instance to chose an {@link Algorithm} instance which is responsible
 *  for performing elementwise operations, whereas otherwise the {@link neureka.backend.standard.algorithms.Broadcast}
 *  algorithm might be called to perform the operation.
 *
 * @param &lt;C&gt; The final type extending this class.
 */
public abstract class AbstractFunctionalAlgorithm&lt;C extends Algorithm&lt;C&gt;&gt; extends AbstractBaseAlgorithm&lt;C&gt;
{
<span class="fc" id="L42">    private static final Logger _LOG = LoggerFactory.getLogger( AbstractFunctionalAlgorithm.class );</span>
    /*
        Consider the following lambdas as effectively immutable because this
        class will warn us if any field variable is set for a second time.
        This makes the backend somewhat hackable, but also manageable with respect to complexity.
     */
    private SuitabilityPredicate _isSuitableFor;
    private ForwardADPredicate   _canPerformForwardADFor;
    private BackwardADPredicate  _canPerformBackwardADFor;
    private ADAgentSupplier      _supplyADAgentFor;
    private ExecutionDispatcher  _handleInsteadOfDevice;
    private ExecutionPreparation _instantiateNewTensorsForExecutionIn;
    /*
        This flag will ensure that we can warn the user that the state has been illegally modified.
     */
<span class="fc" id="L57">    private boolean _isFullyBuilt = false;</span>


<span class="fc" id="L60">    public AbstractFunctionalAlgorithm( String name ) { super(name); }</span>

    /**
     *  The {@link SuitabilityPredicate} checks if a given instance of an {@link ExecutionCall} is
     *  suitable to be executed in {@link neureka.backend.api.ImplementationFor}
     *  residing in this {@link Algorithm} as components.
     *  It can be implemented as s simple lambda.
     */
    @Override
    public float isSuitableFor( ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call ) {
<span class="fc" id="L70">        _checkReadiness();</span>
<span class="fc" id="L71">        return _isSuitableFor.isSuitableFor(call);</span>
    }

    /**
     *  The {@link ForwardADPredicate} lambda implemented by this method checks if forward mode auto differentiation
     *  can be performed for a given {@link ExecutionCall}.
     *  The analyzer returns a boolean truth value.
     */
    @Override
    public boolean canPerformForwardADFor( ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call ) {
<span class="fc" id="L81">        _checkReadiness();</span>
<span class="fc" id="L82">        return _canPerformForwardADFor.canPerformForwardADFor(call);</span>
    }

    /**
     *  A {@link BackwardADPredicate} lambda checks if backward mode auto differentiation
     *  (also known as back-propagation) can be performed for a given {@link ExecutionCall}.
     *  The analyzer returns a boolean truth value.
     */
    @Override
    public boolean canPerformBackwardADFor( ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call ) {
<span class="fc" id="L92">        _checkReadiness();</span>
<span class="fc" id="L93">        return _canPerformBackwardADFor.canPerformBackwardADFor( call );</span>
    }

    /**
     *  This method returns a new instance
     *  of the {@link ADAgent} class responsible for performing automatic differentiation
     *  both for forward and backward mode differentiation. &lt;br&gt;
     *  Therefore an {@link ADAgent} exposes 2 different procedures. &lt;br&gt;
     *  One is the forward mode differentiation, and the other one &lt;br&gt;
     *  is the backward mode differentiation which is more commonly known as back-propagation... &lt;br&gt;
     *  Besides that it may also contain context information used &lt;br&gt;
     *  to perform said procedures.
     */
    @Override
    public ADAgent supplyADAgentFor(Function function, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call, boolean forward ) {
<span class="fc" id="L108">        _checkReadiness();</span>
<span class="fc" id="L109">        return _supplyADAgentFor.supplyADAgentFor(function, call, forward );</span>
    }

    /**
     *  This method implements the {@link ExecutionDispatcher} lambda which
     *  is the final execution procedure responsible for electing an {@link neureka.backend.api.ImplementationFor}
     *  the chosen {@link Device} in a given {@link ExecutionCall}.
     *  However, the  {@link ExecutionDispatcher} does not have to select a device specific implementation.
     *  It can also occupy the rest of the execution without any other steps being taken.
     *  For example, a {@link neureka.backend.api.ImplementationFor} or a {@link RecursiveExecutor}
     *  would not be used if not explicitly called.
     *  Bypassing other procedures is useful for full control and of course to implement unorthodox types of operations
     *  like the {@link neureka.backend.standard.operations.other.Reshape} operation
     *  which is very different from classical operations.
     *  Although the {@link ExecutionCall} passed to implementations of this will contain
     *  a fairly suitable {@link Device} assigned to a given {@link neureka.backend.api.Algorithm},
     *  one can simply ignore it and find a custom one which fits the contents of the given
     *  {@link ExecutionCall} instance better.
     *
     * @param caller The {@link FunctionNode} from which this request for execution emerged.
     * @param call The {@link ExecutionCall} whose contents ought to be executed.
     * @return The result of the execution.
     */
    @Override
    public Tsr&lt;?&gt; dispatch( FunctionNode caller, ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call ) {
<span class="fc" id="L134">        _checkReadiness();</span>
<span class="fc bfc" id="L135" title="All 2 branches covered.">        if ( call == null ) return _handleInsteadOfDevice.dispatch( caller, call );</span>
<span class="fc" id="L136">        MemValidator checker = MemValidator.forInputs( call.getTensors(), ()-&gt;_handleInsteadOfDevice.dispatch( caller, call ) );</span>
<span class="pc bpc" id="L137" title="1 of 2 branches missed.">        if ( checker.isWronglyIntermediate() ) {</span>
<span class="nc" id="L138">            throw new IllegalStateException(</span>
<span class="nc" id="L139">                    &quot;Output of algorithm '&quot; + this.getName() + &quot;' &quot; +</span>
                    &quot;is marked as intermediate result, despite the fact &quot; +
                    &quot;that it is a member of the input array. &quot; +
                    &quot;Tensors instantiated by library users instead of operations in the backend are not supposed to be flagged &quot; +
                    &quot;as 'intermediate', because they are not eligible for deletion!&quot;
            );
        }
<span class="pc bpc" id="L146" title="1 of 2 branches missed.">        if ( checker.isWronglyNonIntermediate() ) {</span>
<span class="nc" id="L147">            throw new IllegalStateException(</span>
<span class="nc" id="L148">                    &quot;Output of algorithm '&quot; + this.getName() + &quot;' &quot; +</span>
                    &quot;is neither marked as intermediate result nor a member of the input array. &quot; +
                    &quot;Tensors instantiated by operations in the backend are expected to be flagged &quot; +
                    &quot;as 'intermediate' in order to be eligible for deletion!&quot;
            );
        }
<span class="fc" id="L154">        return checker.getResult();</span>
    }

    /**
     *  Preparing refers to instantiating output tensors for the provided {@link ExecutionCall}.
     *  
     * @param call The execution call which needs to be prepared for execution.
     * @return The prepared {@link ExecutionCall} instance.
     */
    @Override
    public ExecutionCall&lt;? extends Device&lt;?&gt;&gt; prepare( ExecutionCall&lt;? extends Device&lt;?&gt;&gt; call ) {
<span class="fc" id="L165">        _checkReadiness();</span>
<span class="fc bfc" id="L166" title="All 2 branches covered.">        if ( call != null ) {</span>
<span class="fc" id="L167">            Tsr&lt;?&gt;[] inputs = call.getTensors().clone();</span>
<span class="fc" id="L168">            ExecutionCall&lt;? extends Device&lt;?&gt;&gt; prepared = _instantiateNewTensorsForExecutionIn.prepare(call);</span>
<span class="fc" id="L169">            Arrays.stream(prepared.getTensors())</span>
<span class="fc" id="L170">                    .filter(</span>
<span class="fc" id="L171">                            out -&gt; Arrays.stream(inputs)</span>
<span class="fc bfc" id="L172" title="All 2 branches covered.">                                    .noneMatch(in -&gt; in == out)</span>
                    )
<span class="fc" id="L174">                    .forEach(t -&gt; t.getUnsafe().setIsIntermediate(true));</span>

<span class="fc" id="L176">            return prepared;</span>
        }
<span class="fc" id="L178">        else return null;</span>
    }

    /**
     * @return A new concrete implementation of the {@link AbstractFunctionalAlgorithm} which
     *         is fully built and ready to be used as an {@link Operation} component.
     */
    public C buildFunAlgorithm() {
<span class="pc bpc" id="L186" title="5 of 12 branches missed.">        if (</span>
            _isSuitableFor == null ||
            _canPerformForwardADFor == null ||
            _canPerformBackwardADFor == null ||
            _supplyADAgentFor == null ||
            _handleInsteadOfDevice == null ||
            _instantiateNewTensorsForExecutionIn == null
        ) {
<span class="fc" id="L194">            throw new IllegalStateException(</span>
<span class="fc" id="L195">                    &quot;Instance '&quot;+getClass().getSimpleName()+&quot;' incomplete!&quot;</span>
            );
        }

<span class="fc" id="L199">        _isFullyBuilt = true;</span>
<span class="fc" id="L200">        return (C) this;</span>
    }

    /**
     *  This method ensures that this algorithm was fully supplied with all the
     *  required lambdas...
     */
    private void _checkReadiness() {
<span class="fc bfc" id="L208" title="All 2 branches covered.">        if ( !_isFullyBuilt ) {</span>
<span class="fc" id="L209">            throw new IllegalStateException(</span>
<span class="fc" id="L210">                &quot;Trying use an instance of '&quot;+this.getClass().getSimpleName()+&quot;' with name '&quot; + getName() + &quot;' &quot; +</span>
                &quot;which was not fully built!&quot;
            );
        }
<span class="fc" id="L214">    }</span>

    /**
     *  Neureka is supposed to be extremely modular and in a sense its backend should be &quot;hackable&quot; to a degree.
     *  However, this comes with a lot of risk, because it requires us to expose mutable state, which is not good.
     *  This class is semi-immutable, by simply warning us about any mutations after building was completed!
     *
     * @param newState The state which will be set.
     * @param &lt;T&gt; The type of the thing which is supposed to be set.
     * @param current The state which is currently set.
     * @return The checked thing.
     */
    private &lt;T&gt; T _checked( T newState, T current, Class&lt;T&gt; type ) {
<span class="fc bfc" id="L227" title="All 2 branches covered.">        if ( _isFullyBuilt ) {</span>
<span class="fc" id="L228">            _LOG.warn(</span>
<span class="fc" id="L229">                &quot;Implementation '&quot; + type.getSimpleName() + &quot;' in algorithm '&quot;+this+&quot;' was modified! &quot; +</span>
                &quot;Please consider only modifying the standard backend state of Neureka for experimental reasons.&quot;
            );
<span class="pc bpc" id="L232" title="1 of 4 branches missed.">        } else if ( current != null &amp;&amp; newState == null ) {</span>
<span class="nc" id="L233">            throw new IllegalArgumentException(</span>
<span class="nc" id="L234">                &quot;Trying set an already specified implementation of lambda '&quot;+current.getClass().getSimpleName()+&quot;' to null!&quot;</span>
            );
        }
<span class="fc" id="L237">        return newState;</span>
    }

    /**
     *  The {@link SuitabilityPredicate} received by this method 
     *  checks if a given instance of an {@link ExecutionCall} is
     *  suitable to be executed in {@link neureka.backend.api.ImplementationFor} instances
     *  residing in this {@link Algorithm} as components.
     *  The lambda will be called by the {@link #isSuitableFor(ExecutionCall)} method
     *  by any given {@link Operation} instances this algorithm belongs to.
     *
     * @return This very instance to enable method chaining.
     */
    public AbstractFunctionalAlgorithm&lt;C&gt; setIsSuitableFor( SuitabilityPredicate isSuitableFor ) {
<span class="fc" id="L251">        _isSuitableFor = _checked(isSuitableFor, _isSuitableFor, SuitabilityPredicate.class);</span>
<span class="fc" id="L252">        return this;</span>
    }

    /**
     *  A {@link ForwardADPredicate} lambda checks if this
     *  {@link Algorithm} can perform forward AD for a given {@link ExecutionCall}.
     *  The lambda will be called by the {@link #canPerformForwardADFor(ExecutionCall)} method
     *  by any given {@link Operation} instances this algorithm belongs to.
     *
     *
     * @param canPerformForwardADFor The lambda which evaluates if a provided {@link ExecutionCall} can be forward propagated.
     * @return This very instance to enable method chaining.
     */
    public AbstractFunctionalAlgorithm&lt;C&gt; setCanPerformForwardADFor( ForwardADPredicate canPerformForwardADFor ) {
<span class="fc" id="L266">        _canPerformForwardADFor = _checked(canPerformForwardADFor, _canPerformForwardADFor, ForwardADPredicate.class);</span>
<span class="fc" id="L267">        return this;</span>
    }

    /**
     *  A {@link BackwardADPredicate} lambda checks if this
     *  {@link Algorithm} can perform backward AD for a given {@link ExecutionCall}.
     *  The lambda will be called by the {@link #canPerformBackwardADFor(ExecutionCall)} method
     *  by any given {@link Operation} instances this algorithm belongs to.
     *
     *
     * @return This very instance to enable method chaining.
     */
    public AbstractFunctionalAlgorithm&lt;C&gt; setCanPerformBackwardADFor( BackwardADPredicate canPerformBackwardADFor ) {
<span class="fc" id="L280">        _canPerformBackwardADFor = _checked(canPerformBackwardADFor, _canPerformBackwardADFor, BackwardADPredicate.class);</span>
<span class="fc" id="L281">        return this;</span>
    }

    /**
     *  This method receives a {@link neureka.backend.api.algorithms.fun.ADAgentSupplier} which will supply
     *  {@link ADAgent} instances which can perform backward and forward auto differentiation.
     *  The lambda will be called by the {@link #supplyADAgentFor(Function, ExecutionCall, boolean)} method
     *  by any given {@link Operation} instances this algorithm belongs to.
     *
     *
     * @return This very instance to enable method chaining.
     */
    public AbstractFunctionalAlgorithm&lt;C&gt; setSupplyADAgentFor( ADAgentSupplier supplyADAgentFor ) {
<span class="fc" id="L294">        _supplyADAgentFor = _checked(supplyADAgentFor, _supplyADAgentFor, ADAgentSupplier.class);</span>
<span class="fc" id="L295">        return this;</span>
    }

    /**
     *  The {@link ExecutionDispatcher} lambda
     *  is the most important procedure within an {@link Algorithm}, which is responsible for
     *  electing an {@link neureka.backend.api.ImplementationFor}
     *  the chosen {@link Device} in a given {@link ExecutionCall} passed to the {@link ExecutionDispatcher}.
     *  However, the  {@link ExecutionDispatcher} does not have to select a device specific implementation.
     *  It can also occupy the rest of the execution without any other steps being taken.
     *  For example, a {@link neureka.backend.api.ImplementationFor} or a {@link RecursiveExecutor}
     *  would not be used if not explicitly called.
     *  Bypassing other procedures is useful for full control and of course to implement unorthodox types of operations
     *  like the {@link neureka.backend.standard.operations.other.Reshape} operation
     *  which is very different from classical operations.
     *  Although the {@link ExecutionCall} passed to implementations of this will contain
     *  a fairly suitable {@link Device} assigned to a given {@link neureka.backend.api.Algorithm},
     *  one can simply ignore it and find a custom one which fits the contents of the given
     *  {@link ExecutionCall} instance better.
     *  The lambda passed to this will be called by the {@link #dispatch(FunctionNode, ExecutionCall)} method
     *  by any given {@link Operation} instances this algorithm belongs to.
     *
     */
    public AbstractFunctionalAlgorithm&lt;C&gt; setExecutionDispatcher(ExecutionDispatcher handleInsteadOfDevice ) {
<span class="fc" id="L319">        _handleInsteadOfDevice = _checked(handleInsteadOfDevice, _handleInsteadOfDevice, ExecutionDispatcher.class);</span>
<span class="fc" id="L320">        return this;</span>
    }

    /**
     *  An {@link Algorithm} will typically produce a result when executing an {@link ExecutionCall}.
     *  This result must be created somehow.
     *  A {@link ExecutionPreparation} implementation instance will do just that...
     *  Often times the first entry in the array of tensors stored inside the call
     *  will be null to serve as a position for the output to be placed at.
     *  The creation of this output tensor is of course highly dependent on the type
     *  of operation and algorithm that is currently being used.
     *  Element-wise operations for example will require the creation of an output tensor
     *  with the shape of the provided input tensors, whereas the execution of a
     *  linear operation like for example a broadcast operation will require a very different approach...
     *  The lambda passed to this will be called by the {@link #prepare(ExecutionCall)} method
     *  by any given {@link Operation} instances this algorithm belongs to.
     */
    public AbstractFunctionalAlgorithm&lt;C&gt; setCallPreparation( ExecutionPreparation instantiateNewTensorsForExecutionIn ) {
<span class="fc" id="L338">        _instantiateNewTensorsForExecutionIn = _checked(instantiateNewTensorsForExecutionIn, _instantiateNewTensorsForExecutionIn, ExecutionPreparation.class);</span>
<span class="fc" id="L339">        return this;</span>
    }

}


</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>