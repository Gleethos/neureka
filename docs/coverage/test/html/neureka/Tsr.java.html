<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>Tsr.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neureka</a> &gt; <a href="index.source.html" class="el_package">neureka</a> &gt; <span class="el_source">Tsr.java</span></div><h1>Tsr.java</h1><pre class="source lang-java linenums">/*
MIT License

Copyright (c) 2019 Gleethos

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 __________
 \__    ___\
    |  |____ _ __
    | /  ___/ '___\
    | \___  \ |
     \/_____/_|         A long yet shallow class.

    This is the the core work-horse class of Neureka. The 'Tsr' class!
    It is a three-letter abbreviation of the word &quot;Tensor&quot;!

------------------------------------------------------------------------------------------------------------------------

   'Any fool can write code that a computer can understand.
    Good programmers write code that humans can understand.'
    â€“ Martin Fowler

    Use the following as search keys :)

    $(1) : CONSTRUCTION
        $(1.0) : GENERIC CONSTRUCTION
        $(1.1) : SHAPE LIST BASED CONSTRUCTION
        $(1.2) : SHAPE ARRAY BASED CONSTRUCTION
        Â§(1.3) : LAMBDA BASED CONSTRUCTION
        Â§(1.4) : FUNCTION BASED CONSTRUCTION

    Â§(2) : FLAGS
        Â§(2.0) : GRADIENT REQUIREMENT
        Â§(2.1) : SOURCE LOCATION (DEVICE)
        Â§(2.2) : VIRTUAL / ACTUAL
        Â§(2.3) : GRADIENT APPLY REQUIREMENT
        Â§(2.4) : DELETION

    Â§(3) : COMPONENT SYSTEM
        Â§(3.0) : SETTING / REJECTING
        Â§(3.1) : REMOVING / REJECTING
        Â§(3.2) : UPDATING

    Â§(4) : PROPERTIES
        $(4.0) : HIGH LEVEL PROPERTIES
        Â§(4.1) : COMPONENT PROPERTIES
        Â§(4.2) : INNER PROPERTIES

    Â§(5) : OBJECT STATE MODIFICATION

    Â§(6) : ND-ITERATOR LOGIC

    Â§(7) : COMPONENT SPECIFIC
        Â§(7.0) : AUTO-GRAD
        Â§(7.1) : FRAMING

    Â§(8) : (OVERLOADABLE) OPERATORS &amp; OPERATIONS
        Â§(8.0) : OPERATORS
        Â§(8.1) : OPERATIONS

    Â§(9) : SLICING, INDEXING &amp; INJECTING
        Â§(9.0) : SLICING
        Â§(9.1) : INJECTING
*/

package neureka;

import neureka.autograd.GraphNode;
import neureka.autograd.JITProp;
import neureka.backend.api.ExecutionCall;
import neureka.backend.standard.operations.other.Reshape;
import neureka.calculus.Function;
import neureka.calculus.assembly.FunctionBuilder;
import neureka.common.composition.Component;
import neureka.devices.Device;
import neureka.devices.host.HostCPU;
import neureka.devices.opencl.OpenCLDevice;
import neureka.dtype.DataType;
import neureka.dtype.custom.*;
import neureka.framing.NDFrame;
import neureka.framing.Relation;
import neureka.framing.states.AxisFrame;
import neureka.ndim.AbstractNDArray;
import neureka.ndim.Initializer;
import neureka.ndim.config.AbstractNDC;
import neureka.ndim.config.NDConfiguration;
import neureka.ndim.iterators.NDIterator;
import neureka.optimization.Optimizer;
import neureka.utility.DataConverter;
import neureka.utility.ListReader;
import neureka.utility.TsrAsString;
import neureka.common.composition.AbstractComponentOwner;
import neureka.utility.fluent.TensorBuilder;
import neureka.utility.fluent.states.WithShapeOrScalarOrVector;
import neureka.utility.slicing.SliceBuilder;
import neureka.utility.slicing.SmartSlicer;
import org.jetbrains.annotations.NotNull;
import org.slf4j.LoggerFactory;

import java.math.BigDecimal;
import java.util.*;
import java.util.stream.Collectors;


/**
 *  This class name &quot;Tsr&quot; is a 3 letter abbreviation of the word &quot;tensor&quot;, a mathematical concept.
 *  A tensor is a type of multidimensional data-structure with certain transformation properties.
 *  Technically however, it is mostly a simple container / data-structure which can house data indexed by N dimensions.
 *  Therefore it is often also described as an nd-array.
 *  Elements of a tensor are also mostly numeric.&lt;br&gt;
 *  This means that: &lt;br&gt;
 *  &lt;i&gt;&lt;b&gt;...a tensor of rank 0 is a scalar, a tensor of rank 1 is a vector and a tensor of rank 2 is a matrix, etc...&lt;/b&gt;&lt;/i&gt;
 *  &lt;br&gt;&lt;br&gt;
 *  Consequently, tensors are a perfect fit for applying various operations on them.
 *  Such operations might be simple elementwise operations or more complex linear operations like
 *  the dot-product, matrix- or even tensor multiplications. &lt;br&gt;
 *  &lt;br&gt;
 * @param &lt;V&gt; The type parameter for the individual value items within this tensor.
 */
<span class="pc bpc" id="L138" title="1 of 2 branches missed.">public class Tsr&lt;V&gt; extends AbstractNDArray&lt;Tsr&lt;V&gt;, V&gt; implements Component&lt;Tsr&lt;V&gt;&gt;, Cloneable</span>
{
    static {
<span class="fc" id="L141">        _CPU = HostCPU.instance();</span>
<span class="fc" id="L142">        _LOG = LoggerFactory.getLogger( Tsr.class );</span>
<span class="fc" id="L143">    }</span>

    /**
     *  The default device is an instance of the &quot;{@link HostCPU}&quot; class. &lt;br&gt;
     *  This field is a reference to this default device implementation.
     */
    private static final Device&lt;Number&gt; _CPU;

    /**
     *  This field contains multiple flags.
     *  The bits of this integer are used to encode various states which a tensor can have.
     *  These bits are flipped by bitmasks which are defined below.
     */
<span class="fc" id="L156">    private int _flags = 0;</span>

    /**
     *  The following fields are bit masks used to store true / false values
     *  in a targeted bit inside the &quot;_flags&quot; variable.
     */
    private static final int RQS_GRADIENT_MASK = 1;
    private static final int IS_OUTSOURCED_MASK = 2;
    private static final int IS_VIRTUAL_MASK = 4;
    private static final int GRADIENT_APPLY_RQD_MASK = 8;
    private static final int WAS_DELETED_MASK = 18;

    /**
     *  The version of the data ( _data ) stored within this tensor.
     *  It gets incremented every time an inline operation occurs!
     *  {@link GraphNode} instances tied to this tensor (as component) store
     *  a reference version which is a copy of this field.
     *  If this version changes, despite there being a GraphNode which might
     *  perform auto-differentiation at some point, then an exception will be thrown for debugging.
     *  &lt;br&gt;
     *  The getter returns the version of the data (_data) stored within this tensor.
     */
<span class="fc" id="L178">    private int _version = 0;</span>


    /*==================================================================================================================
    |
    |       Â§(1) : CONSTRUCTION
    |   ---------------------------
    */
    /*
        -------------------------------------------
            Â§(1.0) : GENERIC CONSTRUCTION
        --------------------------------------------
    */

    /**
     *  This static factory method creates and return a completely empty and undefined tensor
     *  which is void of any contents and meaning.
     *  The use case for this would be to use the produced {@link Tsr}
     *  instance as a target for an inline operations which fills the instance with an actual value. &lt;br&gt;
     *  An example of this approach would be to call the {@link #putAt(List, Tsr)} method with an empty list as key.
     *  This will be interpreted as an inline copy of the contents of the
     *  second parameter into this {@link Tsr} instance.
     */
<span class="fc" id="L201">    public static Tsr&lt;Object&gt; newInstance() { return new Tsr&lt;&gt;(); }</span>

    /**
     *  This constructor creates a completely empty tensor which is void of any contents and meaning.
     *  The use case for this would be to use the produced {@link Tsr}
     *  instance as a target for an inline operations which fills this instance with an actual value. &lt;br&gt;
     *  An example of this approach would be to call the {@link #putAt(List, Tsr)} method with an empty list as key.
     *  This will be interpreted as an inline copy of the contents of the
     *  second parameter into this {@link Tsr} instance.
     */
<span class="fc" id="L211">    private Tsr() {}</span>

    /**
     *  This static {@link Tsr} factory method tries to interpret the provided
     *  arguments to create the instance the use might wants.
     *
     * @param args The arguments which ought to be interpreted.
     * @return The result of the interpretation in the form of a {@link Tsr} instance of typ {@link Object}.
     */
<span class="fc" id="L220">    public static &lt;T&gt; Tsr&lt;T&gt; of( Object... args ) { return _of( args ); }</span>

    private static &lt;T&gt; Tsr&lt;T&gt; _of( Object... args )
    {
<span class="pc bpc" id="L224" title="2 of 4 branches missed.">        if ( args == null || args.length == 0 ) return new Tsr&lt;&gt;();</span>
<span class="fc bfc" id="L225" title="All 2 branches covered.">        if ( args.length == 1 ) {</span>
<span class="pc bpc" id="L226" title="1 of 2 branches missed.">            if ( args[ 0 ] instanceof Object[] ) {</span>
<span class="nc" id="L227">                return _of( (Object[]) args[ 0 ] );</span>
<span class="fc bfc" id="L228" title="All 2 branches covered.">            } else if ( args[ 0 ] instanceof BigDecimal ) {</span>
<span class="fc" id="L229">                Tsr&lt;T&gt; t = new Tsr&lt;&gt;();</span>
<span class="fc" id="L230">                t.createConstructionAPI()._constructAllF64( new int[]{ 1 }, ( (BigDecimal) args[ 0 ] ).doubleValue());</span>
<span class="fc" id="L231">                return t;</span>
<span class="fc bfc" id="L232" title="All 2 branches covered.">            } else if ( args[ 0 ] instanceof Integer ) {</span>
<span class="fc" id="L233">                Tsr&lt;T&gt; t = new Tsr&lt;&gt;();</span>
<span class="fc" id="L234">                t.createConstructionAPI()._constructAllF64( new int[]{ 1 }, ( (Integer) args[ 0 ] ).doubleValue() );</span>
<span class="fc" id="L235">                return t;</span>
            } else {
<span class="fc" id="L237">                String message = &quot;Cannot create tensor from argument of type '&quot; + args[ 0 ].getClass().getName() + &quot;'!&quot;;</span>
<span class="fc" id="L238">                _LOG.error( message );</span>
<span class="fc" id="L239">                throw new IllegalArgumentException( message );</span>
            }
        }
<span class="fc bfc" id="L242" title="All 2 branches covered.">        args[ 0 ] = ( args[ 0 ] instanceof ArrayList ) ? ( (List&lt;?&gt;) args[ 0 ] ).toArray() : args[ 0 ];</span>
<span class="pc bpc" id="L243" title="1 of 2 branches missed.">        args[ 1 ] = ( args[ 1 ] instanceof ArrayList ) ? ( (List&lt;?&gt;) args[ 1 ] ).toArray() : args[ 1 ];</span>
<span class="fc bfc" id="L244" title="All 2 branches covered.">        if ( args[ 0 ] instanceof Object[] ) {</span>
<span class="pc bpc" id="L245" title="3 of 4 branches missed.">            if ( ( (Object[]) args[ 0 ] )[ 0 ] instanceof Integer || ((Object[])args[ 0 ])[ 0 ] instanceof Double) {</span>
<span class="fc" id="L246">                args[ 0 ] = _intArray( (Object[]) args[ 0 ] );</span>
            }
        }
<span class="pc bpc" id="L249" title="1 of 2 branches missed.">        if ( args[ 1 ] instanceof Object[] ) {</span>
<span class="nc bnc" id="L250" title="All 2 branches missed.">            if ( ((Object[]) args[ 1 ] )[ 0 ] instanceof Integer ) args[ 1 ] = _doubleArray( (Object[]) args[ 1 ] );</span>
<span class="nc bnc" id="L251" title="All 2 branches missed.">            else if ( ( ( Object[] ) args[ 1 ] )[ 0 ] instanceof BigDecimal ) args[ 1 ] = _doubleArray( (Object[]) args[ 1 ] );</span>
        }
        //CASES:
<span class="fc bfc" id="L254" title="All 2 branches covered.">        if ( args[ 0 ] instanceof int[] ) {</span>
<span class="fc bfc" id="L255" title="All 4 branches covered.">            if ( args[ 1 ] instanceof Double || args[ 1 ] instanceof Integer ) {</span>
<span class="fc" id="L256">                Tsr&lt;T&gt; t = new Tsr&lt;&gt;();</span>
<span class="fc bfc" id="L257" title="All 2 branches covered.">                args[ 1 ] = ( args[ 1 ] instanceof Integer ) ? ( (Integer) args[ 1 ] ).doubleValue() : args[ 1 ];</span>
<span class="fc" id="L258">                t.createConstructionAPI()._constructAllF64( (int[]) args[ 0 ], (Double) args[ 1 ] );</span>
<span class="fc" id="L259">                return t;</span>
<span class="pc bpc" id="L260" title="1 of 2 branches missed.">            } else if ( args[ 1 ] instanceof double[] ) {</span>
<span class="nc" id="L261">                Tsr&lt;T&gt; t = new Tsr&lt;&gt;();</span>
<span class="nc" id="L262">                t.createConstructionAPI().constructForDoubles( (int[]) args[ 0 ], (double[]) args[ 1 ] );</span>
<span class="nc" id="L263">                return t;</span>
            } else {
<span class="fc" id="L265">                Tsr&lt;T&gt; t = new Tsr&lt;&gt;();</span>
<span class="fc" id="L266">                t.setDataType( DataType.of( args[1].getClass() ) );</span>
<span class="fc" id="L267">                t._construct( (int[]) args[0], true, true );</span>
<span class="fc" id="L268">                ((Object[])t.getData())[0] = args[1];</span>
<span class="fc" id="L269">                return t;</span>
            }
        }
 
        /* EXPRESSION BASED CONSTRUCTION: 
            The following allows the creation of tensors based on passing an expression
            alongside input tensors to the constructor.
            An example would be:
            
                Tsr&lt;?&gt; t = Tsr.of( &quot;tanh(&quot;, x, &quot;) * 7 ^&quot;, y );  
        */
<span class="fc" id="L280">        boolean containsString = false;</span>
<span class="fc" id="L281">        int numberOfTensors = 0;</span>
<span class="fc" id="L282">        ArrayList&lt;Tsr&lt;T&gt;&gt; tsrList = new ArrayList&lt;&gt;();</span>
<span class="fc bfc" id="L283" title="All 2 branches covered.">        for ( Object o : args ) {</span>
<span class="fc bfc" id="L284" title="All 4 branches covered.">            containsString = ( o instanceof String ) || containsString;</span>
<span class="fc bfc" id="L285" title="All 2 branches covered.">            if ( o instanceof Tsr ) {</span>
<span class="fc" id="L286">                tsrList.add( (Tsr&lt;T&gt;) o );</span>
<span class="fc" id="L287">                numberOfTensors++;</span>
            }
        }
<span class="fc" id="L290">        boolean doAD = true;</span>
<span class="fc" id="L291">        Tsr&lt;T&gt;[] tensors = new Tsr[ numberOfTensors ];</span>
<span class="fc" id="L292">        StringBuilder f = new StringBuilder();</span>
<span class="fc" id="L293">        int ti = 0;</span>
<span class="fc bfc" id="L294" title="All 2 branches covered.">        for ( Object o : args ) {</span>
<span class="fc bfc" id="L295" title="All 2 branches covered.">            if ( tsrList.contains( o ) ) {</span>
<span class="fc" id="L296">                tensors[ ti ] = ( (Tsr&lt;T&gt;) o );</span>
<span class="fc" id="L297">                f.append( &quot;I[&quot; ).append( ti ).append( &quot;]&quot; );</span>
<span class="fc" id="L298">                ti++;</span>
            }
<span class="pc bpc" id="L300" title="1 of 2 branches missed.">            else if ( o instanceof  String ) f.append( (String) o );</span>
<span class="nc bnc" id="L301" title="All 2 branches missed.">            else if ( o instanceof  Boolean ) doAD = (Boolean) o;</span>
        }
<span class="pc bpc" id="L303" title="2 of 4 branches missed.">        if ( tensors.length == 0 || tensors[0] == null) return new Tsr&lt;&gt;();</span>
<span class="fc" id="L304">        return Function.of( f.toString(), doAD ).call( tensors );</span>
    }


    /*
        -------------------------------------------
            Â§(1.1) : SHAPE LIST BASED CONSTRUCTION
        --------------------------------------------
    */

    /**
     *  This is a convenient factory method for creating {@link Tsr} instances for
     *  values of type {@link T} based on a list of integers
     *  defining a shape made up of axes sizes as well as a scalar value of type {@link T}
     *  which will fill out the data array spanned by the provided shape information.
     *
     * @param shape A list of integers whose values ought to define the size of the axes of the shape of the new {@link Tsr}.
     * @param value An object of type {@link T} which will populate the data array of the new instance.
     * @return A new {@link Tsr} instance for the generic type {@link T}.
     */
<span class="fc" id="L324">    public static &lt;T&gt; Tsr&lt;T&gt; of( List&lt;Integer&gt; shape, T value ) { return _of( shape, value ); }</span>

    /**
     *  This factory method will create and return a {@link Tsr} instance
     *  based on a list of {@link Number} instances whose rounded values will be interpreted as
     *  the shape of this new {@link Tsr} instance and a seed which will serve
     *  as a source of pseudo randomness to generate the values for the new instance.
     *
     * @param shape A list of {@link Number} instances which will be interpreted as a shape array.
     * @param seed A source of pseudo randomness for the {@link Tsr} instance created by this method.
     * @return A new {@link Tsr} instance created based on a shape and a seed.
     */
    public static Tsr&lt;Double&gt; of( List&lt;? extends Number&gt; shape, String seed ) {
<span class="fc" id="L337">        int[] shp = new int[ shape.size() ];</span>
<span class="fc bfc" id="L338" title="All 2 branches covered.">        for ( int i = 0; i &lt; shp.length; i++ ) shp[ i ] = shape.get( i ).intValue();</span>
<span class="fc" id="L339">        return new Tsr&lt;&gt;( shp, seed );</span>
    }

    public static &lt;V&gt; Tsr&lt;V&gt; of( List&lt;? extends Number&gt; shape, List&lt;V&gt; range ) {
<span class="fc" id="L343">        return Tsr.of(</span>
<span class="fc" id="L344">                        DataType.of(Object.class),</span>
<span class="fc" id="L345">                        shape.stream().mapToInt(Number::intValue).toArray(),</span>
                        range
                );
    }

    public static &lt;V&gt; Tsr&lt;V&gt; of( int[] shape, List&lt;V&gt; range ) {
<span class="fc" id="L351">        return Tsr.of(</span>
<span class="fc" id="L352">                        DataType.of(Object.class),</span>
                        shape,
                        range
                );
    }

    /**
     *  This factory method will turn a list of either nested lists or values into a {@link Tsr}
     *  instance with the corresponding.
     *
     * @param conf A list of either values or nested lists which are themselves either or.
     * @return A new {@link Tsr} instance whose shape and data is based on the provided list structure.
     */
    public static Tsr&lt;Object&gt; of( List&lt;Object&gt; conf ) {
<span class="fc" id="L366">        boolean isMatrix = conf.stream()</span>
<span class="fc" id="L367">                                .allMatch( e -&gt;</span>
<span class="fc bfc" id="L368" title="All 2 branches covered.">                                        e instanceof List &amp;&amp;</span>
<span class="pc bpc" id="L369" title="1 of 2 branches missed.">                                                ((List&lt;Object&gt;) e).stream().noneMatch( v -&gt; v instanceof List)</span>
                                );

<span class="fc bfc" id="L372" title="All 2 branches covered.">        if ( isMatrix )</span>
<span class="fc" id="L373">            return new Tsr&lt;&gt;( conf );</span>

<span class="fc" id="L375">        List&lt;Integer&gt; growingShape = new ArrayList&lt;&gt;();</span>
<span class="fc" id="L376">        List&lt;Object&gt; growingData = new ArrayList&lt;&gt;();</span>
<span class="fc" id="L377">        ListReader reader = new ListReader(</span>
                                    conf,
                                    0,
                                    growingData,
                                    growingShape,
<span class="pc bpc" id="L382" title="1 of 2 branches missed.">                                    o -&gt; ( o instanceof Number ? ((Number)o).doubleValue() : o )</span>
                                );
<span class="fc" id="L384">        return (Tsr&lt;Object&gt;) Tsr.of(</span>
<span class="fc" id="L385">                                DataType.of(reader.getType()),</span>
<span class="fc" id="L386">                                growingShape.stream().mapToInt(i -&gt; i).toArray(),</span>
<span class="fc" id="L387">                                growingData.toArray()</span>
                            );
    }

    public static Tsr&lt;Number&gt; ofShape( List&lt;? extends Number&gt; axesSizes ) {
<span class="fc" id="L392">        return ofShape( axesSizes.toArray(new Number[0]) );</span>
    }

    @SafeVarargs
    public static &lt;T extends Number&gt; Tsr&lt;Number&gt; ofShape( T... axesSizes ) {
<span class="fc" id="L397">        int[] shape = Arrays.stream( axesSizes ).mapToInt( Number::intValue ).toArray();</span>
<span class="fc" id="L398">        return Tsr.ofShape( shape );</span>
    }

    /**
     *  See {@link #of(List)}.
     */
<span class="fc" id="L404">    private Tsr( List&lt;Object&gt; conf ) {</span>
<span class="fc" id="L405">        createConstructionAPI().construct( conf.stream().map( e -&gt; (List&lt;Object&gt;) e ).collect( Collectors.toList() ) );</span>
<span class="fc" id="L406">    }</span>

    /*
        -------------------------------------------
            Â§(1.2) : SHAPE ARRAY BASED CONSTRUCTION
        --------------------------------------------
    */

<span class="fc" id="L414">    public static &lt;V&gt; WithShapeOrScalarOrVector&lt;V&gt; of(Class&lt;V&gt; typeClass ) { return new TensorBuilder( typeClass ); }</span>

<span class="fc" id="L416">    public static WithShapeOrScalarOrVector&lt;Double&gt; ofDoubles() { return of(Double.class); }</span>

<span class="nc" id="L418">    public static WithShapeOrScalarOrVector&lt;Float&gt; ofFloats() { return of(Float.class); }</span>

<span class="nc" id="L420">    public static WithShapeOrScalarOrVector&lt;Integer&gt; ofInts() { return of(Integer.class); }</span>

<span class="fc" id="L422">    public static Tsr&lt;Double&gt; of( double value ) { return new Tsr(value); }</span>

<span class="fc" id="L424">    private Tsr( double value ) { createConstructionAPI()._constructAllF64( new int[]{ 1 }, value ); }</span>

<span class="fc" id="L426">    public static Tsr&lt;Float&gt; of( float[] value ) { return new Tsr&lt;&gt;( value ); }</span>

<span class="fc" id="L428">    private Tsr( float[] value ) { createConstructionAPI().constructForFloats( new int[]{ value.length }, value ); }</span>

<span class="fc" id="L430">    public static Tsr&lt;Double&gt; of( int[] shape, String seed ) { return new Tsr&lt;&gt;( shape, seed ); }</span>

    /**
     *  See {@link #of(int[], String)}
     *  ...and {@link #of(List, String)}
     */
<span class="fc" id="L436">    private Tsr( int[] shape, String seed ) {</span>
<span class="fc" id="L437">        _construct( shape, false, false );</span>
<span class="fc" id="L438">        _setData( DataConverter.Utility.seededDoubleArray( (double[]) getData(), seed ) );</span>
<span class="fc" id="L439">    }</span>

<span class="fc" id="L441">    public static Tsr&lt;Number&gt; ofShape( int[] shape ) { return new Tsr&lt;&gt;( shape ); }</span>

<span class="fc" id="L443">    private Tsr( int[] shape ) { _construct( shape, true, true ); }</span>

<span class="fc" id="L445">    public static Tsr&lt;Double&gt; of( int[] shape, double value ) { return new Tsr&lt;&gt;( shape, value ); }</span>

<span class="fc" id="L447">    private Tsr( int[] shape, double value ) { createConstructionAPI()._constructAllF64( shape, value ); }</span>

<span class="fc" id="L449">    public static Tsr&lt;Double&gt; of( int[] shape, double[] value ) { return new Tsr&lt;&gt;( shape, value ); }</span>

<span class="fc" id="L451">    private Tsr( int[] shape, double[] value ) { createConstructionAPI().constructForDoubles( shape, value ); }</span>

<span class="fc" id="L453">    public static &lt;V&gt; Tsr&lt;V&gt; of( DataType&lt;V&gt; type, int[] shape ) { return new Tsr&lt;&gt;( shape, type ); }</span>


    private Tsr( int[] shape, DataType&lt;?&gt; type )
<span class="fc" id="L457">    {</span>
<span class="fc" id="L458">        setDataType( DataType.of( type.getTypeClass() ) );</span>
<span class="fc" id="L459">        _construct( shape, true, true );</span>
<span class="fc" id="L460">    }</span>

<span class="fc" id="L462">    public static &lt;V&gt; Tsr of( Class&lt;V&gt; typeClass, int[] shape, Object data ) { return new Tsr&lt;&gt;( shape, typeClass, data ); }</span>

<span class="nc" id="L464">    public static &lt;V&gt; Tsr&lt;V&gt; of( Class&lt;V&gt; typeClass, List&lt;Integer&gt; shape, Object data ) { return new Tsr&lt;&gt;( shape.stream().mapToInt(i -&gt; i).toArray(), typeClass, data ); }</span>

    private Tsr( int[] shape, Class&lt;?&gt; typeClass, Object data )
<span class="fc" id="L467">    {</span>
<span class="fc" id="L468">        setDataType( DataType.of( typeClass ) );</span>
<span class="fc" id="L469">        createConstructionAPI().configureFromNewShape( shape, false, false );</span>
<span class="fc" id="L470">        setValue( data );</span>
<span class="fc" id="L471">    }</span>

    public static &lt;V&gt; Tsr&lt;V&gt; of( Class&lt;V&gt; typeClass, List&lt;Integer&gt; shape, List&lt;V&gt; data ) {
<span class="fc" id="L474">        return Tsr.of(</span>
<span class="fc" id="L475">                    DataType.of( typeClass ),</span>
<span class="fc" id="L476">                    shape.stream().mapToInt( e -&gt; e ).toArray(),</span>
                    data
                );
    }

    /**
     *  This factory method is among the most flexible and forgiving ways to create a {@link Tsr} instance.
     *  It receives a {@link DataType} for type safety and to ensure that the produced {@link Tsr} instance
     *  will contain elements of the correct type, a shape array which stores the sizes of the axes that the
     *  instance ought to possess and finally it receives a data {@link Object} which can be anything ranging from
     *  a {@link List} to an array or simply a single value which ought to fill out the entire {@link Tsr}.
     *
     * @param dataType The data type of the data represented by {@link Tsr} instance created by this method.
     * @param shape An array of axis sizes describing the dimensionality of the {@link Tsr} created by this method.
     * @param data The data for the {@link Tsr} that is about to be created, which can be a list, an array or scalar.
     * @param &lt;V&gt;
     * @return A new {@link Tsr} instance of the specified type, shape and containing the provided data.
     */
<span class="fc" id="L494">    public static &lt;V&gt; Tsr of( DataType&lt;V&gt; dataType, int[] shape, Object data ) { return new Tsr&lt;&gt;( shape, dataType, data ); }</span>

<span class="fc" id="L496">    private Tsr( int[] shape, DataType&lt;?&gt; dataType, Object data ) { createConstructionAPI()._tryConstructing( shape, dataType, data ); }</span>

    public static &lt;V&gt; Tsr&lt;V&gt; of( DataType&lt;V&gt; dataType, List&lt;Integer&gt; shape,  List&lt;V&gt; data ) {
<span class="fc" id="L499">        return Tsr.of(</span>
                dataType,
<span class="fc" id="L501">                shape.stream().mapToInt( i -&gt; i ).toArray(),</span>
<span class="fc" id="L502">                data.toArray()</span>
        );
    }

    // Inner construction layer:

    private void _construct( int[] shape, boolean allocate, boolean virtual )
    {
<span class="fc" id="L510">        createConstructionAPI().configureFromNewShape( shape, virtual, true );</span>
<span class="fc" id="L511">    }</span>

    private static int[] _intArray( Object[] arg ) {
<span class="fc" id="L514">        int length = arg.length;</span>
<span class="fc" id="L515">        int[] array = new int[ length ];</span>
<span class="fc bfc" id="L516" title="All 2 branches covered.">        for ( int i = 0; i &lt; length; i++ ) {</span>
<span class="pc bpc" id="L517" title="1 of 2 branches missed.">            if ( arg[ i ] instanceof Double ) array[ i ] = ( (Double) arg[ i ] ).intValue();</span>
<span class="fc" id="L518">            else array[ i ] = (Integer) arg[ i ];</span>
        }
<span class="fc" id="L520">        return array;</span>
    }

    private static double[] _doubleArray( Object[] arg )
    {
<span class="nc" id="L525">        int length = arg.length;</span>
<span class="nc" id="L526">        double[] array = new double[ length ];</span>
<span class="nc bnc" id="L527" title="All 2 branches missed.">        for ( int i = 0; i &lt; length; i++ ) {</span>
<span class="nc bnc" id="L528" title="All 2 branches missed.">            if ( arg[ i ] instanceof Integer ) array[ i ] = (Integer) arg[ i ];</span>
<span class="nc bnc" id="L529" title="All 2 branches missed.">            else if ( arg[ i ] instanceof Double ) array[ i ] = (Double) arg[ i ];</span>
<span class="nc bnc" id="L530" title="All 2 branches missed.">            else if ( arg[ i ] instanceof BigDecimal ) array[ i ] = ( (BigDecimal) arg[ i ] ).doubleValue();</span>
        }
<span class="nc" id="L532">        return array;</span>
    }

    /*
        -------------------------------------------
            Â§(1.3) : LAMBDA BASED CONSTRUCTION
        --------------------------------------------
    */

    /**
     *  This factory method allows the creation of tensors with an additional initialization
     *  lambda for filling the underlying data array with desired values.
     *  Besides regular numeric types it is also possible to initialize the
     *  tensor with regular Objects like String instances or custom data types like complex
     *  numbers for example... &lt;br&gt;
     *  Therefore the constructor requires not only a shape as argument but also
     *  the data type which ought to be allocated as well as the initialization
     *  lambda which will be called iteratively.
     *
     * @param type The data type this tensor ought to have.
     * @param shape The shape of this new tensor ought to have.
     * @param initializer The lambda Object which ought to fill this tensor with the appropriate data.
     * @param &lt;T&gt; The type parameter for the actual data array items.
     */
    public static &lt;T&gt; Tsr&lt;T&gt; of( DataType&lt;T&gt; type, List&lt;Integer&gt; shape, Initializer&lt;T&gt; initializer ) {
<span class="fc" id="L557">        return Tsr.of(</span>
                    type,
<span class="fc" id="L559">                    shape.stream().mapToInt( e -&gt; e ).toArray(),</span>
                    initializer
                );
    }

    /**
     *  This factory method allows the creation of tensors with an additional initialization
     *  lambda for filling the underlying data array with desired values.
     *  Besides regular numeric types it is also possible to initialize the
     *  tensor with regular objects like {@link String} instances or custom data types like complex
     *  numbers for example... &lt;br&gt;
     *  Therefore the constructor requires not only a shape as argument but also
     *  the data type which ought to be allocated as well as the initialization
     *  lambda which will be called iteratively.
     *
     * @param type The data type this tensor ought to have.
     * @param shape The shape of this new tensor ought to have.
     * @param initializer The lambda Object which ought to fill this tensor with the appropriate data.
     * @param &lt;T&gt; The type parameter for the actual data array items.
     */
    public static &lt;T&gt; Tsr&lt;T&gt; of( DataType&lt;T&gt; type, int[] shape, Initializer&lt;T&gt; initializer ) {
<span class="fc" id="L580">        return new Tsr&lt;&gt;( shape, type, initializer );</span>
    }

    /**
     *  see {@link #of(DataType, int[], Initializer)}
     *
     * @param shape The shape of this new tensor ought to have.
     * @param type The data type this tensor ought to have.
     * @param initializer The lambda Object which ought to fill this tensor with the appropriate data.
     * @param &lt;T&gt; The type parameter for the actual data array items.
     */
    private &lt;T&gt; Tsr( int[] shape, DataType&lt;T&gt; type, Initializer&lt;T&gt; initializer )
<span class="fc" id="L592">    {</span>
<span class="fc" id="L593">        _constructFromInitializer( shape, type, initializer );</span>
<span class="fc" id="L594">    }</span>

    /**
     * @param shape The shape of that this new tensor ought to have.
     * @param type The data type that this tensor ought to have.
     * @param initializer The lambda Object which ought to fill this tensor with the appropriate data.
     * @param &lt;T&gt; The type parameter for the actual data array items.
     */
    private &lt;T&gt; void _constructFromInitializer(int[] shape, DataType&lt;T&gt; type, Initializer&lt;T&gt; initializer )
    {
<span class="fc" id="L604">        setDataType( type );</span>
<span class="fc" id="L605">        _construct( shape, true, false );</span>
<span class="fc" id="L606">        _initData( initializer );</span>
<span class="fc" id="L607">    }</span>


    /*
        -------------------------------------------
            Â§(1.4) : FUNCTION BASED CONSTRUCTION
        --------------------------------------------
     */

    /**
     *  This factory method allows for the creation and execution of Function instances
     *  without actually instantiating them manually,
     *  where the result will then become this very tensor. &lt;br&gt;&lt;br&gt;
     *  The passed {@link String} will be parsed into a {@link Function} AST which will be cached
     *  using the expression as key in case it will be used in future constructor calls
     *  like this one, or elsewhere...
     *  The created / retrieved {@link Function} will then be called with the supplied input list
     *  in order to trigger an execution.
     *  The result of which will be used for the population of the fields of this
     *  very instance.                                                                      &lt;br&gt;
     *  An example would be the following :                                                 &lt;br&gt;
     * &lt;ul&gt;
     *      &lt;li&gt;&lt;i&gt; 'Tsr a = Tsr.of( &quot;sin( I[0] ) / I[1]&quot;, List.of(b, c) )'&lt;/i&gt;&lt;/li&gt;
     * &lt;/ul&gt;
     *
     * @param expression A String which will be used for parsing a Function AST.
     * @param inputs A list of inputs which can be tensors or numeric types.
     */
    public static &lt;V&gt; Tsr&lt;V&gt; of( String expression, List&lt;? extends Object&gt; inputs ) {
<span class="fc bfc" id="L636" title="All 2 branches covered.">        if ( inputs.stream().allMatch( e -&gt; e instanceof Tsr ) )</span>
<span class="fc" id="L637">            return _constructFunctional(</span>
                    null,
<span class="fc" id="L639">                    inputs.stream().toArray( Tsr[]::new ),</span>
                    expression,
                    true
            );
        else
<span class="fc" id="L644">            return _constructFunctional(</span>
                    null,
<span class="fc" id="L646">                    inputs.stream().map( args -&gt; _of(args) ).toArray( Tsr[]::new ),</span>
                    expression,
                    true
            );
    }

    /**
     *  This method takes a tensor and a String expression describing
     *  operations which ought to be applied to said tensor.
     *  This expression will be parsed to a {@link Function} instance expecting one input,
     *  namely : &quot;I[0]&quot; &lt;br&gt;
     *  An example would be the following :
     * &lt;ul&gt;
     *      &lt;li&gt;&lt;i&gt; 'Tsr a = Tsr.of( b, &quot;sin( I[0] ) * 2&quot; )'&lt;/i&gt;&lt;/li&gt;
     * &lt;/ul&gt;
     *
     *  Which takes the tensor 'b' and applies the function &quot;f(x) = sin(x) * 2&quot;
     *  elementwise to produce a new tensor 'a'! &lt;br&gt;
     *  &lt;br&gt;
     *
     * @param tensor A tensor which serves as input to the Function instance parsed from the given expression.
     * @param expression The expression describing operations applied to the provided tensor.
     */
    public static &lt;V&gt; Tsr&lt;V&gt; of( String expression, Tsr&lt;V&gt; tensor ) {
<span class="fc" id="L670">        return _constructFunctional(null, new Tsr[]{tensor}, expression, true);</span>
    }

    /**
     *  This method takes an array of tensors and a String expression describing
     *  operations which ought to be applied to the tensors in said array.
     *  This expression will be parsed to a {@link Function} instance expecting as many inputs
     *  as there are array entries, namely : &quot;I[0]&quot;, &quot;I[1]&quot;, &quot;I[2]&quot;, ... &lt;br&gt;
     *  An example would be the following :
     * &lt;ul&gt;
     *      &lt;li&gt;&lt;i&gt; 'Tsr a = Tsr.of( &quot;sin( I[0] ) / I[1]&quot;, b, c )'&lt;/i&gt;&lt;/li&gt;
     * &lt;/ul&gt;
     *
     *  Which takes the tensor 'b' and 'c' and applies the function &quot;f(x,y) = sin(x) / y&quot;
     *  elementwise to produce a new tensor 'a'! &lt;br&gt;
     *
     * @param expression The expression describing operations applied to the provided tensors.
     * @param tensors An array of tensors used as inputs to the Function instance parsed from the provided expression.
     */
    @SafeVarargs
    public static &lt;V&gt; Tsr&lt;V&gt; of( String expression, Tsr&lt;V&gt;... tensors ) {
<span class="fc" id="L691">        return _constructFunctional( null, tensors, expression, true );</span>
    }

    /**
     *  This method takes an array of tensors and a String expression describing
     *  operations which ought to be applied to the tensors in said array.
     *  It also receives a boolean flag which determines if the defined function
     *  should be executed with autograd enabled.
     *  The provided expression will be parsed to a {@link Function} instance expecting as many inputs
     *  as there are array entries, namely : &quot;I[0]&quot;, &quot;I[1]&quot;, &quot;I[2]&quot;, ...                    &lt;br&gt;
     *  An example would be the following :                                                 &lt;br&gt;
     * &lt;ul&gt;
     *      &lt;li&gt;&lt;i&gt; 'Tsr a = Tsr.of( &quot;sin( I[0] ) / I[1]&quot;, true, b, c )'&lt;/i&gt;&lt;/li&gt;
     * &lt;/ul&gt;
     *  Which takes the tensor 'b' and 'c' and applies the function &quot;f(x,y) = sin(x) / y&quot;
     *  elementwise to produce a new tensor 'a'!
     *  Additionally there is a helpful flag which allows one to specify if the
     *  parsed {@link Function} instance emerging from the provided expression
     *  should also allow the tracking of computations via a computation graph ({@link GraphNode} instances).
     *  This history tracking then enables auto-differentiation. &lt;br&gt;
     *
     * @param expression The expression describing operations applied to the provided tensors.
     * @param doAD A flag which when set to true commands the creation of a computation graph during operation execution.
     * @param tensors An array of tensors used as inputs to the Function instance parsed from the provided expression.
     *
     */
    public static &lt;V&gt; Tsr&lt;V&gt; of( String expression, boolean doAD,  Tsr&lt;V&gt;... tensors ) {
<span class="fc" id="L718">        return _constructFunctional( null, tensors, expression, doAD );</span>
    }

    /**
     *  In essence tensors are merely fancy wrappers for some form of array of any type...
     *  This wrapper usually stays the same for a given data array.
     *  However, sometimes a tensor changes its identity, or rather the underlying
     *  data changes the wrapping tensor instance.
     *
     *  This change currently only happens when tensors are being instantiated by
     *  passing inputs and a math expression to its constructor.
     *  This triggers the creation of a Function instance and execution on the provided
     *  input tensors. In that case the output tensor will be created somewhere
     *  along the execution call stack, however the result is expected to be
     *  stored within the tensor whose constructor initialized all of this.
     *  In that case this tensor will rip out the guts of the resulting output
     *  tensor and stuff onto its own field variables. &lt;br&gt;
     *  &lt;br&gt;
     *
     * @param tensors The tensors which will be passed to the function.
     * @param expression The expression defining a function.
     * @param doAD The flag which will enable or disable autograd for the instantiated Function.
     */
    private static &lt;V&gt; Tsr&lt;V&gt; _constructFunctional( Tsr&lt;V&gt; drain, Tsr&lt;V&gt;[] tensors, String expression, boolean doAD )
    {
<span class="pc bpc" id="L743" title="3 of 6 branches missed.">        if ( tensors == null || tensors.length == 0 || tensors[ 0 ] == null ) return drain;</span>
<span class="fc" id="L744">        return Function.of(expression, doAD).call( tensors );</span>
    }


    /*==================================================================================================================
    |
    |       Â§(2) : FLAGS
    |   ----------------------
    */
    /*
        --------------------------------------------
            Â§(2.0) : GRADIENT REQUIREMENT  :
        --------------------------------------------
    */

    /**
     *  Settings this flag via this setter will indirectly trigger the activation of
     *  the autograd / auto-differentiation system of this library!
     *  If the flag is set to 'true' and the tensor is used for computation then
     *  it will also receive gradients when the {@link #backward()} method is being called
     *  on any descendant tensor within the computation graph.
     *
     * @param rqsGradient The truth value determining if this tensor ought to receive gradients via
     *                     the built in automatic backpropagation system.
     * @return This very {@link Tsr} instance in order to enable method chaining.
     */
    public Tsr&lt;V&gt; setRqsGradient(boolean rqsGradient ) {
<span class="fc bfc" id="L771" title="All 4 branches covered.">        if ( rqsGradient() != rqsGradient &amp;&amp; !rqsGradient ) this.remove( Tsr.class );</span>
<span class="fc" id="L772">        _setRqsGradient( rqsGradient );</span>
<span class="fc" id="L773">        return this;</span>
    }

    /**
     *  This flag will indirectly trigger the activation of the autograd / auto-differentiation system of this library!
     *  If the flag is set to 'true' and the tensor is used for computation then
     *  it will also receive gradients when the {@link #backward()} method is being called
     *  on any descendant tensor within the computation graph.
     *
     * @return The truth value determining if this tensor ought to receive gradients via
     *         the built in automatic backpropagation system.
     */
<span class="fc bfc" id="L785" title="All 2 branches covered.">    public boolean rqsGradient() { return ( _flags &amp; RQS_GRADIENT_MASK ) == RQS_GRADIENT_MASK; }</span>

    protected void _setRqsGradient( boolean rqsGradient ) {
<span class="fc bfc" id="L788" title="All 2 branches covered.">        if ( rqsGradient() != rqsGradient ) {</span>
<span class="fc bfc" id="L789" title="All 2 branches covered.">            if ( rqsGradient ) _flags += RQS_GRADIENT_MASK;</span>
<span class="fc" id="L790">            else               _flags -= RQS_GRADIENT_MASK;</span>
        }
<span class="fc" id="L792">    }</span>

    /*
    ---------------------------------------------
        Â§(2.1) : SOURCE LOCATION (DEVICE)  :
    ---------------------------------------------
    */

    public Tsr&lt;V&gt; setIsOutsourced( boolean isOutsourced ) {
<span class="fc" id="L801">        _setIsOutsourced( isOutsourced );</span>
<span class="fc bfc" id="L802" title="All 2 branches covered.">        if ( isOutsourced )</span>
<span class="fc" id="L803">            _setData( null );</span>
<span class="fc" id="L804">        else if (</span>
<span class="fc bfc" id="L805" title="All 2 branches covered.">                !forComponent(</span>
                        Device.class,
                        device -&gt; {
                            try {
<span class="pc bpc" id="L809" title="1 of 2 branches missed.">                                if ( device.has( this ) ) device.restore( this );</span>
<span class="nc" id="L810">                            } catch ( Exception exception ) {</span>
<span class="nc" id="L811">                                _LOG.error(</span>
                                        &quot;Tensor could not be restored from device component when trying to migrate it back to RAM.&quot;,
                                        exception
                                );
<span class="nc" id="L815">                                throw exception;</span>
<span class="fc" id="L816">                            }</span>
<span class="fc" id="L817">                            this.remove( Device.class );</span>
<span class="fc" id="L818">                            forComponent(</span>
                                    Tsr.class,
                                    gradient -&gt;
<span class="fc" id="L821">                                            ( (Tsr&lt;V&gt;) gradient ).forComponent(</span>
                                                    Device.class,
                                                    gradDevice -&gt; {
                                                        try {
<span class="pc bpc" id="L825" title="1 of 2 branches missed.">                                                            if ( gradDevice.has( gradient ) ) gradDevice.restore( gradient );</span>
                                                        }
<span class="nc" id="L827">                                                        catch ( Exception exception ) {</span>
<span class="nc" id="L828">                                                            _LOG.error(</span>
                                                                    &quot;Gradient could not be restored from device component when trying to migrate it back to RAM.&quot;,
                                                                    exception
                                                            );
<span class="nc" id="L832">                                                            throw exception;</span>
<span class="fc" id="L833">                                                        }</span>
<span class="fc" id="L834">                                                        gradient.remove( Device.class );</span>
<span class="fc" id="L835">                                                    })</span>
                            );
<span class="fc" id="L837">                        }</span>
<span class="pc bpc" id="L838" title="1 of 2 branches missed.">                ) &amp;&amp; getData() == null</span>
        ) {
<span class="fc" id="L840">            _setIsVirtual( true );</span>
<span class="fc" id="L841">            _allocate( 1 ); // Only a single value representing the rest.</span>
        }
<span class="fc" id="L843">        return this;</span>
    }

    /**
     * Outsourced means that the tensor is stored on a {@link Device} implementation instance.
     *
     * @return The truth value determining if the data of this tensor is not actually stored inside of it
     *         in the form of of a traditional primitive JVM array!
     */
<span class="fc bfc" id="L852" title="All 2 branches covered.">    public boolean isOutsourced() { return ( _flags &amp; IS_OUTSOURCED_MASK ) == IS_OUTSOURCED_MASK; }</span>

    protected void _setIsOutsourced( boolean isOutsourced ) {
<span class="fc bfc" id="L855" title="All 2 branches covered.">        if ( isOutsourced() != isOutsourced ) {</span>
<span class="fc bfc" id="L856" title="All 2 branches covered.">            if ( isOutsourced ) _flags += IS_OUTSOURCED_MASK;</span>
<span class="fc" id="L857">            else                _flags -= IS_OUTSOURCED_MASK;</span>
        }
<span class="fc" id="L859">    }</span>

    /*
    --------------------------------------------
        Â§(2.2) : VIRTUAL / ACTUAL  :
    --------------------------------------------
    */

    /**
     *  Virtualizing is the opposite to actualizing a tensor.
     *  A tensor is virtual if the size of the underlying data is not actually equal to
     *  the number of elements which the tensor claims to store, aka its size.
     *  This is for example the case when initializing a tensor filled with a single
     *  value continuously. In that case the tensor will flag itself as virtual and only allocate the
     *  underlying data array to hold a single item even though the tensor might actually hold
     *  many more items.
     *  The reasons for this feature is that it greatly improves performance in certain cases.
     *  In essence this feature is a form of lazy loading.
     *  &lt;br&gt;&lt;br&gt;
     *
     * @param isVirtual The truth value determining if this tensor ought to be virtualized.
     * @return This very tensor to enable method chaining.
     */
    @Override
    public Tsr&lt;V&gt; setIsVirtual( boolean isVirtual ) {

<span class="pc bpc" id="L885" title="2 of 4 branches missed.">        assert getNDConf() != null;</span>

<span class="fc bfc" id="L887" title="All 2 branches covered.">        if ( isVirtual() != isVirtual ) {</span>
            // Currently we avoid offloading the virtualization by restoring outsourced tensors into RAM...
<span class="fc" id="L889">            Device&lt;V&gt; device = this.get( Device.class );</span>
            try {
<span class="fc bfc" id="L891" title="All 2 branches covered.">                if ( device != null ) device.restore( this );</span>
<span class="nc" id="L892">            } catch ( Exception exception ) {</span>
<span class="nc" id="L893">                _LOG.error(</span>
                        &quot;Tensor could not be restored from device component when changing flag 'isVirtual' to &quot; + isVirtual + &quot;.&quot;,
                        exception
                );
<span class="nc" id="L897">                throw exception;</span>
<span class="fc" id="L898">            }</span>
<span class="fc bfc" id="L899" title="All 2 branches covered.">            if ( isVirtual ) {</span>
<span class="fc bfc" id="L900" title="All 2 branches covered.">                if ( getData() != null ) _virtualize();</span>
            }
<span class="fc" id="L902">            else _actualize();</span>
            // Virtual and actual tensors require a different mapping from a given index to the underlying data..
            // Therefore we need to re-initialize the NDConfiguration object:
<span class="fc bfc" id="L905" title="All 2 branches covered.">            createConstructionAPI().configureFromNewShape( getNDConf().shape(), isVirtual, getData() == null );</span>
<span class="fc bfc" id="L906" title="All 2 branches covered.">            if( isVirtual ) {</span>
<span class="fc" id="L907">                Relation&lt;V&gt; relation = get( Relation.class );</span>
<span class="pc bpc" id="L908" title="1 of 2 branches missed.">                if ( relation!=null )</span>
<span class="nc" id="L909">                    relation.foreachChild( c -&gt; {</span>
<span class="nc" id="L910">                                c._setData( getData());</span>
<span class="nc" id="L911">                                c.setIsVirtual( true );</span>
<span class="nc" id="L912">                            });</span>
<span class="fc" id="L913">            } else {</span>
<span class="pc bpc" id="L914" title="1 of 2 branches missed.">                Tsr&lt;?&gt; parentTensor = ( this.isSlice() ) ? get(Relation.class).getParent() : null;</span>
<span class="pc bpc" id="L915" title="1 of 2 branches missed.">                if ( parentTensor != null ) parentTensor.get( Relation.class ).remove( this );</span>
            }

            try {
<span class="fc bfc" id="L919" title="All 2 branches covered.">                if ( device != null ) device.store( this );</span>
<span class="nc" id="L920">            } catch ( Exception exception ) {</span>
<span class="nc" id="L921">                String message =</span>
                        &quot;Tensor could not be migrated back to host device after changing flag 'isVirtual' to &quot;+isVirtual+&quot;.&quot;;
<span class="nc" id="L923">                _LOG.error(</span>
                        message,
                        exception
                );
<span class="nc" id="L927">                throw new IllegalStateException( message );</span>
<span class="fc" id="L928">            }</span>
<span class="pc bpc" id="L929" title="3 of 4 branches missed.">        } else if ( isVirtual &amp;&amp; getData() == null ) _allocate( 1 ); //&gt; Only a single value representing the rest.</span>
<span class="fc" id="L930">        return this;</span>
    }

    /**
     *  A tensor is virtual if the size of the underlying data is not actually equal to
     *  the number of elements which the tensor claims to store, aka its size.
     *  This is for example the case when initializing a tensor filled with a single
     *  value continuously. In that case the tensor will flag itself as virtual and only allocate the
     *  underlying data array to hold a single item even though the tensor might actually hold
     *  many more items.
     *  The reasons for this feature is that it greatly improves performance in certain cases.
     *  In essence this feature is a form of lazy loading.
     *  &lt;br&gt;&lt;br&gt;
     * @return The truth value determining if this tensor is virtual (and therefore not &quot;actual&quot;).
     */
    @Override
<span class="fc bfc" id="L946" title="All 2 branches covered.">    public boolean isVirtual() { return ( _flags &amp; IS_VIRTUAL_MASK ) == IS_VIRTUAL_MASK; }</span>

    /**
     *  This method is the inner counterpart to the public &quot;{@link Tsr#setIsVirtual}&quot; method.
     *  It actually performs the bit flipping by applying the corresponding bit mask. &lt;br&gt;
     *  &lt;br&gt;
     * @param isVirtual The truth value which ought to be applied.
     */
    @Override
    protected void _setIsVirtual( boolean isVirtual ) {
<span class="fc bfc" id="L956" title="All 2 branches covered.">        if ( isVirtual() != isVirtual ) {</span>
<span class="fc bfc" id="L957" title="All 2 branches covered.">            if ( isVirtual ) _flags += IS_VIRTUAL_MASK;</span>
<span class="fc" id="L958">            else             _flags -= IS_VIRTUAL_MASK;</span>
        }
<span class="fc" id="L960">    }</span>

    /*
    --------------------------------------------
        Â§(2.3) : GRADIENT APPLY REQUIREMENT  :
    --------------------------------------------
    */

    /**
     *  This flag works alongside two autograd features which can be enables inside the library settings.
     *  They will come into effect when flipping their feature flags, &lt;br&gt;
     *  namely: &lt;i&gt;'isApplyingGradientWhenRequested'&lt;/i&gt; and &lt;i&gt;'isApplyingGradientWhenTensorIsUsed'&lt;/i&gt;&lt;br&gt;
     *  As the first flag name suggests gradients will be applied to their tensors when it is set to true,
     *  however this will only happened when the second flag is set to true as well, because otherwise gradients
     *  wouldn't be applied to their tensors automatically in the first place... &lt;br&gt;
     *  &lt;br&gt;
     *  Setting both flags to true will inhibit effect of the second setting &lt;i&gt;'isApplyingGradientWhenTensorIsUsed'&lt;/i&gt;
     *  unless a form of &quot;permission&quot; is being signaled to the autograd system.
     *  This signal comes in the form of a &quot;request&quot; flag which marks a tensor as &lt;b&gt;allowed to
     *  be updated by its gradient&lt;/b&gt;.&lt;br&gt;
     *  &lt;br&gt;
     * @param applyRequested The truth value determining if the application of the gradient of this tensor is requested.
     * @return This very tensor instance in order to enable method chaining.
     */
    public Tsr&lt;V&gt; setGradientApplyRequested(boolean applyRequested ) {
<span class="fc bfc" id="L985" title="All 2 branches covered.">        if ( gradientApplyRequested() != applyRequested ) {</span>
<span class="fc bfc" id="L986" title="All 2 branches covered.">            if ( applyRequested ) {</span>
                if (
<span class="fc bfc" id="L988" title="All 2 branches covered.">                        Neureka.get().settings().autograd().isApplyingGradientWhenRequested() &amp;&amp;</span>
<span class="fc bfc" id="L989" title="All 2 branches covered.">                                !Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed()</span>
                )
<span class="fc" id="L991">                    this.applyGradient();</span>
                else
<span class="fc" id="L993">                    _flags += GRADIENT_APPLY_RQD_MASK;</span>
            }
<span class="fc" id="L995">            else _flags -= GRADIENT_APPLY_RQD_MASK;</span>
        }
<span class="fc" id="L997">        return this;</span>
    }

    /**
     *  This flag works alongside two autograd features which can be enables inside the library settings.
     *  They will come into effect when flipping their feature flags, &lt;br&gt;
     *  namely: &lt;i&gt;'isApplyingGradientWhenRequested'&lt;/i&gt; and &lt;i&gt;'isApplyingGradientWhenTensorIsUsed'&lt;/i&gt;&lt;br&gt;
     *  As the first flag name suggests gradients will be applied to their tensors when it is set to true,
     *  however this will only happened when the second flag is set to true as well, because otherwise gradients
     *  wouldn't be applied to their tensors automatically in the first place... &lt;br&gt;
     *  &lt;br&gt;
     *  Setting both flags to true will inhibit the effect of the second setting &lt;i&gt;'isApplyingGradientWhenTensorIsUsed'&lt;/i&gt;
     *  unless a form of &quot;permission&quot; is being signaled to the autograd system.
     *  This signal comes in the form of a &quot;request&quot; flag which marks a tensor as &lt;b&gt;allowed to
     *  be updated by its gradient&lt;/b&gt;.&lt;br&gt;
     *  &lt;br&gt;
     * @return The truth value determining if the application of the gradient of this tensor is requested.
     */
<span class="fc bfc" id="L1015" title="All 2 branches covered.">    public boolean gradientApplyRequested() { return ( _flags &amp; GRADIENT_APPLY_RQD_MASK ) == GRADIENT_APPLY_RQD_MASK; }</span>

    /*
    --------------------------------------------
        Â§(2.4) : DELETION  :
    --------------------------------------------
    */

    public boolean isDeleted() {
<span class="fc bfc" id="L1024" title="All 2 branches covered.">        return ( _flags &amp; WAS_DELETED_MASK ) == WAS_DELETED_MASK;</span>
    }

    /**
     *  Although tensors will be garbage collected when they are not strongly referenced,
     *  there is also the option to manually free up the tensor and its associated data.
     *  This is especially useful when tensors are stored on a device like the OpenCLDevice.
     *  In that case calling the &quot;{@link Tsr#delete()}&quot; method will free the memory reserved for this tensor.
     *  This manual memory freeing through this method can be faster than waiting for
     *  the garbage collector to kick in... &lt;br&gt;
     *  &lt;br&gt;
     *
     * @return This very tensor instance to allow for method chaining.
     */
    public Tsr&lt;V&gt; delete()
    {
<span class="fc" id="L1040">        forComponent( GraphNode.class, n -&gt; {</span>
<span class="fc bfc" id="L1041" title="All 2 branches covered.">            if ( n.isUsedAsDerivative() ) {</span>
<span class="fc" id="L1042">                String message = &quot;Cannot delete a tensor which is used as derivative by the AD computation graph!&quot;;</span>
<span class="fc" id="L1043">                _LOG.error( message );</span>
<span class="fc" id="L1044">                throw new IllegalStateException( message );</span>
            }
<span class="fc" id="L1046">        });</span>
<span class="fc" id="L1047">        forComponent( Device.class, device -&gt; device.free( this ) );</span>
<span class="fc" id="L1048">        _setData( null );</span>
<span class="fc" id="L1049">        setNDConf( null );</span>
<span class="fc" id="L1050">        _flags = -1;</span>
<span class="fc" id="L1051">        forComponent( Tsr.class, Tsr::delete );</span>
<span class="fc" id="L1052">        _deleteComponents();</span>
<span class="pc bpc" id="L1053" title="1 of 2 branches missed.">        if ( !isDeleted() ) _flags += IS_OUTSOURCED_MASK;</span>
<span class="fc" id="L1054">        return this;</span>
    }

    /*==================================================================================================================
    |
    |       Â§(3) : COMPONENT SYSTEM
    |   --------------------------------
    */
    /*
    --------------------------------------------
        Â§(3.0) : SETTING / REJECTING  :
    --------------------------------------------
    */

    /**
     * This method is executed when a new Component is added to the tensor.
     * The public add method is implemented in the super class
     * '{@link AbstractComponentOwner}' from which this class inherits.
     * In this super class the component logic is implemented.
     *
     * @param newComponent A component used to access features. ({@link GraphNode}, {@link NDFrame}, {@link Relation}, int[], ...)
     * @return The unchanged object or maybe in future versions: null (component rejected)
     */
    @Override
    protected &lt; T extends Component&lt;Tsr&lt;V&gt;&gt; &gt; T _setOrReject( T newComponent )
    {
<span class="fc" id="L1080">        return newComponent;</span>
    }

    /*
    --------------------------------------------
        Â§(3.1) : REMOVING / REJECTING  :
    --------------------------------------------
    */
    /**
     * This method is executed when a component is being removed from the tensor.
     * The public remove method is implemented in the super class
     * '{@link AbstractComponentOwner}' from which this class inherits.
     * In this super class the component logic is implemented.
     *
     * @param newComponent A component used to access features. ({@link GraphNode}, {@link NDFrame}, {@link Relation}, int[], ...)
     * @return The unchanged object or when rejected: null (component rejected)
     */
    @Override
    protected &lt;T extends Component&lt;Tsr&lt;V&gt;&gt;&gt; T _removeOrReject(T newComponent )
    {
<span class="fc bfc" id="L1100" title="All 2 branches covered.">        if ( newComponent instanceof Device ) {</span>
<span class="fc" id="L1101">            Device&lt;V&gt; device = (Device&lt;V&gt;) newComponent;</span>
            /*
                The following seems like a redundant check, however often times a tensor
                will be removed from a Device implementation inside the &quot;restore&quot; method
                when the tensor has already been removed from the device...
                With out the condition below a stack overflow would occur!
             */
<span class="fc bfc" id="L1108" title="All 2 branches covered.">            if ( device.has( this ) ) {</span>
                try {
<span class="fc" id="L1110">                    device.restore( this );</span>
<span class="nc" id="L1111">                } catch ( Exception exception ) {</span>
<span class="nc" id="L1112">                    _LOG.error(</span>
                            &quot;Removing device from tensor / tensor from device failed.\n&quot; +
                            &quot;Restoring tensor from device threw exception.\n&quot;,
                            exception
                    );
<span class="nc" id="L1117">                    throw exception;</span>
<span class="fc" id="L1118">                }</span>
            }
        }
<span class="fc" id="L1121">        return newComponent;</span>
    }

    /*
    ----------------------------
        Â§(3.2) : UPDATING  :
    ----------------------------
    */
    /**
     *  Important : Components of type {@link Tsr} are simply gradients!
     *  Currently this method is used only to catch illegal arguments which
     *  is for example the case when trying to attach a gradient with a different shape...
     *  (Otherwise the gradient tensor &quot;does not mind&quot; an owner change...)
     */
    @Override
    public boolean update( OwnerChangeRequest&lt;Tsr&lt;V&gt;&gt; changeRequest ) {
<span class="fc bfc" id="L1137" title="All 2 branches covered.">        if ( changeRequest.type() == IsBeing.ADDED ) {</span>
<span class="fc" id="L1138">            if (</span>
<span class="pc bpc" id="L1139" title="1 of 2 branches missed.">                    changeRequest.getNewOwner().shape().hashCode() != this.shape().hashCode() ||</span>
<span class="pc bpc" id="L1140" title="1 of 2 branches missed.">                            Arrays.hashCode(changeRequest.getNewOwner().getNDConf().shape()) != Arrays.hashCode( getNDConf().shape() )</span>
            ) {
<span class="nc" id="L1142">                throw new IllegalArgumentException(</span>
                        &quot;Trying to attach a tensor as gradient component to a tensor with different shape.&quot;
                );
            }
        }
<span class="fc" id="L1147">        changeRequest.executeChange();</span>
        // If the change request type is set to &quot;REPLACED&quot; then
        // this is means that this tensor is a gradient that is being
        // transferred to another tensor to serve as gradient...
        // No update task needs to occur. (This might change in the future...)
<span class="fc" id="L1152">        return true;</span>
    }

    /**
     * This method taked a {@link Device} and tries to migrate the contents of this {@link Tsr}
     * instance to that {@link Device}!
     *
     * @param device The {@link Device} which should host this {@link Tsr} as well as be added to its components list.
     * @return This very class to enable method chaining.
     */
<span class="fc" id="L1162">    public Tsr&lt;V&gt; to(Device&lt;?&gt; device ){ super._set(device); return this; }</span>

    /*==================================================================================================================
    |
    |       Â§(4) : PROPERTIES :
    |   ---------------------------------------
    */
    /*
    --------------------------------------------
        Â§(4.0) : HIGH LEVEL PROPERTIES  :
    --------------------------------------------
    */

    /**
     *  A tensor is empty if there is neither data referenced within the tensor directly
     *  or within any given device to which the tensor might belong.
     *
     * @return The truth value determining if this tensor has data.
     */
<span class="fc bfc" id="L1181" title="All 4 branches covered.">    public boolean isEmpty() { return getData() == null &amp;&amp; !this.isOutsourced(); }</span>

    /**
     *  A tensor is &quot;undefined&quot; if it has either no {@link NDConfiguration} implementation instance
     *  or this instance does not have a shape set for this {@link Tsr} which is needed for
     *  a tensor to also have a rank and dimensionality...
     *
     * @return The truth value determining if this tensor has an {@link NDConfiguration} stored internally.
     */
<span class="pc bpc" id="L1190" title="2 of 4 branches missed.">    public boolean isUndefined() { return getNDConf() == null || getNDConf().shape() == null; }</span>

    /**
     *  If this tensor is a slice of a parent tensor then this method will yield true.
     *  Slices can be created by calling the variations of the &quot;{@link Tsr#getAt}&quot; method.
     *
     * @return The truth value determining if this tensor is a slice of another tensor.
     */
    public boolean isSlice() {
<span class="fc" id="L1199">        Relation&lt;V&gt; child = get( Relation.class );</span>
<span class="fc bfc" id="L1200" title="All 4 branches covered.">        return ( child != null &amp;&amp; child.hasParent() );</span>
    }

    /**
     *  This method returns the number of slices which have been
     *  created from this very tensor.
     *  It does so by accessing the {@link Relation} component if present
     *  which internally keeps track of slices via weak references.
     *
     * @return The number of slices derived from this tensor.
     */
    public int sliceCount() {
<span class="fc" id="L1212">        Relation&lt;V&gt; child = get( Relation.class );</span>
<span class="pc bpc" id="L1213" title="1 of 2 branches missed.">        return ( child != null ) ? child.childCount() : 0;</span>
    }

    /**
     *  If slices have been derived from this tensor then it is a &quot;slice parent&quot;.
     *  This is what this method will determine, in which case, it will return true.
     *
     * @return The truth value determining if slices have been derived from this tensor.
     */
    public boolean isSliceParent() {
<span class="fc" id="L1223">        Relation&lt;V&gt; parent = get( Relation.class );</span>
<span class="pc bpc" id="L1224" title="1 of 4 branches missed.">        return ( parent != null &amp;&amp; parent.hasChildren() );</span>
    }

    /**
     *  Tensors which are used or produced by the autograd system will have a {@link GraphNode} component attached to them.
     *  This is because autograd requires recording a computation graph for back-prop traversal.
     *  This autograd system however, will only be triggered by {@link Function} implementations which
     *  are not &quot;detached&quot;, meaning they have their &quot;{@link Function#isDoingAD()}&quot; flags set to true! &lt;br&gt;
     *  Detached functions (like those pre-instantiated in Function.Detached.*) will not attach {@link GraphNode}
     *  instances to involved tensors which will prevent the formation of a computation graph.
     *
     * @return The truth value determining if this tensor belongs to a recorded computation graph.
     */
<span class="fc" id="L1237">    public boolean belongsToGraph() { return this.has( GraphNode.class ); }</span>

    /**
     *  Tensors which are used or produced by the autograd system will have a {@link GraphNode} component attached to them.
     *  This is because autograd requires recording a computation graph for back-prop traversal.
     *  This autograd system however, will only be triggered by {@link Function} implementations which
     *  are not &quot;detached&quot;, meaning they have their &quot;{@link Function#isDoingAD()}&quot; flags set to true! &lt;br&gt;
     *  A tensor is a leave if it is attached to a computation graph in which it is not an intermediate / branch node
     *  but input / branch node.
     *
     * @return The truth value determining if this tensor is attached to a computation graph as leave node.
     */
<span class="pc bpc" id="L1249" title="1 of 4 branches missed.">    public boolean isLeave() { return (!this.has( GraphNode.class )) || this.get( GraphNode.class ).isLeave(); }</span>

    /**
     *  Tensors which are used or produced by the autograd system will have a {@link GraphNode} component attached to them.
     *  This is because autograd requires recording a computation graph for back-prop traversal.
     *  This autograd system however, will only be triggered by {@link Function} implementations which
     *  are not &quot;detached&quot;, meaning they have their &quot;{@link Function#isDoingAD()}&quot; flags set to true! &lt;br&gt;
     *  A tensor is a branch if it is attached to a computation graph in which it is not an input / leave node
     *  but intermediate / branch node.
     *
     * @return The truth value determining if this tensor is attached to a computation graph as branch node.
     */
<span class="fc bfc" id="L1261" title="All 2 branches covered.">    public boolean isBranch() { return !this.isLeave(); }</span>

    /**
     *  Tensors can be components of other tensors which makes the
     *  implicitly their gradients.
     *
     * @return The truth value determining if this tensor has another tensor attached to it (which is its gradient).
     */
<span class="fc" id="L1269">    public boolean hasGradient() { return this.has( Tsr.class ); }</span>

    /*
        ----------------------------------------------
            Â§(4.1) : COMPONENT BASED PROPERTIES :
        ----------------------------------------------
     */

    /**
     * @return The gradient of this tensor which is internally stored as component.
     */
<span class="fc" id="L1280">    public Tsr&lt;V&gt; getGradient() { return this.get( Tsr.class ); }</span>

    /**
     * @return The device on which this tensor is stored or 'CPU' if it is not outsourced.
     */
    public Device&lt;V&gt; getDevice() {
<span class="fc bfc" id="L1286" title="All 2 branches covered.">        if ( this.isOutsourced() ) return this.get( Device.class );</span>
<span class="fc" id="L1287">        return (Device&lt;V&gt;) _CPU;</span>
    }

    /**
     * @return The graph node of the computation graph to which this tensor belongs or null if not part of a graph.
     */
<span class="fc" id="L1293">    public GraphNode&lt;V&gt; getGraphNode() { return get( GraphNode.class ); }</span>

    /**
     * @return An instance of the {@link NDFrame} component if present.
     */
<span class="fc" id="L1298">    public NDFrame&lt;V&gt; frame() { return get( NDFrame.class ); }</span>


    /*
        ---------------------------------------
            Â§(4.2) : INNER PROPERTIES :
        ---------------------------------------
     */


    /*==================================================================================================================
    |
    |       Â§(5) : OBJECT STATE MODIFICATION :
    |   ------------------------------------------
    */
    /**
     *  This method is responsible for incrementing
     *  the &quot;_version&quot; field variable which represents the version of the data of this tensor.
     *  Meaning :
     *  Every time the underlying data (_value) changes this version ought to increment alongside.
     *  The method is called during the execution procedure.
     *
     * @param call The context object containing all relevant information that defines a call for tensor execution.
     * @return This very tensor instance. (factory pattern)
     */
    public Tsr&lt;V&gt; incrementVersionBecauseOf( ExecutionCall&lt;?&gt; call ) {
<span class="fc bfc" id="L1324" title="All 2 branches covered.">        if ( Neureka.get().settings().autograd().isPreventingInlineOperations() ) {</span>
<span class="fc" id="L1325">            _version++;</span>
<span class="fc" id="L1326">            GraphNode&lt;?&gt; node = get( GraphNode.class );</span>
<span class="pc bpc" id="L1327" title="1 of 4 branches missed.">            if ( node != null &amp;&amp; node.getPayloadReferenceVersion() != _version ) {</span>
<span class="pc bpc" id="L1328" title="1 of 4 branches missed.">                if ( node.usesAD() || node.isUsedAsDerivative() ) {</span>
<span class="fc" id="L1329">                    String error = &quot;Inline operation occurred on tensor which is part of a computation graph node with autograd support!\n&quot; +</span>
<span class="fc" id="L1330">                            &quot;The following OperationType caused an internal version mismatch: '&quot;+call.getOperation().getFunction()+&quot;'&quot;;</span>
<span class="fc" id="L1331">                    _LOG.error( error );</span>
<span class="fc" id="L1332">                    throw new IllegalStateException( error );</span>
                }
            }
        }
<span class="fc" id="L1336">        return this;</span>
    }

    /**
     *  In essence tensors are merely fancy wrapper for some form of array of any type... 
     *  This wrapper usually stays the same of a given data array.
     *  However, sometimes a tensor changes its identity, or rather the underlying
     *  data changes the wrapping tensor instance. &lt;br&gt;
     *  &lt;br&gt;
     *  This change currently only happens when tensors are being instantiated by
     *  certain constructors to which input tensors and a math expression is passed.
     *  This triggers the creation of a {@link Function} instance and execution on the provided
     *  input tensors. In that case the output tensor will be created somewhere
     *  along the execution call stack, however the result is expected to be 
     *  stored within the tensor whose constructor initialized all of this.
     *  In that case this tensor will rip out the guts of the resulting output
     *  tensor and stuff onto its own field variables.
     * 
     * @param tensor The tensor whose identity should be stolen.
     * @return This very tensor instance in order to enable method chaining.
     */
    @Deprecated( since = &quot;0.7&quot; ) // This ought to be removed due to the fact that tensor instantiation is now factory method based.
    protected Tsr&lt;V&gt; _become( Tsr&lt;V&gt; tensor )
    {
<span class="pc bpc" id="L1360" title="1 of 2 branches missed.">        if ( tensor == null ) return this;</span>
<span class="fc" id="L1361">        this.setDataType( tensor.getDataType() );</span>
<span class="fc" id="L1362">        _setData( tensor.getData() );</span>
<span class="fc" id="L1363">        setNDConf( tensor.getNDConf() );</span>
<span class="fc" id="L1364">        _flags = tensor._flags;</span>
<span class="fc" id="L1365">        _transferFrom( tensor );</span>
<span class="fc" id="L1366">        tensor._setData( null );</span>
<span class="fc" id="L1367">        tensor.setDataType( null );</span>
<span class="fc" id="L1368">        tensor.setNDConf( null );</span>
<span class="fc" id="L1369">        tensor._flags = -1;</span>
<span class="fc" id="L1370">        return this;</span>
    }



    /*==================================================================================================================
    |
    |       Â§(6) : ND-ITERATOR LOGIC :
    |   ---------------------------------------
    */

    /**
     * This method returns an iterator over the elements of this tensor. &lt;br&gt;
     *
     * @return An iterator over elements of type ValType.
     */
    @NotNull
    @Override
    public Iterator&lt;V&gt; iterator()
    {
<span class="fc" id="L1390">        NDIterator _ndi = NDIterator.of( this );</span>
<span class="fc" id="L1391">        return new Iterator&lt;V&gt;()</span>
<span class="fc" id="L1392">        {</span>
<span class="fc" id="L1393">            private int _count = 0;</span>
<span class="fc" id="L1394">            private final int _size = size();</span>

            @Override
            public boolean hasNext() {
<span class="fc bfc" id="L1398" title="All 2 branches covered.">                return _count != _size;</span>
            }

            @Override
            public V next() {
<span class="fc" id="L1403">                V value = getDataAt( _ndi.i() );</span>
<span class="fc" id="L1404">                _ndi.increment();</span>
<span class="fc" id="L1405">                _count ++;</span>
<span class="fc" id="L1406">                return value;</span>
            }
        };
    }


    /*==================================================================================================================
    |
    |       Â§(7) : COMPONENT SPECIFIC :
    |   ---------------------------------------
    */
    /*
        -------------------------------
            Â§(7.0) : AUTO-GRAD :
        -------------------------------
        ... for more context see package 'autograd' ...
     */

    /**
     *  Tensors which are used or produced by the autograd system will have a {@link GraphNode} component attached to them.
     *  This is because autograd requires recording a computation graph for back-prop traversal.
     *  If this tensor is part of a computation graph then this method
     *  will traverse an error backward in the recorded history towards tensors which require
     *  the accumulation of gradients.
     *
     * @param error A tensor which is back-propagated to gradients. Must match the size og this tensor.
     * @return The tensor on which this method was called. (factory pattern)
     */
    public Tsr&lt;V&gt; backward( Tsr&lt;V&gt; error ) {
<span class="fc bfc" id="L1435" title="All 2 branches covered.">        if ( this.isOutsourced() ) {</span>
<span class="fc" id="L1436">            error = error.clone();</span>
<span class="fc" id="L1437">            error = error.to(this.getDevice());</span>
        }
<span class="fc" id="L1439">        Tsr&lt;V&gt; finalError = error;</span>
<span class="pc bpc" id="L1440" title="1 of 4 branches missed.">        if ( !forComponent( GraphNode.class, node -&gt; node.backward(finalError) ) &amp;&amp; this.rqsGradient() ) {</span>
<span class="fc" id="L1441">            addToGradient( error );</span>
        }
<span class="fc" id="L1443">        return this;</span>
    }

    /**
     *  Tensors which are used or produced by the autograd system will have a {@link GraphNode} component attached to them.
     *  This is because autograd requires recording a computation graph for back-prop traversal.
     *  If this tensor is part of a computation graph then this method
     *  will traverse an error backward in the recorded history towards tensors which require
     *  the accumulation of gradients.&lt;br&gt;
     *  &lt;br&gt;
     *  This method turns the given scalar value and
     *  turns it into a matching tensor ( with the same shape)
     *  which will then be back-propagated through the
     *  recorded computation graph.
     *
     * @param value A scalar which is back-propagated to gradients. Must match the size og this tensor.
     * @return The tensor on which this method was called. (factory pattern)
     */
    public Tsr&lt;V&gt; backward( double value ) {
<span class="fc" id="L1462">        backward( new Tsr&lt;&gt;( getNDConf().shape(), value ) );</span>
<span class="fc" id="L1463">        return this;</span>
    }

    /**
     *  Tensors which are used or produced by the autograd system will have a {@link GraphNode} component attached to them.
     *  This is because autograd requires recording a computation graph for back-prop traversal.
     *  If this tensor is part of a computation graph then this method
     *  will traverse an error backward in the recorded history towards tensors which require
     *  the accumulation of gradients. &lt;br&gt;
     *  &lt;br&gt;
     *  This method assumes that the user wants to back-propagate
     *  an error of &quot;1&quot; having the same shape as
     *  this tensor.
     *
     * @return The tensor on which this method was called. (factory pattern)
     */
    public Tsr&lt;V&gt; backward()
    {
<span class="fc" id="L1481">        backward( 1 ); // By default we back-propagate a base factor of 1.</span>
<span class="fc" id="L1482">        return this;</span>
    }

    /**
     *  If this tensor owns a gradient tensor as component, then it can be applied by this method. &lt;br&gt;
     *  &quot;Applying&quot; a gradient to a tensor simply means adding the values inside the gradient element-wise
     *  to the owning host tensor via an inline operation. &lt;br&gt;
     */
    public void applyGradient()
    {
        /*
           If the tensor has a JITProp component then it will trigger the continuation of the back-propagation which
           has been put on hold by saving the pending graph nodes inside the component. &lt;br&gt;
           This is because the gradient most likely has not yet been fully calculated.
         */
<span class="fc" id="L1497">        forComponent( JITProp.class, JITProp::execute );</span>
        // Afterwards the JITProp component is not needed anymore! So we remove it.
<span class="fc" id="L1499">        remove( JITProp.class );</span>
        // Now the gradient can be applied (Gradients are also tensors, which is why we provide its class as key).
<span class="fc" id="L1501">        forComponent(</span>
                Tsr.class,
                g -&gt; {
                    // If an optimizer is present then we also optimize the gradient first!
<span class="fc bfc" id="L1505" title="All 2 branches covered.">                    if ( this.has( Optimizer.class ) )</span>
<span class="fc" id="L1506">                        g = this.get(Optimizer.class).optimize( this );</span>
                    // And then we remove the gradient because it is no longer needed.
<span class="fc" id="L1508">                    remove( Tsr.class );</span>
                    // We are now ready to apply the gradient to the tensor. This is an inline operation!
                    // Therefore we need to turn off the inline operation safety net:
<span class="fc" id="L1511">                    boolean inlineSafety = Neureka.get().settings().autograd().isPreventingInlineOperations();</span>
<span class="pc bpc" id="L1512" title="1 of 2 branches missed.">                    if ( inlineSafety ) Neureka.get().settings().autograd().setIsPreventingInlineOperations( false );</span>
                    // INLINE OPERATION :
<span class="fc" id="L1514">                    Neureka.get().context().getFunction().plusAssign().call( this, g ); //-&gt; Finally applying the gradient!</span>
                    // INLINE END ! -&gt; We can now revert to the previous setting:
<span class="pc bpc" id="L1516" title="1 of 2 branches missed.">                    if ( inlineSafety ) Neureka.get().settings().autograd().setIsPreventingInlineOperations( true );</span>
<span class="fc" id="L1517">                }</span>
        );
<span class="fc" id="L1519">    }</span>

    /**
     *  &lt;b&gt;This method detaches this tensor from its underlying computation-graph
     *  or simply does nothing if no graph is present.&lt;/b&gt; &lt;br&gt;
     *  Nodes within a computation graph are instances of the &quot;{@link GraphNode}&quot; class which are also
     *  simple components of the tensors they represent in the graph. &lt;br&gt;
     *  Therefore, &quot;detaching&quot; this tensor from the graph simply means removing its {@link GraphNode} component.
     *
     * @return This very instance in order to allows for a more streamline usage of this method.
     */
<span class="fc" id="L1530">    public Tsr&lt;V&gt; detach() { this.remove( GraphNode.class ); return this; }</span>

    /*
        ----------------------------
            Â§(7.1) : FRAMING :
        ----------------------------
        ... for more context see package 'framing'...
     */

    /**
     *  This method receives a nested {@link String} array which
     *  ought to contain a label for the index of this tensor.
     *  The index for a single element of this tensor would be an array
     *  of numbers as long as the rank where every number is
     *  in the range of the corresponding shape dimension...
     *  Labeling an index means that for every dimension there
     *  must be a label for elements in this range array! &lt;br&gt;
     *  For example the shape (2,3) could be labeled as follows:    &lt;br&gt;
     *                                                              &lt;br&gt;
     *      dim 0 : [&quot;A&quot;, &quot;B&quot;]                                      &lt;br&gt;
     *      dim 1 : [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]                                 &lt;br&gt;
     *                                                              &lt;br&gt;
     *
     * @param labels A nested String array containing labels for indexes of the tensor dimensions.
     * @return This tensor (method chaining).
     */
    public Tsr&lt;V&gt; label( String[][] labels )
    {
<span class="fc" id="L1558">        _label( null, labels );</span>
<span class="fc" id="L1559">        return this;</span>
    }

    /**
     *  This method receives a label for this tensor and a
     *  nested {@link String} array which ought to contain a
     *  label for the index of this tensor.
     *  The index for a single element of this tensor would be an array
     *  of numbers as long as the rank where every number is
     *  in the range of the corresponding shape dimension...
     *  Labeling an index means that for every dimension there
     *  must be a label for elements in this range array! &lt;br&gt;
     *  For example the shape (2,3) could be labeled as follows:    &lt;br&gt;
     *                                                              &lt;br&gt;
     *      dim 0 : [&quot;A&quot;, &quot;B&quot;]                                      &lt;br&gt;
     *      dim 1 : [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]                                 &lt;br&gt;
     *                                                              &lt;br&gt;
     *
     * @param tensorName A label for this tensor itself.
     * @param labels A nested String array containing labels for indexes of the tensor dimensions.
     * @return This tensor (method chaining).
     */
    public Tsr&lt;V&gt; label( String tensorName, String[][] labels )
    {
<span class="fc" id="L1583">        _label( tensorName, labels );</span>
<span class="fc" id="L1584">        return this;</span>
    }

    /**
     *  This private method is used by public {@link Tsr#label} methods as a single source of
     *  responsibility for performing the actual labeling based on the user input...
     *
     * @param tensorName The name of this tensor which will be stored in an {@link NDFrame} component.
     * @param labels The label / alias information which will also be stored in an {@link NDFrame} component.
     */
    private void _label( String tensorName, String[][] labels )
    {
<span class="fc" id="L1596">        NDFrame&lt;V&gt; frame = get( NDFrame.class );</span>
<span class="fc bfc" id="L1597" title="All 2 branches covered.">        if ( frame == null ) {</span>
<span class="fc" id="L1598">            frame = new NDFrame( this.rank(), tensorName );</span>
<span class="fc" id="L1599">            set(frame);</span>
        }
<span class="pc bpc" id="L1601" title="2 of 4 branches missed.">        assert labels.length &lt;= this.rank();</span>
<span class="fc bfc" id="L1602" title="All 2 branches covered.">        for( int i = 0; i &lt; labels.length; i++ ) {</span>
<span class="pc bpc" id="L1603" title="1 of 2 branches missed.">            if ( labels[ i ] != null ) {</span>
<span class="fc" id="L1604">                AxisFrame&lt;Integer, V&gt; atAxis = frame.atAxis( i );</span>
<span class="fc bfc" id="L1605" title="All 2 branches covered.">                for ( int ii = 0; ii &lt; labels[ i ].length; ii++ ) {</span>
<span class="pc bpc" id="L1606" title="1 of 2 branches missed.">                    if ( labels[ i ][ ii ] != null )</span>
<span class="fc" id="L1607">                        atAxis.atIndexAlias( labels[ i ][ ii ] ).setIndex( ii );</span>
                }
            }
        }
<span class="fc" id="L1611">    }</span>

    /**
     *  This method receives a nested {@link String} list which
     *  ought to contain a label for the index of this tensor.
     *  The index for a single element of this tensor would be an array
     *  of numbers as long as the rank where every number is
     *  in the range of the corresponding shape dimension...
     *  Labeling an index means that for every dimension there
     *  must be a label for elements in this range array! &lt;br&gt;
     *  For example the shape (2,3) could be labeled as follows: &lt;br&gt;
     *                                                           &lt;br&gt;
     *      dim 0 : [&quot;A&quot;, &quot;B&quot;]                                   &lt;br&gt;
     *      dim 1 : [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]                              &lt;br&gt;
     *                                                           &lt;br&gt;
     * @param labels A nested String list containing labels for indexes of the tensor dimensions.
     * @return This tensor (method chaining).
     */
    public Tsr&lt;V&gt; label( List&lt;List&lt;Object&gt;&gt; labels )
    {
<span class="fc" id="L1631">        NDFrame&lt;V&gt; frame = get( NDFrame.class );</span>
<span class="pc bpc" id="L1632" title="1 of 2 branches missed.">        if ( frame == null ) set( new NDFrame( labels, null ) );</span>
<span class="fc" id="L1633">        return this;</span>
    }

    /**
     *  This method receives a label for this tensor and a nested
     *  {@link String} list which ought to contain a label for the index of
     *  this tensor The index for a single element of this tensor would
     *  be an array of numbers as long as the rank where every number is
     *  in the range of the corresponding shape dimension...
     *  Labeling an index means that for every dimension there
     *  must be a label for elements in this range array! &lt;br&gt;
     *  For example the shape (2,3) could be labeled as follows: &lt;br&gt;
     *                                                           &lt;br&gt;
     *      dim 0 : [&quot;A&quot;, &quot;B&quot;]                                   &lt;br&gt;
     *      dim 1 : [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]                              &lt;br&gt;
     *                                                           &lt;br&gt;
     * @param tensorName A label for this tensor itself.
     * @param labels A nested String list containing labels for indexes of the tensor dimensions.
     * @return This tensor (method chaining).
     */
    public Tsr&lt;V&gt; label( String tensorName, List&lt;List&lt;Object&gt;&gt; labels )
    {
<span class="fc" id="L1655">        NDFrame&lt;V&gt; frame = get( NDFrame.class );</span>
<span class="pc bpc" id="L1656" title="1 of 2 branches missed.">        if ( frame == null ) set( new NDFrame&lt;&gt;( labels, tensorName ) );</span>
<span class="fc" id="L1657">        return this;</span>
    }

    /**
     *  This method provides the ability to
     *  label not only the indices of the shape of this tensor, but also
     *  the dimension of the shape.
     *  The first and only argument of the method expects a map instance
     *  where keys are the objects which ought to act as dimension labels
     *  and the values are lists of labels for the indices of said dimensions.
     *  For example the shape (2,3) could be labeled as follows:            &lt;br&gt;
     *  [                                                                   &lt;br&gt;
     *      &quot;dim 0&quot; : [&quot;A&quot;, &quot;B&quot;],                                           &lt;br&gt;
     *      &quot;dim 1&quot; : [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]                                       &lt;br&gt;
     *  ]                                                                   &lt;br&gt;
     *                                                                      &lt;br&gt;
     * @param labels A map in which the keys are dimension labels and the values are lists of index labels for the dimension.
     * @return This tensor (method chaining).
     */
    public Tsr&lt;V&gt; label( Map&lt;Object, List&lt;Object&gt;&gt; labels )
    {
<span class="fc" id="L1678">        this.set( new NDFrame&lt;&gt;( labels, this, null ) );</span>
<span class="fc" id="L1679">        return this;</span>
    }

    public Tsr&lt;V&gt; label( String tensorName, Map&lt;Object, List&lt;Object&gt;&gt; labels )
    {
<span class="nc" id="L1684">        this.set( new NDFrame&lt;&gt;( labels, this, tensorName ) );</span>
<span class="nc" id="L1685">        return this;</span>
    }

    /*==================================================================================================================
    |
    |       Â§(8) : (OVERLOADABLE) OPERATORS &amp; OPERATIONS :
    |   -----------------------------------------------------
    |       ...for more context see package 'calculus'...
    |*/
    /*
        -----------------------------
            Â§(8.0) : OPERATORS :
        -----------------------------
     */

    /**
     *  The {@link #plus(Tsr)} method will produce the sum of
     *  two arrays with the same rank (or two ranks which can be made compatible with padding ones),
     *  where the left operand is this {@link Tsr}
     *  instance and the right operand is the tensor passed to the method.
     *  If the shapes of both of the involved tensors is identical then
     *  the result will be a regular elementwise addition.
     *  Otherwise the method will also be able to perform broadcasting, however only if
     *  for every pair of shape dimension the following is true:
     *  Either the dimensions have the same size or one of them has size 1. &lt;br&gt;
     *  Here is an example of 2 matching shapes: (1, 4, 1) and (3, 4, 1)       &lt;br&gt;
     *  And here is an example of a mismatch: (2, 4, 1) and (3, 4, 1)         &lt;br&gt;
     *
     * @param other The right operand of the addition.
     * @return The sum of this instance as the left and the passed {@link Tsr} instance as right operand.
     */
    public Tsr&lt;V&gt; plus( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1717">        return Neureka.get().context().getAutogradFunction().plus().call( this, other );</span>
    }

    public Tsr&lt;V&gt; plusAssign( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1721">        return Neureka.get().context().getFunction().plusAssign().call( this, other );</span>
    }

    public Tsr&lt;V&gt; plus( double value ) {
<span class="fc" id="L1725">        return plus( _of( this.shape(), value ) );</span>
    }

    /**
     *  The {@link #minus(Tsr)} method will perform subtraction on
     *  two arrays with the same rank (or two ranks which can be made compatible with padding ones),
     *  where the left operand is this {@link Tsr}
     *  instance and the right operand is the tensor passed to the method.
     *  If the shapes of both of the involved tensors is identical then
     *  the result will be a regular elementwise subtraction.
     *  Otherwise the method will also be able to perform broadcasting, however only if
     *  for every pair of shape dimension the following is true:
     *  Either the dimensions have the same size or one of them has size 1. &lt;br&gt;
     *  Here is an example of 2 matching shapes: (1, 4, 1) and (3, 4, 1)       &lt;br&gt;
     *  And here is an example of a mismatch: (2, 4, 1) and (3, 4, 1)         &lt;br&gt;
     *
     * @param other The right operand of the subtraction.
     * @return The difference between this instance as the left and the passed {@link Tsr} instance as right operand.
     */
    public Tsr&lt;V&gt; minus( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1745">        return Neureka.get().context().getAutogradFunction().minus().call( this, other );</span>
    }

    public Tsr&lt;V&gt; minus( V other ) {
<span class="fc" id="L1749">        return minus(</span>
<span class="fc" id="L1750">                 Tsr.of((Class&lt;V&gt;)this.getDataType().getTypeClass())</span>
<span class="fc" id="L1751">                             .withShape(this.getNDConf().shape())</span>
<span class="fc" id="L1752">                             .all(other)</span>
        );
    }

    public Tsr&lt;V&gt; minusAssign( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1757">        return Neureka.get().context().getFunction().minusAssign().call( this, other );</span>
    }

    public Tsr&lt;V&gt; negative() {
<span class="fc" id="L1761">        return Neureka.get().context().getAutogradFunction().neg().call( this );</span>
    }

    /**
     *  The {@link #multiply(Tsr)} method is synonymous with the {@link #times(Tsr)} method.
     *  Both of which will produce the product of
     *  two arrays with the same rank (or two ranks which can be made compatible with padding ones),
     *  where the left operand is this {@link Tsr}
     *  instance and the right operand is the tensor passed to the method.
     *  If the shapes of both of the involved tensors is identical then
     *  the result will be a regular elementwise product.
     *  Otherwise the method will also be able to perform broadcasting, however only if
     *  for every pair of shape dimension the following is true:
     *  Either the dimensions have the same size or one of them has size 1. &lt;br&gt;
     *  Here is an example of 2 matching shapes: (1, 4, 1) and (3, 4, 1)       &lt;br&gt;
     *  And here is an example of a mismatch: (2, 4, 1) and (3, 4, 1)         &lt;br&gt;
     *
     * @param other The right operand of the multiplication.
     * @return The product of this instance as the left and the passed {@link Tsr} instance as right operand.
     */
    public Tsr&lt;V&gt; multiply( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1782">        return Neureka.get().context().getAutogradFunction().mul().call( this, other );</span>
    }

    public Tsr&lt;V&gt; multiply( V other ) {
<span class="fc" id="L1786">        return multiply(</span>
<span class="fc" id="L1787">                           Tsr.of( (Class&lt;V&gt;) this.getDataType().getTypeClass() )</span>
<span class="fc" id="L1788">                               .withShape( this.getNDConf().shape() )</span>
<span class="fc" id="L1789">                               .all( other )</span>
                        );
    }

    /**
     *  The {@link #times(Tsr)} method is synonymous to the {@link #multiply(Tsr)}.
     *  Both of which will produce the product of
     *  two arrays with the same rank (or two ranks which can be made compatible with padding ones),
     *  where the left operand is this {@link Tsr}
     *  instance and the right operand is the tensor passed to the method.
     *  If the shapes of both of the involved tensors is identical then
     *  the result will be a regular elementwise product.
     *  Otherwise the method will also be able to perform broadcasting, however only if
     *  for every pair of shape dimension the following is true:
     *  Either the dimensions have the same size or one of them has size 1. &lt;br&gt;
     *  Here is an example of 2 matching shapes: (1, 4, 1) and (3, 4, 1)       &lt;br&gt;
     *  And here is an example of a mismatch: (2, 4, 1) and (3, 4, 1)         &lt;br&gt;
     *
     * @param other The right operand of the multiplication.
     * @return The product of this instance as the left and the passed {@link Tsr} instance as right operand.
     */
<span class="fc" id="L1810">    public Tsr&lt;V&gt; times( Tsr&lt;V&gt; other ) { return multiply( other ); }</span>

<span class="fc" id="L1812">    public Tsr&lt;V&gt; times( V other ) { return multiply( other ); }</span>

    public Tsr&lt;V&gt; timesAssign( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1815">        return Neureka.get().context().getFunction().mulAssign().call( this, other );</span>
    }

    public Tsr&lt;V&gt; multiply( double value ) {
<span class="fc" id="L1819">        return multiply( _of( this.shape(), value ) );</span>
    }

    /**
     *  The {@link #div(Tsr)} method will produce the quotient of
     *  two arrays with the same rank (or two ranks which can be made compatible with padding ones),
     *  where the left operand is this {@link Tsr}
     *  instance and the right operand is the tensor passed to the method.
     *  If the shapes of both of the involved tensors is identical then
     *  the result will be a regular elementwise division.
     *  Otherwise the method will also be able to perform broadcasting, however only if
     *  for every pair of shape dimension the following is true:
     *  Either the dimensions have the same size or one of them has size 1. &lt;br&gt;
     *  Here is an example of 2 matching shapes: (1, 4, 1) and (3, 4, 1)       &lt;br&gt;
     *  And here is an example of a mismatch: (2, 4, 1) and (3, 4, 1)         &lt;br&gt;
     *
     * @param other The right operand of the division.
     * @return The quotient of this instance as the left and the passed {@link Tsr} instance as right operand.
     */
    public Tsr&lt;V&gt; div( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1839">        return Neureka.get().context().getAutogradFunction().div().call( this, other );</span>
    }

    public Tsr&lt;V&gt; div( double value ) {
<span class="fc" id="L1843">        return div( _of( this.shape(), value ) );</span>
    }

    public Tsr&lt;V&gt; divAssign( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1847">        return Neureka.get().context().getFunction().divAssign().call( this, other );</span>
    }

    /**
     *  The {@link #mod(Tsr)} method will produce the modulus of
     *  two arrays with the same rank (or two ranks which can be made compatible with padding ones),
     *  where the left operand is this {@link Tsr}
     *  instance and the right operand is the tensor passed to the method.
     *  If the shapes of both of the involved tensors is identical then
     *  the result will be a regular elementwise modulo operation.
     *  Otherwise the method will also be able to perform broadcasting, however only if
     *  for every pair of shape dimension the following is true:
     *  Either the dimensions have the same size or one of them has size 1. &lt;br&gt;
     *  Here is an example of 2 matching shapes: (1, 4, 1) and (3, 4, 1)       &lt;br&gt;
     *  And here is an example of a mismatch: (2, 4, 1) and (3, 4, 1)         &lt;br&gt;
     *
     * @param other The right operand of the modulo operation.
     * @return The modulus of this instance as the left and the passed {@link Tsr} instance as right operand.
     */
    public Tsr&lt;V&gt; mod( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1867">        return Neureka.get().context().getAutogradFunction().mod().call( this, other );</span>
    }

    public Tsr&lt;V&gt; mod( int other ) {
<span class="nc" id="L1871">        return mod((Tsr&lt;V&gt;) Tsr.of(this.getNDConf().shape(), other));</span>
    }

    public Tsr&lt;V&gt; rem( int other ) {
<span class="fc" id="L1875">        return mod((Tsr&lt;V&gt;) Tsr.of(this.getNDConf().shape(), other));</span>
    }

    public Tsr&lt;V&gt; modAssign( Tsr&lt;V&gt; other ) {
<span class="nc" id="L1879">        return Neureka.get().context().getFunction().modAssign().call( this, other );</span>
    }

    /**
     *  The {@link #power(Tsr)} (Tsr)} method will produce the power of
     *  two arrays with the same rank (or two ranks which can be made compatible with padding ones),
     *  where the left operand is this {@link Tsr}
     *  instance and the right operand is the tensor passed to the method.
     *  If the shapes of both of the involved tensors is identical then
     *  the result will be a regular elementwise exponentiation.
     *  Otherwise the method will also be able to perform broadcasting, however only if
     *  for every pair of shape dimension the following is true:
     *  Either the dimensions have the same size or one of them has size 1. &lt;br&gt;
     *  Here is an example of 2 matching shapes: (1, 4, 1) and (3, 4, 1)       &lt;br&gt;
     *  And here is an example of a mismatch: (2, 4, 1) and (3, 4, 1)         &lt;br&gt;
     *
     * @param other The right operand, also known as exponent, of the exponentiation.
     * @return The power of this instance as the left and the passed {@link Tsr} instance as right operand.
     */
    public Tsr&lt;V&gt; power( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1899">        return Neureka.get().context().getAutogradFunction().pow().call( this, other );</span>
    }

    public Tsr&lt;V&gt; power( double value ) {
<span class="fc" id="L1903">        return power( _of( this.shape(), value ) );</span>
    }

    /**
     *  This method is synonymous to the {@link #power(Tsr)} method.
     */
    public Tsr&lt;V&gt; xor( Tsr&lt;V&gt; other ) {
<span class="fc" id="L1910">        return Neureka.get().context().getAutogradFunction().pow().call( this, other );</span>
    }

    public Tsr&lt;V&gt; xor( double value ) {
<span class="fc" id="L1914">        return xor( _of( this.shape(), value ) );</span>
    }

    /*
        -----------------------------
            Â§(8.1) : OPERATIONS :
        -----------------------------
     */

    /**
     *  A method which returns a new {@link Tsr} instance which is a transposed twin of this instance.
     *  Internally this method constructs a new reshape function which will be able to transform as expected.
     *
     * @return A new transposed tensor with the same underlying data as this tensor.
     */
    public Tsr&lt;V&gt; T() // Transposed!
    {
<span class="fc" id="L1931">        StringBuilder operation = new StringBuilder();</span>
<span class="fc bfc" id="L1932" title="All 4 branches covered.">        for ( int i = rank() - 1; i &gt;= 0; i-- ) operation.append( i ).append( ( i == 0 ) ? &quot;&quot; : &quot;, &quot; );</span>
<span class="fc" id="L1933">        operation = new StringBuilder( &quot;[&quot; + operation + &quot;]:(I[ 0 ])&quot; );</span>
<span class="fc" id="L1934">        return _constructFunctional(null, new Tsr[]{this}, operation.toString(), true);</span>
    }

    /**
     *  This method performs various operations by calling {@link Function} instances
     *  in order to ultimately calculate the mean value of all values
     *  of this very tensor!
     *  This scalar tensor is then returned.
     *
     * @return A scalar tensor which is the mean value of all values of this very tensor.
     */
    public Tsr&lt;V&gt; mean() {
<span class="fc" id="L1946">        Tsr&lt;V&gt; ones = new Tsr&lt;&gt;( this.getNDConf().shape(), 1 );</span>
<span class="fc" id="L1947">        Tsr&lt;V&gt; sum = Neureka.get().context().getAutogradFunction().conv().call( this, ones );</span>
<span class="fc" id="L1948">        return Neureka.get().context().getAutogradFunction().div().call( sum, new Tsr&lt;&gt;( this.size() ) );</span>
    }

    /**
     *  This method performs a convolutional based dot product between the last dimension of this tensor
     *  and the first dimension of the passed tensor.
     *
     * @param b The tensor which is the right part of the dot product operation.
     * @return A new tensor which is the dot product of this tensor and the passed one.
     */
    public Tsr&lt;V&gt; convDot(Tsr&lt;V&gt; b ) {
<span class="fc" id="L1959">        Tsr&lt;V&gt; a = this;</span>
<span class="fc" id="L1960">        int[][] fitter = AbstractNDArray.Utility.Indexing.makeFit( a.getNDConf().shape(), b.getNDConf().shape() );</span>
<span class="fc" id="L1961">        boolean doReshape = false;</span>
<span class="fc bfc" id="L1962" title="All 6 branches covered.">        for ( int i = 0; i &lt; fitter[ 0 ].length &amp;&amp; !doReshape; i++ ) if ( fitter[ 0 ][ i ] != i ) doReshape = true;</span>
<span class="pc bpc" id="L1963" title="1 of 6 branches missed.">        for ( int i = 0; i &lt; fitter[ 1 ].length &amp;&amp; !doReshape; i++ ) if ( fitter[ 1 ][ i ] != i ) doReshape = true;</span>
<span class="fc bfc" id="L1964" title="All 2 branches covered.">        if ( doReshape ) {</span>
<span class="fc" id="L1965">            a = Function.of( AbstractNDArray.Utility.Stringify.strConf( fitter[ 0 ] ) + &quot;:(I[ 0 ])&quot; ).call( a );</span>
<span class="fc" id="L1966">            b = Function.of( AbstractNDArray.Utility.Stringify.strConf( fitter[ 1 ] ) + &quot;:(I[ 0 ])&quot; ).call( b );</span>
        }
<span class="fc" id="L1968">        return Neureka.get()</span>
<span class="fc" id="L1969">                        .context()</span>
<span class="fc" id="L1970">                        .getAutogradFunction()</span>
<span class="fc" id="L1971">                        .conv()</span>
<span class="fc" id="L1972">                        .call( a, b )</span>
<span class="fc" id="L1973">                        .dimtrim();</span>
    }

    /**
     *  This method performs a dot product between the last dimension of this tensor
     *  and the first dimension of the passed tensor.
     *  However, currently this method can only handle matrices which means
     *  that it is functionally completely identical to the {@link #matMul(Tsr)} method.
     *
     * @param b The tensor which is the right part of the dot product operation.
     * @return A new tensor which is the dot product of this tensor and the passed one.
     */
    public Tsr&lt;V&gt; dot( Tsr&lt;V&gt; b ) {
<span class="nc bnc" id="L1986" title="All 4 branches missed.">        if ( this.rank() != 2 &amp;&amp; b.rank() != 2 )</span>
<span class="nc" id="L1987">            throw new IllegalStateException(&quot;Not yet implemented!&quot;); // This is not yet available in the backend!</span>
<span class="nc" id="L1988">        return this.matMul( b );</span>
    }

    /**
     *  The {@link #matMul(Tsr)} method will produce the matrix product of
     *  two 2 dimensional arrays, where the left operand is this {@link Tsr}
     *  instaance and the right operand is the tensor passed to the method.
     *
     * @param b The right operand of the matrix multiplication.
     * @return The matrix product of this instance as the left and the passed {@link Tsr} instance as right operand.
     */
    public Tsr&lt;V&gt; matMul( Tsr&lt;V&gt; b ) {
<span class="pc bpc" id="L2000" title="2 of 4 branches missed.">        if ( this.rank() != 2 || b.rank() != 2 ) {</span>
<span class="nc" id="L2001">            String message = &quot;Cannot perform matrix multiplication for tensors whose ranks are not both 2!\n&quot; +</span>
<span class="nc" id="L2002">                             &quot;Encountered ranks: &quot; + this.rank() + &quot;, &quot; + b.rank() + &quot;;&quot;;</span>
<span class="nc" id="L2003">            _LOG.error( message );</span>
<span class="nc" id="L2004">            throw new IllegalArgumentException( message );</span>
        }
<span class="fc" id="L2006">        return Neureka.get().context().getAutogradFunction().matMul().call( this, b );</span>
    }

    /**
     *  This method creates a new tensor sharing the same data and whose shape is trimmed.
     *  A trimmed shape is simply a shape without preceding and trailing ones. &lt;br&gt;
     *  For example the shape (1x4x1x2x1) would be trimmed to (4x1x2).
     *  The underlying operation does not perform a removal of redundant ones all together.
     *  Only ones at the start and the beginning will be removed.
     *  A scalar tensor will not be affected by this operation.
     *
     * @return A tensor with the same underlying data but possibly trimmed shape without preceding or trailing ones.
     */
    public Tsr&lt;V&gt; dimtrim() {
<span class="fc" id="L2020">        return Neureka.get().context().getAutogradFunction().dimTrim().call( this );</span>
    }

    /**
     *  This method name translates to the &quot;in&quot; keyword in Groovy!
     *  The same is true for the &quot;contains&quot; method in Kotlin.
     *  Both methods do the exact same thing, however they exist
     *  for better language support.
     *
     * @param t The tensor which will be checked.
     * @return The answer to the following question: Is the data of the provided tensor a subset of the data of this tensor?
     */
    public boolean isCase( Tsr&lt;V&gt; t ) {
<span class="fc" id="L2033">        boolean[] found = { false };</span>
<span class="fc" id="L2034">        this.forComponent( Relation.class, r -&gt; r.foreachChild( c -&gt; {</span>
<span class="fc bfc" id="L2035" title="All 2 branches covered.">                if ( c.equals( t ) ) found[ 0 ] = true;</span>
<span class="fc" id="L2036">            }));</span>
<span class="fc" id="L2037">        return found[ 0 ];</span>
    }

    /**
     *  This method name translates to the &quot;in&quot; keyword in Kotlin!
     *  The same is true for the &quot;isCase&quot; method in Groovy.
     *  Both methods do the exact same thing, however they exist
     *  for better language support.
     *
     * @param t The tensor which will be checked.
     * @return The answer to the following question: Is the data of the provided tensor a subset of the data of this tensor?
     */
    public boolean contains( Tsr&lt;V&gt; t ) {
<span class="fc" id="L2050">        return isCase( t );</span>
    }


    /*==================================================================================================================
    |
    |       Â§(9) : SLICING, INDEXING &amp; INJECTING :
    |   -----------------------------------------------------
    |       ...for more context see package 'ndim.config'...
    */
    /*
        -----------------------------
            Â§(9.0) : SLICING :
        -----------------------------
     */

    /**
     *  The following method enables access to specific scalar elements within the tensor.
     *  The method name also translates to the subscript operator in Groovy.
     *
     * @param indices The index array of the element which should be returned.
     * @return An element located at the provided index.
     */
    public V getAt( int... indices ) {
<span class="nc" id="L2074">        return getDataAt( getNDConf().indexOfIndices( indices ) );</span>
    }

    /**
     *  The following method enables the creation of tensor slices which access
     *  the same underlying data (possibly from a different view).
     *  The method name also translates to the subscript operator in Groovy.
     *
     * @param args An arbitrary number of arguments which can be used for slicing.
     * @return A slice tensor created based on the passed keys.
     */
    public Tsr&lt;V&gt; getAt( Object... args ) {
<span class="fc" id="L2086">        List&lt;Object&gt; argsList = Arrays.asList( args );</span>
<span class="fc" id="L2087">        return getAt( argsList );</span>
    }

    /**
     *  This getter method creates and returns a slice of the original tensor.
     *  The returned slice is a scalar tensor wrapping a single value element which
     *  is being targeted by the provided integer index.
     *
     * @param i The index of the value item which should be returned as a tensor instance.
     * @return A tensor holding a single value element which is internally still residing in the original tensor.
     */
    public Tsr&lt;V&gt; getAt( int i ) {
<span class="fc" id="L2099">        return getAt( new Object[]{ i, i } );</span>
    }

    /**
     *  The following method returns a raw value item within this tensor
     *  targeted by a scalar index.
     *
     * @param i The scalar index of the value item which should be returned by the method.
     * @return The value item found at the targeted index.
     */
    public V getValueAt( int i ) {
<span class="fc" id="L2110">        return getDataAt( getNDConf().indexOfIndex( i ) );</span>
    }

    /**
     *  This method returns a raw value item within this tensor
     *  targeted by an index array which is expect to hold an index for
     *  every dimension of the shape of this tensor.
     *  So the provided array must have the same length as the
     *  rank of this tensor!
     *
     * @param indices The index array which targets a single value item within this tensor.
     * @return The found raw value item targeted by the provided index array.
     */
    public V getValueAt( int... indices ) {
<span class="fc" id="L2124">        return getDataAt( getNDConf().indexOfIndices( indices ) );</span>
    }

    /**
     *  Individual entries for value items in this tensor can be set
     *  via this method.
     *
     * @param i The scalar index targeting a specific value position within this tensor
     *          which ought to be replaced by the one provided by the second parameter
     *          of this method.
     *
     * @param o The item which ought to be placed at the targeted position.
     * @return This very tensor in order to enable method chaining...
     */
    public Tsr&lt;V&gt; setAt( int i, V o ) {
<span class="fc" id="L2139">        setDataAt( getNDConf().indexOfIndex( i ), o );</span>
<span class="fc" id="L2140">        return this;</span>
    }

    public Tsr&lt;V&gt; getAt( double i ) {
<span class="nc" id="L2144">        return getAt( Collections.singletonList( getNDConf().indicesOfIndex( (int) Math.floor(i) ) ).toArray() );</span>
    }

    public Tsr&lt;V&gt; getAt(BigDecimal i ) {
<span class="fc" id="L2148">        return getAt( Collections.singletonList( getNDConf().indicesOfIndex( (i).intValue() ) ).toArray() );</span>
    }

    public Tsr&lt;V&gt; getAt(Map&lt;?,Integer&gt; rangToStrides )
    {
<span class="pc bpc" id="L2153" title="1 of 2 branches missed.">        if ( rangToStrides == null ) return this;</span>
        // ...not a simple slice... Advanced:
<span class="fc" id="L2155">        return SmartSlicer.slice(</span>
                        new Object[]{rangToStrides},
                        this,
                        this::_sliceOf
                    );
    }

    public Tsr&lt;V&gt; shallowCopy()
    {
<span class="pc bpc" id="L2164" title="1 of 4 branches missed.">        if ( this.isEmpty() || this.isUndefined() ) return this;</span>
<span class="fc" id="L2165">        List&lt;List&lt;Integer&gt;&gt; ranges = new ArrayList&lt;&gt;();</span>
<span class="fc bfc" id="L2166" title="All 2 branches covered.">        for ( int e : this.shape() ) {</span>
<span class="fc" id="L2167">            List&lt;Integer&gt; rangeAsList = new ArrayList&lt;&gt;();</span>
<span class="fc bfc" id="L2168" title="All 2 branches covered.">            for ( int i = 0; i &lt; e; i++ ) rangeAsList.add( i );</span>
<span class="fc" id="L2169">            ranges.add( rangeAsList);</span>
<span class="fc" id="L2170">        }</span>
<span class="fc" id="L2171">        return getAt( ranges.toArray() );</span>
    }

    @Override
    public Tsr&lt;V&gt; clone() {
<span class="fc" id="L2176">        return Neureka.get()</span>
<span class="fc" id="L2177">                        .context()</span>
<span class="fc" id="L2178">                        .getFunction()</span>
<span class="fc" id="L2179">                        .idy()</span>
<span class="fc" id="L2180">                        .call(</span>
<span class="fc" id="L2181">                                (Tsr&lt;V&gt;) Tsr.of(this.shape(), 0.0), this</span>
                        );
    }


    /**
     *  This method enables tensor slicing!
     *  It takes a key of various types and configures a slice
     *  tensor which shares the same underlying data as the original tensor.
     *
     * @param key This object might be a wide range of objects including maps, lists or arrays...
     * @return A slice tensor or scalar value.
     */
    public Tsr&lt;V&gt; getAt( Object key ) {
<span class="pc bpc" id="L2195" title="1 of 2 branches missed.">        if ( key == null ) return this;</span>
<span class="pc bpc" id="L2196" title="3 of 4 branches missed.">        if ( key instanceof Object[] &amp;&amp; ((Object[]) key).length == 0 ) key = new ArrayList&lt;&gt;();</span>
<span class="fc bfc" id="L2197" title="All 4 branches covered.">        if ( key instanceof List &amp;&amp; ( (List&lt;?&gt;) key ).isEmpty() ) {</span>
            /*
                An empty List instance is being interpreted as
                the request to create an identical slice, meaning that the
                resulting tensor views the same data as its parent while not
                being the same instance. (In a sense, its a shallow copy!)
             */
<span class="fc" id="L2204">            return shallowCopy();</span>
        }

<span class="fc bfc" id="L2207" title="All 2 branches covered.">        key = ( key instanceof List ) ? ((List&lt;?&gt;) key).toArray() : key;</span>

<span class="fc bfc" id="L2209" title="All 2 branches covered.">        if ( key instanceof Object[] ) {</span>
<span class="fc" id="L2210">            boolean allInt = true;</span>
<span class="fc bfc" id="L2211" title="All 6 branches covered.">            for ( Object o : (Object[]) key ) allInt = allInt &amp;&amp; o instanceof Integer;</span>
<span class="fc bfc" id="L2212" title="All 4 branches covered.">            if ( allInt &amp;&amp; ( (Object[]) key ).length == rank() ) {</span>
<span class="fc" id="L2213">                int[] newOffset = _intArray((Object[]) key);</span>
<span class="fc bfc" id="L2214" title="All 2 branches covered.">                for ( int i = 0; i &lt; this.rank(); i++ )</span>
<span class="fc bfc" id="L2215" title="All 2 branches covered.">                    newOffset[ i ] = ( newOffset[ i ] &lt; 0 ) ? getNDConf().shape( i ) + newOffset[ i ] : newOffset[ i ];</span>
<span class="fc bfc" id="L2216" title="All 2 branches covered.">                for ( int i = 0; i &lt; this.rank(); i++ )</span>
<span class="fc" id="L2217">                    ((Object[])key)[ i ] = newOffset[ i ];</span>
<span class="fc" id="L2218">                allInt = false;</span>
            }
<span class="fc" id="L2220">            boolean hasScale = false;</span>
<span class="pc bpc" id="L2221" title="1 of 6 branches missed.">            for ( Object o : (Object[]) key ) hasScale = hasScale || o instanceof Map;</span>
<span class="fc" id="L2222">            return SmartSlicer.slice(</span>
<span class="fc bfc" id="L2223" title="All 2 branches covered.">                                    ( allInt ) ? new Object[]{ _intArray( (Object[]) key ) } : (Object[]) key,</span>
                                this,
                                    this::_sliceOf
                                );
        } else {
<span class="fc" id="L2228">            String message = &quot;Cannot create tensor slice from key of type '&quot; + key.getClass().getName() + &quot;'!&quot;;</span>
<span class="fc" id="L2229">            _LOG.error( message );</span>
<span class="fc" id="L2230">            throw new IllegalArgumentException( message );</span>
        }
    }

    /**
     *  This method returns a {@link SliceBuilder} instance exposing a simple builder API
     *  which enables the configuration of a slice of the current tensor via method chaining.    &lt;br&gt;
     *  The following code snippet slices a 3-dimensional tensor into a tensor of shape (2x1x3)  &lt;br&gt;
     * &lt;pre&gt;{@code
     *  myTensor.slice()
     *          .axis(0).from(0).to(1)
     *          .then()
     *          .axis(1).at(5) // equivalent to '.from(5).to(5)'
     *          .then()
     *          .axis().from(0).to(2)
     *          .get();
     * }&lt;/pre&gt;
     *
     * @return An instance of the {@link SliceBuilder} class exposing a readable builder API for creating slices.
     */
    public SliceBuilder&lt;V&gt; slice() {
<span class="fc" id="L2251">        return new SliceBuilder&lt;&gt;( this, this::_sliceOf );</span>
    }

    /**
     *  This method is where the creation of a slice occurs.
     *  When creating a slice via the {@link SliceBuilder} or simply by passing ranges in the form of
     *  arrays, lists or maps to a {@link Tsr#getAt}(...) method, then this method will be called eventually.
     *  The creation of a slice always requires information about the shape of the new slice
     *  its position within the original tensor and also the strides / steps.
     *
     * @param newShape The of the slice which ought to be created.
     * @param newOffset The position of the new slice within this tensor.
     * @param newSpread The spread / steps / strides of the slice within this tensor.
     * @return The newly created slice.
     */
    private Tsr&lt;V&gt; _sliceOf( int[] newShape, int[] newOffset, int[] newSpread )
    {
<span class="fc" id="L2268">        this.setIsVirtual( false );</span>
<span class="fc" id="L2269">        Tsr&lt;V&gt; subset = new Tsr&lt;&gt;();</span>
<span class="fc" id="L2270">        subset.setDataType( this.getDataType() );</span>
<span class="fc" id="L2271">        subset._setData( this.getData() );</span>
<span class="fc" id="L2272">        int[] newTranslation = getNDConf().translation();</span>
<span class="fc" id="L2273">        int[] newIdxmap = NDConfiguration.Utility.newTlnOf( newShape );</span>

<span class="fc bfc" id="L2275" title="All 2 branches covered.">        for ( int i = 0; i &lt; this.rank(); i++ )</span>
<span class="pc bpc" id="L2276" title="1 of 2 branches missed.">            newSpread[ i ] = ( newSpread[i] == 0 ) ? 1 : newSpread[ i ];</span>

<span class="fc bfc" id="L2278" title="All 2 branches covered.">        for ( int i = 0; i &lt; newOffset.length; i++ )</span>
<span class="fc" id="L2279">            newOffset[ i ] = newOffset[ i ] + getNDConf().offset( i ); // Offset is being inherited!</span>

<span class="fc bfc" id="L2281" title="All 2 branches covered.">        Tsr&lt;?&gt; rootTensor = ( this.isSlice() ) ? get( Relation.class ).findRootTensor() : this;</span>
<span class="fc bfc" id="L2282" title="All 2 branches covered.">        Tsr&lt;?&gt; parentTensor = ( this.isSlice() ) ? get( Relation.class ).getParent() : this;</span>
        /*
            The following code check the validity of the slice shape ranges with
            respect to the 'parentTensor' of this new slice.
         */
<span class="fc bfc" id="L2287" title="All 4 branches covered.">        if( parentTensor.rank() != newShape.length || rootTensor != parentTensor ) {</span>
            // TODO! This requires some more thought about how to check this!
            // THIS CASE HAS NOT YET BEEN THOUGHT TROUGH!
<span class="fc" id="L2290">            _LOG.warn(</span>
                    &quot;Exceptional slice request detected. &quot; +
                    &quot;This type of tensor cannot yet be sliced. &quot; +
                    &quot;Please copy this tensor before slicing.&quot;
            );
        } else {
            /*
                1. We know that inside this else branch 'this' tensor is a first order slice!
                (So it is not a slice of a slice... reason : 'rootTensor == parentTensor' )

                2. There is however uncertainty about the 'true shape' of this parent tensor!
                Meaning : It might have been reshaped and could therefore be distorted with
                respect to the slice that is currently being prepared!
                -&gt; This means we have to take this possible reshaping into account!
                Like so:

                The following uses an int array also called 'reshapeRelation'.
                This is simply the 'reshape array' which has been recorded inside the 'Relation' component
                by the 'Reshape' operation! ( Hopefully! :) ... custom shape operations need to consider this as well! )

                The following would occur when : &quot;new Tsr&lt;&gt;(...).T().getAt(...);&quot;
                Transposing a tensor performs an inline reshaping of an identical
                slice of the original tensor! Then again slicing this tensor
                via the 'getAt(...)' method leads us to a situation where
                the following variable is NOT NULL! :
             */
<span class="fc bfc" id="L2316" title="All 2 branches covered.">            int[] reshaped = ( this.isSlice() ) ? parentTensor.get( Relation.class ).getReshapeRelationFor( this ) : null;</span>
<span class="fc bfc" id="L2317" title="All 2 branches covered.">            reshaped = ( reshaped != null ) ? Reshape.invert( reshaped ) : null;</span>
<span class="fc bfc" id="L2318" title="All 2 branches covered.">            for ( int i = 0; i &lt; parentTensor.rank(); i++ ) {</span>
<span class="fc bfc" id="L2319" title="All 2 branches covered.">                int ii = ( reshaped != null ) ? reshaped[ i ] : i;</span>
<span class="fc" id="L2320">                int top = newOffset[ i ] + newShape[ i ];</span>
<span class="fc bfc" id="L2321" title="All 2 branches covered.">                if ( top &gt; parentTensor.shape( ii ) ) {</span>
<span class="fc" id="L2322">                    String message =</span>
                            &quot;Cannot create slice because ranges are out of the bounds of the targeted tensor.\n&quot; +
                                    &quot;At index '&quot; + i + &quot;' : offset '&quot; + newOffset[ i ] + &quot;' + shape '&quot; + newShape[ i ] + &quot;' = '&quot; + top + &quot;',\n&quot; +
<span class="fc" id="L2325">                                    &quot;which is larger than the target shape '&quot; + parentTensor.shape( ii ) + &quot;' at the same index!&quot;;</span>
<span class="fc" id="L2326">                    Exception exception = new IllegalArgumentException( message );</span>
<span class="fc" id="L2327">                    _LOG.error( message, exception );</span>
<span class="fc" id="L2328">                    throw new IllegalArgumentException( exception );</span>
                }
            }
        }

<span class="fc" id="L2333">        subset.setNDConf( AbstractNDC.construct( newShape, newTranslation, newIdxmap, newSpread, newOffset ) );</span>

<span class="fc bfc" id="L2335" title="All 2 branches covered.">        if ( this.isOutsourced() ) {</span>
<span class="fc" id="L2336">            Device&lt;V&gt; device = this.get( Device.class );</span>
<span class="fc" id="L2337">            device.store( subset, this );</span>
<span class="fc" id="L2338">            subset.setIsOutsourced( true );</span>
        }
<span class="pc bpc" id="L2340" title="1 of 2 branches missed.">        if ( this.isVirtual() ) subset.setIsVirtual( true );</span>
<span class="fc" id="L2341">        subset.set( new Relation().addParent( this ) );</span>
<span class="fc" id="L2342">        Relation&lt;V&gt; parent = get( Relation.class );</span>
<span class="fc bfc" id="L2343" title="All 2 branches covered.">        parent = ( parent != null ) ? parent : new Relation&lt;&gt;();</span>
<span class="fc" id="L2344">        parent.addChild( subset );</span>
<span class="fc" id="L2345">        this.set( parent );</span>
<span class="fc" id="L2346">        return subset;</span>
    }




    /*
        -----------------------------
            Â§(9.1) : INJECTING :
        -----------------------------
     */

    /**
     *  This method enables injecting slices of tensor to be assigned into this tensor!
     *  It takes a key of various types which is used to configure a slice
     *  tensor sharing the same underlying data as the original tensor.
     *  This slice is then used to assign the second argument to it, namely
     *  the &quot;value&quot; argument.
     *
     * @param key This object is a list defining a targeted index or range of indices...
     * @return A slice tensor or scalar value.
     */
    public Tsr&lt;V&gt; putAt( List&lt;?&gt; key, Tsr&lt;V&gt; value ) {
<span class="fc" id="L2369">        _putAtCheckFor( value );</span>
<span class="pc bpc" id="L2370" title="1 of 2 branches missed.">        Tsr&lt;V&gt; slice = ( key == null ) ? this : getAt( key );</span>
<span class="fc" id="L2371">        return _putAt( slice, value );</span>
    }

    /**
     *  This method enables injecting slices of tensor to be assigned into this tensor!
     *  It takes a key which is used to configure a slice
     *  tensor sharing the same underlying data as the original tensor.
     *  This slice is then used to assign the second argument to it, namely
     *  the &quot;value&quot; argument.
     *
     * @param key This object is a map defining a stride and a targeted index or range of indices...
     * @return A slice tensor or scalar value.
     */
    public Tsr&lt;V&gt; putAt(Map&lt;?,Integer&gt; key, Tsr&lt;V&gt; value ) {
<span class="fc" id="L2385">        _putAtCheckFor( value );</span>
<span class="pc bpc" id="L2386" title="1 of 2 branches missed.">        Tsr&lt;V&gt; slice = ( key == null ) ? this : getAt( key );</span>
<span class="fc" id="L2387">        return _putAt( slice, value );</span>
    }

    private void _putAtCheckFor( Tsr&lt;?&gt; value ) {
<span class="fc bfc" id="L2391" title="All 2 branches covered.">        if ( value.isEmpty() ) {</span>
<span class="fc" id="L2392">            String message = &quot;Provided tensor is empty! Empty tensors cannot be injected.&quot;;</span>
<span class="fc" id="L2393">            _LOG.error( message );</span>
<span class="fc" id="L2394">            throw new IllegalArgumentException( message );</span>
        }
<span class="fc" id="L2396">    }</span>

    private Tsr&lt;V&gt; _putAt(Tsr&lt;V&gt; slice, Tsr&lt;V&gt; value )
    {
<span class="fc" id="L2400">        boolean valueIsDeviceVisitor = false;</span>
<span class="fc bfc" id="L2401" title="All 4 branches covered.">        if ( slice.isOutsourced() &amp;&amp; !value.isOutsourced() ) {</span>
<span class="fc" id="L2402">            Device&lt;V&gt; device = slice.get( Device.class );</span>
            try {
<span class="fc" id="L2404">                device.store( value );</span>
<span class="nc" id="L2405">            } catch ( Exception e ) {</span>
<span class="nc" id="L2406">                _LOG.error( &quot;Trying to migrate target slice tensor to device failed.&quot;, e );</span>
<span class="nc" id="L2407">                throw e;</span>
<span class="fc" id="L2408">            }</span>
<span class="fc" id="L2409">            valueIsDeviceVisitor = true;</span>
        }
<span class="pc bpc" id="L2411" title="2 of 6 branches missed.">        if ( this.isEmpty() &amp;&amp; slice.isEmpty() || slice.size() != value.size() ) _become( value ); // TODO: Rethink this a little</span>
<span class="fc" id="L2412">        else return _constructFunctional( null, new Tsr[]{ slice, value }, &quot;I[ 0 ] &lt;- I[ 1 ]&quot;, false );</span>
        try {
<span class="pc bpc" id="L2414" title="1 of 2 branches missed.">            if ( valueIsDeviceVisitor ) value.get( Device.class ).restore( value );</span>
<span class="nc" id="L2415">        } catch ( Exception exception ) {</span>
<span class="nc" id="L2416">            _LOG.error( &quot;Trying to migrate source tensor back to original location failed.&quot;, exception );</span>
<span class="nc" id="L2417">            throw exception;</span>
<span class="fc" id="L2418">        }</span>
<span class="fc" id="L2419">        return this;</span>
    }

    /**
     *  A tensor ought to have some way to access its underlying data array.
     *  This method simple returns an element within this data array sitting at position &quot;i&quot;.
     * @param i The position of the targeted item within the raw data array of the tensor.
     * @return The found object sitting at the specified index position.
     */
    @Override
    public V getDataAt( int i )
    {
<span class="fc bfc" id="L2431" title="All 2 branches covered.">        if ( this.isOutsourced() ) {</span>
<span class="fc" id="L2432">            Device&lt;V&gt; device = this.get( Device.class );</span>
<span class="pc bpc" id="L2433" title="1 of 2 branches missed.">            if ( device instanceof OpenCLDevice ) {</span>
<span class="fc" id="L2434">                return (V)(Double)( (OpenCLDevice) device ).value64f( (Tsr&lt;Number&gt;) this, i );</span>
            }
<span class="nc" id="L2436">        }</span>
<span class="fc bfc" id="L2437" title="All 2 branches covered.">        else if ( getData() instanceof float[] )  return (V)(Float)  ( (float[]) getData())[ i ];</span>
<span class="fc bfc" id="L2438" title="All 2 branches covered.">        else if ( getData() instanceof double[] ) return (V)(Double) ( (double[]) getData())[ i ];</span>
<span class="fc bfc" id="L2439" title="All 2 branches covered.">        else if ( getData() instanceof short[] )  return (V)(Short)  ( (short[]) getData())[ i ];</span>
<span class="pc bpc" id="L2440" title="1 of 2 branches missed.">        else if ( getData() instanceof int[] )    return (V)(Integer)( (int[]) getData())[ i ];</span>
<span class="pc bpc" id="L2441" title="1 of 2 branches missed.">        else if ( getData() instanceof byte[] )   return (V)(Byte)   ( (byte[]) getData())[ i ];</span>
<span class="pc bpc" id="L2442" title="1 of 2 branches missed.">        else if ( getData() instanceof long[] )   return (V)(Long)   ( (long[]) getData())[ i ];</span>
<span class="fc" id="L2443">        else return ( (V[]) getData())[ i ];</span>
<span class="nc" id="L2444">        return null;</span>
    }

    /**
     *  A tensor ought to have some way to selectively modify its underlying data array.
     *  This method simply returns an element within this data array sitting at position &quot;i&quot;.
     * @param i The index of the data array entry which ought to be addressed.
     * @param o The object which ought to be placed at the requested position.
     * @return This very tensor in order to enable method chaining.
     */
    @Override
    public Tsr&lt;V&gt; setDataAt(int i, V o ) {
<span class="fc" id="L2456">        _guardMod(&quot;data object&quot;);</span>
<span class="pc bpc" id="L2457" title="1 of 2 branches missed.">        if ( getData() instanceof Object[] ) ( (Object[]) getData() )[ i ] = o;</span>
<span class="nc bnc" id="L2458" title="All 2 branches missed.">        else if ( getData() instanceof float[]  ) ( (float[])  getData() )[ i ] = (float)  o;</span>
<span class="nc bnc" id="L2459" title="All 2 branches missed.">        else if ( getData() instanceof double[] ) ( (double[]) getData() )[ i ] = (double) o;</span>
<span class="nc bnc" id="L2460" title="All 2 branches missed.">        else if ( getData() instanceof int[]    ) ( (int[])    getData() )[ i ] = (int)    o;</span>
<span class="nc bnc" id="L2461" title="All 2 branches missed.">        else if ( getData() instanceof long[]   ) ( (long[])   getData() )[ i ] = (long)   o;</span>
<span class="nc bnc" id="L2462" title="All 2 branches missed.">        else if ( getData() instanceof short[]  ) ( (short[])  getData() )[ i ] = (short)  o;</span>
<span class="nc bnc" id="L2463" title="All 2 branches missed.">        else if ( getData() instanceof byte[]   ) ( (byte[])   getData() )[ i ] = (byte)   o;</span>
<span class="fc" id="L2464">        return this;</span>
    }

    /**
     * @param value The primitive double array whose value ought to be used to populate this tensor.
     */
    private void _setValue64( double[] value ) {
<span class="fc bfc" id="L2471" title="All 2 branches covered.">        if ( this.isOutsourced() ) this.get( Device.class ).write( this, value );</span>
<span class="fc bfc" id="L2472" title="All 2 branches covered.">        else if ( getData() == null ) {</span>
<span class="fc" id="L2473">            setDataType( DataType.of( F64.class ) );</span>
<span class="fc" id="L2474">            _setData( value );</span>
        }
<span class="fc bfc" id="L2476" title="All 2 branches covered.">        else if ( getData() instanceof float[] )</span>
<span class="fc bfc" id="L2477" title="All 2 branches covered.">            for ( int i = 0; i &lt; value.length; i++ ) ( (float[]) getData())[ i ] = (float) value[ i ];</span>
<span class="pc bpc" id="L2478" title="1 of 2 branches missed.">        else if ( getData() instanceof double[] )</span>
<span class="fc" id="L2479">            System.arraycopy(value, 0, getData(), 0, value.length);</span>
<span class="fc" id="L2480">    }</span>

    /**
     * @param value The primitive float array whose value ought to be used to populate this tensor.
     */
    private void _setValue32( float[] value ) {
<span class="fc bfc" id="L2486" title="All 2 branches covered.">        if ( this.isOutsourced() ) this.get( Device.class ).write( this, value );</span>
<span class="pc bpc" id="L2487" title="1 of 2 branches missed.">        else if ( getData() == null ) {</span>
<span class="nc" id="L2488">            setDataType( DataType.of( F32.class ) );</span>
<span class="nc" id="L2489">            _setData( value );</span>
        }
<span class="pc bpc" id="L2491" title="1 of 2 branches missed.">        else if ( getData() instanceof float[] )</span>
<span class="nc" id="L2492">            System.arraycopy(value, 0, getData(), 0, value.length);</span>
<span class="pc bpc" id="L2493" title="1 of 2 branches missed.">        else if ( getData() instanceof double[] )</span>
<span class="fc bfc" id="L2494" title="All 2 branches covered.">            for ( int i = 0; i &lt; value.length; i++ ) ( (double[]) getData())[ i ] = value[ i ];</span>
<span class="fc" id="L2495">    }</span>

    /**
     *  This method will receive an object an try to interpret
     *  it or its contents to be set as value for this tensor.
     *  It will not necessarily replace the underlying data array object of this
     *  tensor itself, but also try to convert and copy the provided value
     *  into the data array of this tensor.
     *
     * @param value The value which may be a scalar or array and will be used to populate this tensor.
     * @return This very tensor to enable method chaining.
     */
    public Tsr&lt;V&gt; setValue( Object value )
    {
<span class="fc bfc" id="L2509" title="All 2 branches covered.">        if ( value instanceof float[] ) this._setValue32( (float[]) value );</span>
<span class="fc bfc" id="L2510" title="All 2 branches covered.">        else if ( value instanceof  double[] ) this._setValue64( (double[]) value );</span>
<span class="fc bfc" id="L2511" title="All 2 branches covered.">        else if ( value instanceof Float ) {</span>
<span class="fc" id="L2512">            this.setIsVirtual( true );</span>
<span class="fc bfc" id="L2513" title="All 2 branches covered.">            if ( getData() instanceof float[] ) ( (float[]) getData())[ 0 ] = (Float) value;</span>
<span class="fc" id="L2514">            else ( (double[]) getData())[ 0 ] = ( (Float) value ).doubleValue();</span>
<span class="fc bfc" id="L2515" title="All 2 branches covered.">        } else if ( value instanceof Double ) {</span>
<span class="fc" id="L2516">            this.setIsVirtual( true );</span>
<span class="pc bpc" id="L2517" title="1 of 2 branches missed.">            if ( getData() instanceof double[] ) ( (double[]) getData())[ 0 ] = (Double) value;</span>
<span class="nc" id="L2518">            else ( (float[]) getData())[ 0 ] = ( (Double) value ).floatValue();</span>
<span class="fc bfc" id="L2519" title="All 2 branches covered.">        } else if ( value instanceof Integer ) {</span>
<span class="fc" id="L2520">            this.setIsVirtual( true );</span>
<span class="fc" id="L2521">            ( (int[]) getData())[ 0 ] = (Integer) value;</span>
<span class="fc bfc" id="L2522" title="All 2 branches covered.">        } else if ( value instanceof Long ) {</span>
<span class="fc" id="L2523">            this.setIsVirtual( true );</span>
<span class="fc" id="L2524">            ( (long[]) getData())[ 0 ] = (Long) value;</span>
<span class="pc bpc" id="L2525" title="1 of 2 branches missed.">        } else if ( value instanceof int[] ) {</span>
<span class="nc" id="L2526">            setDataType( DataType.of(I32.class) );</span>
<span class="nc" id="L2527">            _setData( value );</span>
<span class="nc" id="L2528">            setIsVirtual( false );</span>
<span class="pc bpc" id="L2529" title="1 of 2 branches missed.">        } else if ( value instanceof short[] ) {</span>
<span class="fc" id="L2530">            setDataType( DataType.of(I16.class) );</span>
<span class="fc" id="L2531">            _setData( value );</span>
<span class="fc" id="L2532">            setIsVirtual( false );</span>
<span class="nc bnc" id="L2533" title="All 2 branches missed.">        } else if ( value instanceof long[] ) {</span>
<span class="nc" id="L2534">            setDataType( DataType.of(I64.class) );</span>
<span class="nc" id="L2535">            _setData( value );</span>
<span class="nc" id="L2536">            setIsVirtual( false );</span>
        }
<span class="fc" id="L2538">        return this;</span>
    }

    public Object getValue() { // TODO : Make this what it is supposed to be!!! (returning a copy of the targeted data)
<span class="fc bfc" id="L2542" title="All 2 branches covered.">        if ( this.isOutsourced() ) {</span>
<span class="fc" id="L2543">            Device&lt;V&gt; device = get( Device.class );</span>
<span class="fc bfc" id="L2544" title="All 2 branches covered.">            if ( device != null )</span>
<span class="fc" id="L2545">                return device.valueFor( this );</span>
            else
<span class="fc" id="L2547">                return getData();</span>
        }
<span class="fc bfc" id="L2549" title="All 2 branches covered.">        else if ( !this.isVirtual() ) return getData();</span>
<span class="fc" id="L2550">        else return getDataType().actualize( getData(), this.size() );</span>
    }

    /**
     *  This method takes the provided {@link Tsr} instance and adds it's
     *  contents to the contents of the {@link Tsr} which is set as gradient of this very {@link Tsr}.
     *
     * @param error The error gradient which ought to be added to the gradient of this tensor.
     * @return This very tensor instance to enable method chaining.
     */
    public Tsr&lt;V&gt; addToGradient(Tsr&lt;V&gt; error ) {
<span class="fc" id="L2561">        if (</span>
<span class="fc bfc" id="L2562" title="All 2 branches covered.">                !forComponent(</span>
                    Tsr.class,
                        gradient -&gt;
<span class="fc" id="L2565">                        this.set(</span>
<span class="fc" id="L2566">                                Neureka.get().context().getFunction().plusAssign().call(gradient, error)</span>
                        )
                )
<span class="fc" id="L2569">        ) set( error ).forComponent( Device.class, device -&gt; {</span>
            try {
<span class="fc" id="L2571">                device.store( error ) ;</span>
<span class="nc" id="L2572">            } catch ( Exception exception ) {</span>
<span class="nc" id="L2573">                _LOG.error( &quot;Failed trying to store a given error to a device for gradient accumulation.&quot;, exception );</span>
<span class="nc" id="L2574">                throw exception;</span>
<span class="fc" id="L2575">            }</span>
<span class="fc" id="L2576">        });</span>
<span class="fc" id="L2577">        return this;</span>
    }

    /**
     *  This method constitutes a pure operation producing a new tensor instance
     *  which is a deep copy of this original tensor and contains data whose
     *  elements have been converted to a new data type, namely :&lt;br&gt;
     *  The type specified by the argument &lt;br&gt;
     *  &lt;br&gt;
     *  The method does not change this tensor, which is why the operation is pure.
     *  Important to note is that the method will return instances of the specified
     *  type but merely another tensor containing elements of that type...
     *  The name of this method for example translates to the &quot;as&quot; operator
     *  found in Groovy, so the following code : &lt;i&gt;&quot; myTensor as Double &quot;&lt;/i&gt; &lt;br&gt;
     *  would not return a Double instance!&lt;br&gt;
     *  &lt;br&gt;
     *
     * @param typeClass The class which is the target of the underlying type conversion...
     * @param &lt;T&gt; The value type of the tensor that will be returned.
     * @return A new tensor which hosting the supplied type.
     */
    public &lt;T&gt; Tsr&lt;T&gt; asType( Class&lt;T&gt; typeClass )
    {
<span class="pc bpc" id="L2600" title="1 of 2 branches missed.">        if ( typeClass == Tsr.class ) return (Tsr&lt;T&gt;) this.slice().get();</span>
<span class="nc" id="L2601">        DataType&lt;?&gt; newDT = DataType.of( typeClass );</span>
        Object newData;
<span class="nc bnc" id="L2603" title="All 2 branches missed.">        if ( this.isOutsourced() ) {</span>
<span class="nc" id="L2604">            Device&lt;V&gt; device = get( Device.class );</span>
<span class="nc" id="L2605">            device.restore( this );</span>
<span class="nc" id="L2606">            newData = _convertedDataOfType( typeClass );</span>
<span class="nc" id="L2607">            device.store( this );</span>
<span class="nc" id="L2608">        }</span>
<span class="nc" id="L2609">        else newData = _convertedDataOfType( typeClass );</span>
<span class="nc" id="L2610">        return new Tsr&lt;&gt;( this.getNDConf().shape(), newDT, newData );</span>
    }

    /**
     *  This method is an inline operation which changes the underlying data of this tensor.
     *  It converts the data types of the elements of this tensor to the specified type!&lt;br&gt;
     *  &lt;br&gt;
     *  &lt;b&gt;WARNING : The use of this method is discouraged for the following reasons: &lt;/b&gt;&lt;br&gt;
     *  &lt;br&gt;
     *  1. Inline operations are inherently error prone for most use cases. &lt;br&gt;
     *  2. This inline operation in particular has no safety net,
     *     meaning that there is no implementation of version mismatch detection
     *     like there is for those operations present in the standard operation backend...
     *     No exceptions will be thrown during backpropagation! &lt;br&gt;
     *  3. This method has not yet been implemented to also handle instances which
     *     are slices of parent tensors!
     *     Therefore there might be unexpected performance penalties or side effects
     *     associated with this method.&lt;br&gt;
     *     &lt;br&gt;
     *
     * @param typeClass The target type class for elements of this tensor.
     * @param &lt;T&gt; The type parameter for the returned tensor.
     * @return The same tensor instance whose data has been converted to hold a different type.
     */
    public &lt;T&gt; Tsr&lt;T&gt; toType( Class&lt;T&gt; typeClass )
    {
<span class="fc bfc" id="L2636" title="All 2 branches covered.">        if ( this.isOutsourced() ) {</span>
<span class="fc" id="L2637">            setDataType( DataType.of( typeClass ) );</span>
<span class="fc" id="L2638">            return (Tsr&lt;T&gt;) this;</span>
        }
        else {
<span class="fc" id="L2641">            Object newData = _convertedDataOfType( typeClass );</span>
<span class="fc" id="L2642">            _setData( null );</span>
<span class="fc" id="L2643">            setDataType( DataType.of( typeClass ) );</span>
<span class="fc" id="L2644">            _setData( newData );</span>
        }
<span class="pc" id="L2646">        forComponent( Tsr.class, gradient -&gt; gradient.toType( typeClass ) );</span>
<span class="fc" id="L2647">        return (Tsr&lt;T&gt;) this;</span>
    }

    public double value64( int i ) {
<span class="fc bfc" id="L2651" title="All 2 branches covered.">        if ( this.isOutsourced() ) {</span>
<span class="pc bpc" id="L2652" title="1 of 2 branches missed.">            if ( get( Device.class ) instanceof OpenCLDevice )</span>
<span class="fc" id="L2653">                return get( OpenCLDevice.class ).value64f( (Tsr&lt;Number&gt;) this, i );</span>
<span class="nc" id="L2654">            else return 0.0;</span>
        }
<span class="fc bfc" id="L2656" title="All 2 branches covered.">        if ( this.isVirtual() ) {</span>
<span class="pc bpc" id="L2657" title="1 of 2 branches missed.">            if ( getData() instanceof double[] ) return ( (double[]) getData() )[ 0 ];</span>
<span class="nc" id="L2658">            else return ( (float[]) getData() )[ 0 ];</span>
        } else {
<span class="fc bfc" id="L2660" title="All 2 branches covered.">            if ( getData() instanceof double[] ) return ( (double[]) getData() )[ i ];</span>
<span class="fc" id="L2661">            else return ( (float[]) getData() )[ i ];</span>
        }
    }

    public double[] value64() {
<span class="fc" id="L2666">        Device&lt;V&gt; found = this.get( Device.class );</span>
<span class="pc bpc" id="L2667" title="1 of 6 branches missed.">        if ( getData() == null &amp;&amp; this.isOutsourced() &amp;&amp; found != null ) {</span>
<span class="pc bpc" id="L2668" title="1 of 2 branches missed.">            if ( found instanceof OpenCLDevice )</span>
<span class="fc" id="L2669">                return ( (OpenCLDevice) found).value64f( (Tsr&lt;Number&gt;) this );</span>
<span class="nc" id="L2670">            else return null;</span>
        }
<span class="fc" id="L2672">        double[] newValue = DataConverter.instance().convert( getData(), double[].class );</span>

<span class="fc bfc" id="L2674" title="All 6 branches covered.">        if ( this.isVirtual() &amp;&amp; newValue != null &amp;&amp; this.size() &gt; 1 ) {</span>

<span class="fc" id="L2676">           double[] value = new double[ this.size() ];</span>
<span class="fc" id="L2677">           Arrays.fill( value, newValue[ 0 ] );</span>
<span class="fc" id="L2678">           return value;</span>
        }
<span class="fc" id="L2680">        return newValue;</span>
    }

    public float value32( int i ) {
<span class="fc bfc" id="L2684" title="All 2 branches covered.">        if ( this.isOutsourced() ) {</span>
<span class="pc bpc" id="L2685" title="1 of 2 branches missed.">            if ( get( Device.class ) instanceof OpenCLDevice )</span>
<span class="fc" id="L2686">                return get( OpenCLDevice.class ).value32f( (Tsr&lt;Number&gt;) this, i );</span>
<span class="nc" id="L2687">            else return 0.0f;</span>
        }
<span class="pc bpc" id="L2689" title="1 of 2 branches missed.">        if ( this.isVirtual() ) {</span>
<span class="nc bnc" id="L2690" title="All 2 branches missed.">            if ( getData() instanceof double[] ) return (float) ( (double[]) getData() )[ 0 ];</span>
<span class="nc" id="L2691">            else return ( (float[]) getData())[ 0 ];</span>
        } else {
<span class="fc bfc" id="L2693" title="All 2 branches covered.">            if ( getData() instanceof double[] ) return (float) ( (double[]) getData() )[ i ];</span>
<span class="fc" id="L2694">            else return ( (float[]) getData() )[ i ];</span>
        }
    }

    public float[] value32() {
<span class="fc" id="L2699">        Device&lt;V&gt; found = this.get( Device.class );</span>
<span class="pc bpc" id="L2700" title="2 of 6 branches missed.">        if ( getData() == null &amp;&amp; this.isOutsourced() &amp;&amp; found != null ) {</span>
<span class="nc bnc" id="L2701" title="All 2 branches missed.">            if ( found instanceof OpenCLDevice )</span>
<span class="nc" id="L2702">                return ( (OpenCLDevice) found ).value32f( (Tsr&lt;Number&gt;) this);</span>
        }
<span class="fc" id="L2704">        float[] newValue = DataConverter.instance().convert( getData(), float[].class );</span>
<span class="fc bfc" id="L2705" title="All 4 branches covered.">        if ( this.isVirtual() &amp;&amp; newValue != null ) {</span>
<span class="fc" id="L2706">            newValue = new float[ this.size() ];</span>
<span class="fc" id="L2707">            Arrays.fill( newValue, newValue[ 0 ] );</span>
        }
<span class="fc" id="L2709">        return newValue;</span>
    }

    //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    public String toString( String mode ) {
<span class="fc bfc" id="L2715" title="All 2 branches covered.">        return _toString( mode, ( mode.contains( &quot;f&quot; ) ) ? &quot;    &quot; : null );</span>
    }

    public String toString( Map&lt;TsrAsString.Should, Object&gt; config, String indent ) {
<span class="fc" id="L2719">        return new TsrAsString( this, config ).toString( indent );</span>
    }

    public String toString( Map&lt;TsrAsString.Should, Object&gt; config ) {
<span class="fc" id="L2723">        return new TsrAsString( this, config ).toString();</span>
    }


    protected String _toString( String mode, String deep )
    {
<span class="fc" id="L2729">        return new TsrAsString( this, mode ).toString( deep );</span>
    }

    @Override
    public String toString()
    {
<span class="fc bfc" id="L2735" title="All 2 branches covered.">        if ( this.isDeleted() ) return &quot;deleted&quot;;</span>
<span class="fc" id="L2736">        return new TsrAsString( this ).toString();</span>
    }


    public static void makeFit( Tsr&lt;?&gt;[] tensors, boolean doesAD )
    {
<span class="fc" id="L2742">        int largest = -1;</span>
<span class="fc" id="L2743">        int[] shape = null;</span>
<span class="fc bfc" id="L2744" title="All 4 branches covered.">        for ( Tsr&lt;?&gt; t : tensors ) if ( t.rank() &gt; largest ) {</span>
<span class="fc" id="L2745">            largest = t.rank();</span>
<span class="fc" id="L2746">            shape = t.getNDConf().shape();</span>
        }
<span class="fc" id="L2748">        int prefix = 0;</span>
<span class="pc bpc" id="L2749" title="2 of 4 branches missed.">        assert shape != null;</span>
<span class="fc bfc" id="L2750" title="All 4 branches covered.">        for ( int s : shape ) if ( s == 1 ) prefix++; else break;</span>
<span class="fc" id="L2751">        int postfix = 0;</span>
<span class="fc bfc" id="L2752" title="All 4 branches covered.">        for ( int i = shape.length-1; i&gt;=0; i-- ) if ( shape[ i ] == 1 ) postfix++; else break;</span>
<span class="fc bfc" id="L2753" title="All 2 branches covered.">        for ( int i = 0; i &lt; tensors.length; i++ ) {</span>
<span class="fc bfc" id="L2754" title="All 2 branches covered.">            if ( tensors[ i ].rank() != largest ) {</span>
<span class="fc" id="L2755">                int[] oldShape = tensors[ i ].getNDConf().shape();</span>
<span class="fc" id="L2756">                int[] newReshape = new int[ largest ];</span>
<span class="fc" id="L2757">                int padding = largest - oldShape.length;</span>

<span class="fc bfc" id="L2759" title="All 2 branches covered.">                int handle = ( postfix &lt;= prefix ) ? padding : largest - padding;</span>
<span class="fc bfc" id="L2760" title="All 4 branches covered.">                for ( int ii = 0; ii &lt; handle; ii++ ) newReshape[ ii ] = ( postfix &lt;= prefix ) ? -1 : ii;</span>
<span class="fc bfc" id="L2761" title="All 4 branches covered.">                for ( int ii = handle; ii &lt; largest; ii++ ) newReshape[ ii ] = ( postfix &lt;= prefix ) ? ii - padding : -1;</span>

<span class="fc" id="L2763">                Function f = Function.of(</span>
<span class="fc" id="L2764">                    AbstractNDArray.Utility.Stringify.strConf( newReshape ) + &quot;:(I[ 0 ])&quot;,</span>
                        doesAD
                );
<span class="fc" id="L2767">                tensors[ i ] = f.call( tensors[ i ] );</span>
            }
        }

<span class="fc" id="L2771">    }</span>

    public int getVersion() {
<span class="fc" id="L2774">        return this._version;</span>
    }

    //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


    /**
     *  This is a static nested utility class
     *  which is used to allow for fast access to
     *  tensors storing doubles.
     */
    public static class IO
    {
<span class="nc" id="L2787">        public IO() {</span>
<span class="nc" id="L2788">        }</span>

        public static double getFrom(Tsr&lt;?&gt; t, int i ) {
<span class="pc bpc" id="L2791" title="2 of 4 branches missed.">            if ( t.isEmpty() || t.isUndefined() ) return 0;</span>
<span class="fc bfc" id="L2792" title="All 2 branches covered.">            else if ( t.isVirtual() ) return t.value64()[ 0 ];</span>
<span class="fc" id="L2793">            return t.value64()[ t.indexOfIndex( i ) ];</span>
        }

        public static double getFrom( Tsr&lt;?&gt; t, int[] idx ) {
<span class="fc" id="L2797">            t.setIsVirtual( false );</span>
<span class="fc" id="L2798">            return t.value64()[ t.indexOfIndices( idx ) ];</span>
        }

        public static void setInto( Tsr&lt;?&gt; t, int i, double value ) {
<span class="fc" id="L2802">            t.setIsVirtual( false );</span>
<span class="fc" id="L2803">            t.value64()[ t.indexOfIndex( i ) ] = value;</span>
<span class="fc" id="L2804">        }</span>

        public static void setInto( Tsr&lt;?&gt; t, int[] idx, double value ) {
<span class="fc" id="L2807">            t.setIsVirtual( false );</span>
<span class="fc" id="L2808">            t.value64()[ t.indexOfIndices( idx ) ] = value;</span>
<span class="fc" id="L2809">        }</span>

        public static void addInto( Tsr&lt;?&gt; t, int i, double value ) {
<span class="fc" id="L2812">            t.setIsVirtual( false );</span>
<span class="fc" id="L2813">            t.value64()[ t.indexOfIndex( i ) ] += value;</span>
<span class="fc" id="L2814">        }</span>

        public static void addInto( Tsr&lt;?&gt; t, int[] idx, double value ) {
<span class="fc" id="L2817">            t.setIsVirtual( false );</span>
<span class="fc" id="L2818">            t.value64()[ t.indexOfIndices( idx ) ] += value;</span>
<span class="fc" id="L2819">        }</span>

        public static Tsr&lt;?&gt; addInto( Tsr&lt;?&gt; t, Tsr&lt;?&gt; source ) {
<span class="pc bpc" id="L2822" title="3 of 4 branches missed.">            if ( t.isVirtual() &amp;&amp; source.isVirtual() ) t.value64()[ 0 ] += source.value64()[ 0 ];</span>
<span class="fc" id="L2823">            else new FunctionBuilder( Neureka.get().context() ).build( &quot;I[ 0 ]&lt;-(I[ 0 ]+I[ 1 ])&quot;, false ).call( new Tsr[]{ t, source } );</span>
<span class="fc" id="L2824">            return source;</span>
        }

        public static void subInto( Tsr&lt;?&gt; t, int i, double value ) {
<span class="fc" id="L2828">            t.setIsVirtual( false );</span>
<span class="fc" id="L2829">            t.value64()[ t.indexOfIndex( i ) ] -= value;</span>
<span class="fc" id="L2830">        }</span>

        public static void subInto( Tsr&lt;?&gt; t, int[] idx, double value ) {
<span class="fc" id="L2833">            t.setIsVirtual( false );</span>
<span class="fc" id="L2834">            t.value64()[ t.indexOfIndices( idx ) ] -= value;</span>
<span class="fc" id="L2835">        }</span>

        public static void subInto( Tsr&lt;?&gt; t, Tsr&lt;?&gt; source ) {
<span class="pc bpc" id="L2838" title="3 of 4 branches missed.">            if ( t.isVirtual() &amp;&amp; source.isVirtual() ) {</span>
<span class="nc" id="L2839">                t.value64()[ 0 ] -= source.value64()[ 0 ];</span>
            } else {
<span class="pc bpc" id="L2841" title="1 of 2 branches missed.">                if ( t.isVirtual() ) t.setIsVirtual( false );</span>
<span class="fc" id="L2842">                int[] index = new int[ t.getNDConf().shape().length ];</span>
<span class="fc" id="L2843">                int size = t.size();</span>
<span class="fc bfc" id="L2844" title="All 2 branches covered.">                for ( int i = 0; i &lt; size; i++ ) {</span>
<span class="fc" id="L2845">                    IO.subInto( t, index, IO.getFrom( source, index ) );</span>
<span class="fc" id="L2846">                    NDConfiguration.Utility.increment( index, t.getNDConf().shape() );</span>
                }
            }
<span class="fc" id="L2849">        }</span>

        public static void mulInto( Tsr&lt;?&gt; t, int i, double value ) {
<span class="fc" id="L2852">            t.setIsVirtual( false );</span>
<span class="fc" id="L2853">            t.value64()[ t.indexOfIndex( i ) ] *= value;</span>
<span class="fc" id="L2854">        }</span>

        public static void mulInto( Tsr&lt;?&gt; t, int[] idx, double value ) {
<span class="fc" id="L2857">            t.setIsVirtual( false );</span>
<span class="fc" id="L2858">            t.value64()[ t.indexOfIndices( idx ) ] *= value;</span>
<span class="fc" id="L2859">        }</span>

    }

    //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    /**
     *  This is a nested static utility class which is used
     *  to create tensor instances.
     */
    public static class Create
    {
<span class="nc" id="L2871">        public Create() { }</span>

        public  static Tsr&lt;?&gt; E( List&lt;Integer&gt; shape ) {
<span class="nc" id="L2874">            return E( shape.stream().mapToInt( e -&gt; e ).toArray() );</span>
        }

        public  static Tsr&lt;Double&gt; E( int... shape ) {
<span class="nc" id="L2878">            return new Tsr&lt;&gt;( shape, 2.7182818284590452353602874713527 );</span>
        }

        public static Tsr&lt;?&gt; newRandom( int... shape ) {
<span class="fc" id="L2882">            return newRandom( shape, 8701252152903546L );</span>
        }

        public static Tsr&lt;?&gt; newRandom( int[] shape, long seed ) {
<span class="fc" id="L2886">            int size = NDConfiguration.Utility.szeOfShp( shape );</span>
<span class="fc" id="L2887">            return Tsr.of( shape, DataConverter.Utility.newSeededDoubleArray( seed, size ) );</span>
        }

        public static Tsr&lt;?&gt; newTsrLike( Tsr&lt;?&gt; template, double value ) {
<span class="fc" id="L2891">            Tsr&lt;Object&gt; t = (Tsr&lt;Object&gt;) _newEmptyLike( template );</span>
<span class="pc bpc" id="L2892" title="1 of 2 branches missed.">            if ( template.getData() instanceof float[] ) t.setValue( (float) value ); //TODO remove instanceof</span>
<span class="fc" id="L2893">            else t.setValue( value );</span>
            try {
<span class="fc bfc" id="L2895" title="All 2 branches covered.">                if ( template.isOutsourced() ) ( (Device&lt;Object&gt;) template.get( Device.class ) ).store( t );</span>
<span class="nc" id="L2896">            } catch ( Exception exception ) {</span>
<span class="nc" id="L2897">                _LOG.error( &quot;Failed storing a newly created tensor from a template tensor to its host device.&quot;, exception );</span>
<span class="nc" id="L2898">                throw exception;</span>
<span class="fc" id="L2899">            }</span>
<span class="fc" id="L2900">            return t;</span>
        }

        public static Tsr&lt;?&gt; newTsrLike( Tsr&lt;?&gt; template ) { // The output tensor will not have gradients!
<span class="fc" id="L2904">            Tsr&lt;Object&gt; t = (Tsr&lt;Object&gt;) _newEmptyLike( template );</span>
<span class="pc bpc" id="L2905" title="1 of 2 branches missed.">            if ( template.getData() instanceof float[] ) t._setValue32( new float[ template.size() ] );</span>
<span class="fc" id="L2906">            else t._setValue64( new double[ template.size() ] );</span>
            try {
<span class="fc bfc" id="L2908" title="All 2 branches covered.">                if ( template.isOutsourced() ) ( (Device&lt;Object&gt;) template.get( Device.class ) ).store( t );</span>
<span class="nc" id="L2909">            } catch ( Exception exception ) {</span>
<span class="nc" id="L2910">                _LOG.error( &quot;Failed storing a newly created tensor from a template tensor to its host device.&quot;, exception );</span>
<span class="nc" id="L2911">                throw exception;</span>
<span class="fc" id="L2912">            }</span>
<span class="fc" id="L2913">            return t;</span>
        }

        private static Tsr&lt;?&gt; _newEmptyLike( Tsr&lt;?&gt; template ) {
<span class="fc" id="L2917">            Tsr&lt;?&gt; t = Tsr.newInstance();</span>
<span class="fc" id="L2918">            t.createConstructionAPI().configureFromNewShape( template.getNDConf().shape(), false, true );</span>
<span class="fc" id="L2919">            return t;</span>
        }

    }

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>