<div class="col-sm 12 col-md-12">
    <div class="ContentBox">
        <h3>
            Optimization:
        </h3>
        <p>
            The optimization package provides a generic interface for implementing
            optimization components called by tensors when gradients will be applied.
            One of the most famous optimization algorithms termed 'ADAM' is
            already implemented and ready for use.
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-6">
                                               <pre><code class="hljs java">

        Tsr w = new Tsr(0)
        Optimizer o = new ADAM(w)
        w.add(o)

                                                </code></pre>
</div>
<div class="col-sm-12 col-md-6">
    <div class="ContentBox">
        <p>
            The optimizer is simply instantiated by passing the targeted tensor instance
            and then added to the tensor as component.
            The optimizer can easily be accessed by calling 'find(Optimizer.class)'
            on the tensor of concern.
        </p>
        <p>
            It is important to note: Every tensor has their own Optimizer instance.
            It is therefore not allowed to be shared due to the fact that the Optimizer
            stores a context specialized to the one given tensor.
        </p>
    </div>
</div>