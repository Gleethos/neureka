<div class="col-sm 12 col-md-12">
    <div class="ContentBox">
        <h3>
            Optimization
        </h3>
        <p class="MarkdownMe">
The optimization package provides the `Optimization` interface for implementing
optimizations as `Tsr` components called by them when their gradients will be applied.
One of the most basic optimization algorithms, *stochastic gradient descent* is
already implemented and ready for use by instantiating the `SGD` class.<br>
Otherwise there is only a single other optimizer implementing this interface,
namely the `ADAM` class which is still work in progress.
If you want more options, consider implementing the interface for custom optimizers.
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-6 col-lg-6">
    <div class="TabWrapper">
        <div class="TabHead">
            <button onclick="switchTab(event, '.groovyTab')" class="selected">Groovy</button>
            <button onclick="switchTab(event, '.javaTab')">Java</button>
            <button onclick="switchTab(event, '.kotlinTab')">Kotlin</button>
        </div>
        <div class="TabBody">
            <div class="groovyTab">
                <pre><code class="hljs java">

        Tsr w = Tsr.of(0)
        def o = new SGD(w)
        w.set(o)

                </code></pre>
            </div>
            <div class="javaTab" style="display:none">
                <pre><code class="hljs java">

        Tsr w = Tsr.of(0);
        Optimizer o = new SGD(w);
        w.set(o);

                </code></pre>
            </div>
            <div class="kotlinTab" style="display:none">
                <pre><code class="hljs kotlin">

        val w = Tsr.of(0)
        val o = new SGD(w)
        w.set(o)

                </code></pre>
            </div>
        </div>
    </div>
</div>

<div class="col-sm-12 col-md-6 col-lg-6">
    <div class="ContentBox">
        <p>
            The optimizer is simply instantiated by passing the targeted tensor instance
            and then added to the tensor as component.
            The optimizer can easily be accessed by calling 'find(Optimizer.class)'
            on the tensor of concern.
        </p>
        <p>
            It is important to note: Every tensor has their own Optimizer instance.
            It is therefore not allowed to be shared due to the fact that the Optimizer
            stores a context specialized to the one given tensor.
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-12 col-lg-12">
    <div class="TabWrapper">
        <div class="TabHead">
            <button onclick="switchTab(event, '.groovyTab')" class="selected">Groovy</button>
            <button onclick="switchTab(event, '.javaTab')">Java</button>
            <button onclick="switchTab(event, '.kotlinTab')">Kotlin</button>
        </div>
        <div class="TabBody">
            <div class="groovyTab">
                <pre><code class="hljs java">

        Tsr w = Tsr.of(0)
        def o = new ADAM(w)
        w.set(o)

                </code></pre>
            </div>
            <div class="javaTab" style="display:none">
                <pre><code class="hljs java">

        Tsr w = Tsr.of(0);
        Optimizer o = new ADAM(w);
        w.set(o);

                </code></pre>
            </div>
            <div class="kotlinTab" style="display:none">
                <pre><code class="hljs kotlin">

        val w = Tsr.of(0)
        val o = new ADAM(w)
        w.set(o)

                </code></pre>
            </div>
        </div>
    </div>
</div>