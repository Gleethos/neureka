<div class="col-sm-12 col-md-12">
    <div class="ContentBox">
        <h3>Autograd</h3>
        <p>
            Neureka implements so called Auto-Differentiation.
            AD-algorithms are the backbone of today's most advanced
            artificial neural networks.
            Such algorithms calculate derivatives with respect to
            weight variables used to correct errors incrementally
            throughout many training iterations,
            whereby neural networks then gain the ability to
            approximate, generalize and learn.
            Differentiation goes both ways: Forward and Backward.
            Namely:</br>
            Forward-mode-AD and backward-mode-AD.</br>
            Neureka utilizes both techniques to get the best performance.
            Especially the use of backward-mode-AD revolutionized
            machine learning in recent decades.
            So called stochastic gradient decent is the most well known
            algorithm which uses backward-mode-AD.
            This technique is widely known as Backpropagation.
            and for most cases this approach increases performance by
            orders of magnitudes compared to other techniques,
            making the difference between a model
            taking weeks instead of years to train.
            Beyond its use in machine learning, Auto-differentiation is a powerful
            computational tool in a multitude of other areas, ranging from weather
            forecasting to analyzing numerical stability.
            AD is a common tool for many other fields dependent on algebraic numerical computation.
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-8 col-lg-8">
    <div class="TabWrapper">
        <div class="TabHead">
            <button onclick="switchTab(event, '.groovyTab')" class="selected">Groovy</button>
            <button onclick="switchTab(event, '.javaTab')">Java</button>
            <button onclick="switchTab(event, '.kotlinTab')">Kotlin</button>
        </div>
        <div class="TabBody">
            <div class="groovyTab">
                <pre><code class="hljs java">

    def x = Tsr.of(3.0).setRqsGradient(true)
    def b = Tsr.of(-4.0)
    def w = Tsr.of(2.0)
    def y = Tsr.of('( ( i0 + i1 ) * i2 ) ^ 2', x, b, w)

    //   f(x) = ((x-4)*2)^2; :=>  f(3) = 4
    //   f(x)' = 8*x - 32 ;  :=>  f(3)' = -8
    //
    //   y.toString(): "(1):[4.0]; ->d(1):[-8.0], "

    y.backward(2)

    //   x.toString(): "(1):[3.0]:g:[-16.0]"

    x.applyGradient()

    //   x.toString(): "(1):[-13.0]:g:[null]"

                </code></pre>
            </div>
            <div class="javaTab" style="display:none">
                <pre><code class="hljs java">

    Tsr&#60;Double&#62; x = Tsr.of(3.0).setRqsGradient(true);
    Tsr&#60;Double&#62; b = Tsr.of(-4.0);
    Tsr&#60;Double&#62; w = Tsr.of(2.0);
    Tsr&#60;Double&#62; y = Tsr.of("( ( i0 + i1 ) * i2 ) ^ 2", x, b, w);

    //   f(x) = ((x-4)*2)^2; :=>  f(3) = 4
    //   f(x)' = 8*x - 32 ;  :=>  f(3)' = -8
    //
    //   y.toString(): "(1):[4.0]; ->d(1):[-8.0], "

    y.backward(2);

    //   x.toString(): "(1):[3.0]:g:[-16.0]"

    x.applyGradient();

    //   x.toString(): "(1):[-13.0]:g:[null]"

                </code></pre>
            </div>
            <div class="kotlinTab" style="display:none">
                <pre><code class="hljs kotlin">

    val x : Tsr&#60;Double&#62; = Tsr.of(3.0).setRqsGradient(true)
    val b : Tsr&#60;Double&#62; = Tsr.of(-4.0)
    val w : Tsr&#60;Double&#62; = Tsr.of(2.0)
    val y : Tsr&#60;Double&#62; = Tsr.of("( ( i0 + i1 ) * i2 ) ^ 2", x, b, w)

    //   f(x) = ((x-4)*2)^2; :=>  f(3) = 4
    //   f(x)' = 8*x - 32 ;  :=>  f(3)' = -8
    //
    //   y.toString(): "(1):[4.0]; ->d(1):[-8.0], "

    y.backward(2)

    //   x.toString(): "(1):[3.0]:g:[-16.0]"

    x.applyGradient()

    //   x.toString(): "(1):[-13.0]:g:[null]"

                </code></pre>
            </div>
        </div>
    </div>
</div>
<div class="col-sm 12 col-md-4">
    <div class="ContentBox">
        <p>
            In this example Neureka calculates the derivative of y with respect to x.
            </br>
            Forward-AD is used due to the fact that no other dependencies
            need to be considered.
            When 'backward' is called, the requested error of '2'
            is being multiplied by the calculated derivative and then
            attached to x.
            </br>
            This newly created gradient can then be applied.
            By also utilizing forward-mode-AD Neureka greatly decreases Memory consumption
            as well as increase performance.
        </p>
    </div>
</div>
<div class="col-sm 12 col-md-12">
    <div class="ContentBox">
        <p>
            Keep in mind however that this optimization is only
            possible when your calculation is being defined
            by an equation embedded into a string expression and
            also without too many inputs requiring gradients.
            </br>
            Besides forward-mode-AD optimizations Neureka
            also aggressively frees the memory of intermediate
            values used to calculate results.
            </br></br>
            Try formalizing your computations via String expressions
            whenever your can in order to get the best performance.
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-8 col-lg-8">
    <div class="TabWrapper">
        <div class="TabHead">
            <button onclick="switchTab(event, '.groovyTab')" class="selected">Groovy</button>
            <button onclick="switchTab(event, '.javaTab')">Java</button>
            <button onclick="switchTab(event, '.kotlinTab')">Kotlin</button>
        </div>
        <div class="TabBody">
            <div class="groovyTab">
                <pre><code class="hljs java">

     Tsr a = Tsr.of(-3).setRqsGradient(true)
     Tsr b = Tsr.of(4).setRqsGradient(true)
     Tsr c = Tsr.of(2)
     Tsr d = Tsr.of('( relu( i0 * i1 ) + i1 ) / i2', a, b, c);

     //  s:>>
     //  (1):[1.94];
     //       =>d|[ (1):[0.5] ]|:
     //           t{
     //               (1):[-0.12];
     //                   =>d|[ (1):[-0.03] ]|:
     //                       t{ (1):[4.0] },
     //                   =>d|[ (1):[0.04] ]|:
     //                       t{ (1):[-3.0] },
     //           },
     //       =>d|[ (1):[0.5] ]|:
     //           t{
     //               (1):[4.0]
     //           },

     // d|[ ... ]|  ... derivative
     // t{ ... }    ... targeted tensor (of a derivative)

                </code></pre>
            </div>
            <div class="javaTab" style="display:none">
                <pre><code class="hljs java">

     Tsr&#60;Double&#62; a = Tsr.of(-3).setRqsGradient(true);
     Tsr&#60;Double&#62; b = Tsr.of(4).setRqsGradient(true);
     Tsr&#60;Double&#62; c = Tsr.of(2);
     Tsr&#60;Double&#62; d = Tsr.of("( relu( i0 * i1 ) + i1 ) / i2", a, b, c);

     //  s:>>
     //  (1):[1.94];
     //       =>d|[ (1):[0.5] ]|:
     //           t{
     //               (1):[-0.12];
     //                   =>d|[ (1):[-0.03] ]|:
     //                       t{ (1):[4.0] },
     //                   =>d|[ (1):[0.04] ]|:
     //                       t{ (1):[-3.0] },
     //           },
     //       =>d|[ (1):[0.5] ]|:
     //           t{
     //               (1):[4.0]
     //           },

     // d|[ ... ]|  ... derivative
     // t{ ... }    ... targeted tensor (of a derivative)

                </code></pre>
            </div>
            <div class="kotlinTab" style="display:none">
                <pre><code class="hljs kotlin">

     val a : Tsr&#60;Double&#62; = Tsr.of(-3).setRqsGradient(true);
     val b : Tsr&#60;Double&#62; = Tsr.of(4).setRqsGradient(true);
     val c : Tsr&#60;Double&#62; = Tsr.of(2);
     val d : Tsr&#60;Double&#62; = Tsr.of("( relu( i0 * i1 ) + i1 ) / i2", a, b, c);

     //  s:>>
     //  (1):[1.94];
     //       =>d|[ (1):[0.5] ]|:
     //           t{
     //               (1):[-0.12];
     //                   =>d|[ (1):[-0.03] ]|:
     //                       t{ (1):[4.0] },
     //                   =>d|[ (1):[0.04] ]|:
     //                       t{ (1):[-3.0] },
     //           },
     //       =>d|[ (1):[0.5] ]|:
     //           t{
     //               (1):[4.0]
     //           },

     // d|[ ... ]|  ... derivative
     // t{ ... }    ... targeted tensor (of a derivative)

                </code></pre>
            </div>
        </div>
    </div>
</div>
<div class="col-sm-12 col-md-4">
    <div class="ContentBox">
        <p>
            As you can see in this example,
            having multiple variables requiring gradients
            as inputs to the constructed function
            will produce many intermediate derivatives which
            are needed for backward AD.
            Multiple AD dependencies increases memory consumption.
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-12 col-lg-12">
    <div class="ContentBox" id="AUTOGRAD_EXPLAINED" style="overflow-x: scroll;">
    </div>
    <script>
        $( document ).ready(function() {
            console.log('Loading report...');
            $('#AUTOGRAD_EXPLAINED').html("").load(
                'spock/reports/ut.autograd.Autograd_Explained.html',
                () => {
                    console.log('Executing format procedure for autograd...');
                    setTimeout(() => {
                            for (let item of document.getElementsByClassName("MarkdownMe")) {
                                item.innerHTML = marked(item.innerHTML);
                                item.classList.remove("MarkdownMe");
                            }
                        },
                        375
                    );
                }
            );
        });
    </script>
</div>