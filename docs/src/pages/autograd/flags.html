<div class="col-sm-12 col-md-12">
    <div class="ContentBox">
        <h3>What?</h3>
        <p>
            Neureka's auto-differentiation and gradient accumulation algorithm is
            highly configurable. <br>
            Different architectures have different requirements for training and optimal performance. <br>
            Simpler feed-forward networks might be covered by a conventional implementation, however in
            networks relying on more complex and dynamic structures, there are many
            techniques available to significantly improve training! <br>
            <br>
            The Neureka class offers many configurable nested classes used to tune the library to your needs.<br>
            Among those classes is of course one named according to the package it governs: AutoDiff
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-6">
    <div class="TabWrapper">
        <div class="TabHead BasicTabHeader">
            <button onclick="switchTab(event, '.groovyTab')" class="selected">Groovy</button>
            <button onclick="switchTab(event, '.javaTab')">Java</button>
            <button onclick="switchTab(event, '.kotlinTab')">Kotlin</button>
        </div>
        <div class="TabBody">
            <div class="groovyTab">
                <pre><code class="hljs language-java">
def ag = Neureka.get().settings().autograd()
                </code></pre>
            </div>
            <div class="javaTab" style="display:none">
                <pre><code class="hljs language-java">
AutoGrad ag = Neureka.get().settings().autograd();
                </code></pre>
            </div>
            <div class="kotlinTab" style="display:none">
                <pre><code class="hljs kotlin">
val ag : AutoGrad = Neureka.get().settings().autograd()
                </code></pre>
            </div>
        </div>
    </div>
</div>
<div class="col-sm-12 col-md-6">
    <div class="ContentBox">
        <h3>Overview</h3>
        <p>
            "is retaining pending error for JIT-Prop"<br>
            "is applying gradient when tensor is used"<br>
            "is applying gradient when requested"<br>
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-6 col-lg-6">
    <div class="TabWrapper">
        <div class="TabHead BasicTabHeader">
            <button onclick="switchTab(event, '.groovyTab')" class="selected">Groovy</button>
            <button onclick="switchTab(event, '.javaTab')">Java</button>
            <button onclick="switchTab(event, '.kotlinTab')">Kotlin</button>
        </div>
        <div class="TabBody">
            <div class="groovyTab">
                <pre><code class="hljs language-java">
// getter :
ag.isRetainingPendingErrorForJITProp()
// setter :
ag.setIsRetainingPendingErrorForJITProp(true)
                </code></pre>
            </div>
            <div class="javaTab" style="display:none">
                <pre><code class="hljs language-java">
// getter :
ag.isRetainingPendingErrorForJITProp();
// setter :
ag.setIsRetainingPendingErrorForJITProp(true);
                </code></pre>
            </div>
            <div class="kotlinTab" style="display:none">
                <pre><code class="hljs kotlin">
// getter :
ag.isRetainingPendingErrorForJITProp()
// setter :
ag.setIsRetainingPendingErrorForJITProp(true)
                </code></pre>
            </div>
        </div>
    </div>
</div>
<div class="col-sm 12 col-md-6 col-lg-6">
    <div class="ContentBox">
        <p>
            This flag enables an optimization technique which only propagates error values to
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them
            at divergent differentiation paths within the computation graph.<br>
            If the flag is set to true <br>
            then error values will accumulate at such junction nodes.
            This technique however uses more memory but will
            improve performance for some networks substantially.
            The technique is termed JIT-Propagation.
        </p>
    </div>
</div>

<div class="col-sm-12 col-md-6 col-lg-6">
    <div class="TabWrapper">
        <div class="TabHead BasicTabHeader">
            <button onclick="switchTab(event, '.groovyTab')" class="selected">Groovy</button>
            <button onclick="switchTab(event, '.javaTab')">Java</button>
            <button onclick="switchTab(event, '.kotlinTab')">Kotlin</button>
        </div>
        <div class="TabBody">
            <div class="groovyTab">
                <pre><code class="hljs language-java">
// getter :
ag.isApplyingGradientWhenTensorIsUsed()
// setter :
ag.setIsApplyingGradientWhenTensorIsUsed(true)
                </code></pre>
            </div>
            <div class="javaTab" style="display:none">
                <pre><code class="hljs language-java">
// getter :
ag.isApplyingGradientWhenTensorIsUsed();
// setter :
ag.setIsApplyingGradientWhenTensorIsUsed(true);
                </code></pre>
            </div>
            <div class="kotlinTab" style="display:none">
                <pre><code class="hljs kotlin">
// getter :
ag.isApplyingGradientWhenTensorIsUsed()
// setter :
ag.setIsApplyingGradientWhenTensorIsUsed(true)
                </code></pre>
            </div>
        </div>
    </div>
</div>
<div class="col-sm 12 col-md-6">
    <div class="ContentBox">
        <p>
            Gradients will automatically be applied to tensors as soon as
            they are being used for calculation (GraphNode instantiation).
            This feature works well with JIT-Propagation.
        </p>
    </div>
</div>


<div class="col-sm-12 col-md-6 col-lg-6">
    <div class="TabWrapper">
        <div class="TabHead BasicTabHeader">
            <button onclick="switchTab(event, '.groovyTab')" class="selected">Groovy</button>
            <button onclick="switchTab(event, '.javaTab')">Java</button>
            <button onclick="switchTab(event, '.kotlinTab')">Kotlin</button>
        </div>
        <div class="TabBody">
            <div class="groovyTab">
                <pre><code class="hljs language-java">
// getter :
ag.isApplyingGradientWhenRequested()
// setter :
ag.setIsApplyingGradientWhenRequested(true)
                </code></pre>
            </div>
            <div class="javaTab" style="display:none">
                <pre><code class="hljs language-java">
// getter :
ag.isApplyingGradientWhenRequested();
// setter :
ag.setIsApplyingGradientWhenRequested(true);
                </code></pre>
            </div>
            <div class="kotlinTab" style="display:none">
                <pre><code class="hljs kotlin">
// getter :
ag.isApplyingGradientWhenRequested()
// setter :
ag.setIsApplyingGradientWhenRequested(true)
                </code></pre>
            </div>
        </div>
    </div>
</div>
<div class="col-sm 12 col-md-6">
    <div class="ContentBox">
        <p>
            Gradients will only be applied if requested.
            Usually this happens immediately, however
            if the flag 'applyGradientWhenTensorIsUsed' is set
            to true, then the tensor will only be updated by its
            gradient if requested AND the tensor is used for calculation! (GraphNode instantiation).
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-12 col-lg-12">
    <div class="ContentBox" id="AUTOGRAD_FLAGS" style="overflow-x: scroll;">
        <script>
            $('#AUTOGRAD_FLAGS').html("").load('spock/reports/ut.autograd.Autograd_Flags_Explained.html');
        </script>
    </div>
</div>