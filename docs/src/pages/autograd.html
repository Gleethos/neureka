<div class="col-sm-12 col-md-12">
    <div class="contentBox">
        <h3>Autograd</h3>
        <p>
            Neureka implements so called Auto-Differentiation.
            AD-algorithms are the backbone of today's most advanced
            artificial neural networks.
            Such algorithms calculate derivatives with respect to
            weight variables used to correct errors incrementally
            throughout many training iterations,
            whereby neural networks then gain the ability to
            approximate, generalize and learn.
            Differentiation goes both ways: Forward and Backward.
            Namely:</br>
            Forward-mode-AD and backward-mode-AD.</br>
            Neureka utilizes both techniques to get the best performance.
            Especially the use of backward-mode-AD revolutionized
            machine learning in recent decades.
            So called stochastic gradient decent is the most well known
            algorithm which uses uses backward-mode-ad.
            This technique is known as Backpropagation.
            and for most cases this approach increases performance by
            orders of magnitudes compared to other techniques,
            making the difference between a model
            taking weeks instead of years to train.
            Beyond its use in machine learning, Auto-differentiation is a powerful
            computational tool in a multitude of other areas, ranging from weather
            forecasting to analyzing numerical stability.
            AD is a common tool
            for many other fields dependent on algebraic numerical computation.
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-8">
                                                <pre><code class="hljs java">
    Tsr x = new Tsr(3).setRqsGradient(true);
    Tsr b = new Tsr(-4);
    Tsr w = new Tsr(2);
    Tsr y = new Tsr(new Tsr[]{x, b, w}, "((i0+i1)*i2)^2");

    //   f(x) = ((x-4)*2)^2; :=>  f(3) = 4
    //   f(x)' = 8*x - 32 ;  :=>  f(3)' = -8
    //
    //   y.toString(): "[1]:(4.0); ->d[1]:(-8.0), "

    y.backward(2)

    //   x.toString(): "[1]:(3.0):g:(-16.0)"

    x.applyGradient()

    //   x.toString(): "[1]:(-13.0):g:(null)"

                                                </code></pre>
</div>
<div class="col-sm 12 col-md-4">
    <div class="contentBox">
        <p>
            In this example Neureka calculates the derivative of y w.r.t x.
            </br>
            Forward-AD is used due to the fact that no other dependencies
            need to be considered.
            When backward is called, the requested error of '2'
            is being multiplied by the calculated error and then
            attached to x.
            </br>
            This newly created gradient can then be applied.
            By also utilizing forward-mode-AD Neureka greatly decreases Memory consumption
            as well as increase performance.
        </p>
    </div>
</div>
<div class="col-sm 12 col-md-12">
    <div class="contentBox">
        <p>
            Keep in mind however that this optimization is only
            possible when your calculation is being defined
            by an equation embedded into a string expression and
            also without too many inputs requiring gradients.
            </br>
            Besides forward-mode-AD optimizations Neureka
            also aggressively frees the memory of intermediate
            values used to calculate results.
            </br></br>
            Try formalizing your computations via String expressions
            whenever your can in order to get the best performance.
        </p>
    </div>
</div>
<div class="col-sm-12 col-md-8">
                                                <pre><code class="hljs java">
     Tsr a = new Tsr(-3).setRqsGradient(true);
     Tsr b = new Tsr(4).setRqsGradient(true);
     Tsr c = new Tsr(2);
     Tsr d = new Tsr(Tsr[]{a, b, c},"(relu(i0*i1)+i1)/i2");

     //  [1]:(1.94);
     //       =>d|[ [1]:(0.5) ]|:
     //           t{
     //               [1]:(-0.12);
     //                   =>d|[ [1]:(-0.03) ]|:
     //                       t{ [1]:(4.0) },
     //                   =>d|[ [1]:(0.04) ]|:
     //                       t{ [1]:(-3.0) },
     //           },
     //       =>d|[ [1]:(0.5) ]|:
     //           t{
     //               [1]:(4.0)
     //           },

                                                </code></pre>
</div>
<div class="col-sm 12 col-md-4">
    <div class="contentBox">
        <p>
            As you can see in this example,
            having multiple variables requiring gradients
            as inputs to constructed functions
            will produce many intermediate derivatives wich
            are needed for backward AD.
            This however increases memory consumption.
        </p>
    </div>
</div>