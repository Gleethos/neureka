{"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":80},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1534},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":173},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Cross device system test runs successfully.","Mapping tensors works for every device (even if they are not used).","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":14077},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":562},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3457},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["The long broad integration test runs successfully."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":63},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":244},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":133},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":221},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":311},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":215},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":167},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":129},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations have expected properties.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":121},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":157},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":45},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The CPU matrix multiplication implementation works as expected.","The internal matrix multiplication test script runs!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3206},"title":"Internal CPU based Matrix Multiplication","narrative":"This specification covers library internal matrix multiplication logic,\n    specifically the CPU implementation.\n    Do not depend on the API used in this specification as it is subject to change!"},"ut.backend.core.OpenCL_Backend_Spec":{"executedFeatures":["The OpenCL backend context can load implementations."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":59},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The \"matMul\" method allows us to perform matrix multiplication.","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":49},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":36},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":92},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":50},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":110},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":61},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result","The softmax function can be applied to tensors with more than one dimension.","The tensor API has built-in methods for applying functions.","We can collect a stream into a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":253},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":10},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":42},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Devices also store slices which can also be restored just like normal tensors.","Devices expose an API for accessing (reading and writing) the data of a tensor.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","In total there are 3 different types of methods for finding device instances.","Passing a numeric array to a tensor should modify its contents!","Virtual tensors stay virtual when outsourced.","We can find Device implementations or null by passing search keys to the \"get\" method.","We can query the backend for devices by specifying both the requested type and a key word."],"ignoredFeatures":["Devices cannot store slices whose parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":170},"title":"Finding Device Types","narrative":"Neureka introduces a the concept of a `Device` which is an interface\n    that represents a computational device used for executing tensor / nd-array operations on them.\n    The `Device` interface is implemented by various classes which represent\n    different types of accelerator hardware such as `CPUs`, `GPUs`, `TPUs`, `FPGAs`, etc.\n    These various `Device` types can not be instantiated directly because they model \n    the concrete and finite hardware that is available on any given system Neureka is running on.\n    This means that they are usually instantiated lazily upon access request or \n    upfront by the library backend (usually a backend extension built fo a specific device).\n    In order to find these instances embedded in the library backend the `Device` interface\n    exposes various static methods which can be used to find a device instance by name or type."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":143},"title":"FileDevice, Storing Tensors in Files","narrative":"The `FileDevice` class, one of many implementations of the `Device` interface, \n    represents a file directory which can store and load tensors as files (`idx`, `jpg`, `png`...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel.","The OpenCLDevice produces a working optimized Function for doubles.","The OpenCLDevice produces a working optimized Function for floats."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":249},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.CPU_Kernel_Spec":{"executedFeatures":["The Reduce implementation for the CPU has realistic behaviour","The Sum implementation for the CPU has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":115},"title":"","narrative":""},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.device.internal.OpenCL_Kernel_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour","The Reduce implementation for the OpenCLDevice has realistic behaviour","The Sum implementation for the OpenCLDevice has realistic behaviour","The Sum implementation for the OpenCLDevice has realistic behaviour for when the number of elements is a prime."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":31},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","We can get the items of an outsourced tensor as a primitive array.","We can take a look at the underlying data array of an outsourced tensor through the unsafe API."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":338},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default tensors use the `CPU` device, but sometimes we want to\n    use something more suitable for large amounts of data and a high degree of parallelization.\n    This is were the `OpenCLDevice` comes into play!\n    It is a `Device` implementation built on top of the JOCL library, a thin OpenCL API.\n    We expect the `OpenCLDevice` to store tensors as well as being able to read and write\n    data from and to stored tensors.\n    Also, an `OpenCLDevice` should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4846},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the `OpenCLDevice` class which\n    represents a single device with OpenCL support.\n    Besides that, there is also the `OpenCLContext` class which\n    represents a OpenCL contexts, platforms and multiple devices on said platforms..."},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":58},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["A matrix (rank 2 tensor) can be labeled and their labels can be used to extract slices / subsets.","A tensor can be labeled partially.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","We can add labels to tensors through lists or maps passed to the \"label()\" method."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":31},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.introductions.Tensor_NDArray_Spec":{"executedFeatures":["Tensor is a subtype of NdArray.","We can use tensors for numeric calculations (but not nd-arrays)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"Tensors or Nd-arrays","narrative":"*What is the difference?*\n\nIn the world of machine learning we use something called a **'tensor'** to represent data.\nThey might be called **'nd-arrays'** in some other frameworks,\nbut although they are very similar, \nthere are also some important distinctions to be made between these two concepts.\nBoth are at their core merely multidimensional arrays, however,\nthey are different in their typical usage and API.\nnd-arrays are merely used to represent any type of data as a \ncollection of elements in a multidimensional grid,  \ntensors on the other hand have additional requirements.\nThey are a type of nd-array which stores numeric data \nas well as expose various mathematical operations for said data.\nIn that sense it is actually merely a more complex kind of number.\nThis concept actually comes from the field of physics, \nwhere it is used to represent a physical quantity.\n\nNeureka models both concepts through the `Tsr` and the `Nda` interfaces.\n`Nda` is an abbreviation of `NdArray`, and `Tsr` is an abbreviation of `Tensor`.\nThe `Tsr` type is a subtype of the `Nda` type, exposing additional methods\nlike for example `plus`, `minus`, `times` and `divide`.\nBoth can be instantiated through static factory methods (and a fluent builder API)."},"ut.ndas.Nda_Assign_Spec":{"executedFeatures":["Assignment can be easily achieved through subscription operators.","We can assign one slice into another one.","We can use the \"mut\" API to assign the contents of one nd-array into another one."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"Nda Inline Assignment","narrative":"In this specification we cover the behaviour of nda's with respect to the assignment operation\n    as well as the assignment of individual Nda items."},"ut.ndas.Nda_Framing":{"executedFeatures":["Concatenating 2 labeled nd-arrays will produce a nd-array which is also labeled.","The slice of a labeled vector is labeled too.","We can concatenate more than 2 nd-arrays.","We can label the columns and rows of a rank 3 nd-array.","We can label the columns of a rank 2 nd-array.","We can use labels as selectors for slicing."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":17},"title":"NDA Framing","narrative":"Framing an nd-array is all about naming its axes and then using those names to\n    access, read or write its values in a more convenient and human readable way."},"ut.ndas.Nda_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndas.Nda_Mutation_Spec":{"executedFeatures":["A ND-Array can be mutated simply using the \"set\" method.","A ND-Array can be mutated using the \"at(..).set(..)\" methods.","A simple vector ND-Array can be mutated using the \"at(..).set(..)\" methods.","A simple vector ND-Array can be mutated using the \"setItemAt\" method.","We can use the subscription operator to mutate a simple vector ND-Array.","We can use the subscription operator to mutate an ND-Array."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":9},"title":"Mutating ND-Arrays","narrative":"ND-Arrays should be considered immutable, so we should prefer creating new \n    ND-Arrays from existing ones using wither methods.\n    However this is not always a good idea as it can be expensive to create new\n    ND-Arrays, especially if the ND-Array is very large.\n    The ability to mutate ND-Arrays is therefore provided, but only\n    accessible via the mutation API exposed by the `getMut()` method."},"ut.ndas.Nda_Wither_Specification":{"executedFeatures":["An Nda can be labeled.","We can create a new Nda instance with a different shape."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"Nda Withers","narrative":"Immutability is a core concept of the Neureka library.\n    This means that the Nda API does not expose mutability directly.\n    Instead, the API exposes methods that return new instances of Nda\n    that are derived from the original instance."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"Making Arrays N-Dimensional","narrative":"Under the hood Neureka implements powerful indexing \n    abstractions through the `NDConfiguration` interface and its various implementations.\n    This allows for the creation of tensors/nd-arrays with arbitrary dimensions, \n    the ability to slice them into smaller tensors/nd-arrays with the same underlying data,\n    and finally the ability to reshape their axes (like transposing them for example).\n    \n    This specification however only focuses on the behaviour of the `NDConfiguration` interface\n    which translates various types of indices."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"What it means to be N-Dimensional","narrative":"This specification covers how implementations\n    of the `NDConfiguration` interface manage to define\n    what it means to be a n-dimensional tensor/nd-array."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"Reshaping Tensors","narrative":"Reshaping a tensor means changing its shape.\n    This is a very important operation in Neureka, because it allows for the creation of new views on the same data.\n    This is very useful for example when you want to perform a matrix multiplication on a tensor which is not a matrix.\n    In this case you can reshape the tensor to a matrix and then perform the multiplication.\n\n    Reshaping a tensor is also very useful when you want to perform other kinds of linear\n    operations like for example doing 4D convolution with a tensor which is not a 4D tensor.\n    In this case you can create a reshape 4D tensor then perform the convolution.\n  \n    Reshaping is also a very cheap operation because it does not copy any data but merely\n    creates a new view on the same data with a different access pattern."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":12},"title":"Reshaping Slices of Tensors","narrative":"Neureka provides a convenient way to reshape tensors\n    even if they are slices of other tensors sharing the same underlying data.\n    This is possible because of the under the hood indexing \n    abstractions provided by the `NDConfiguration` interface and its various implementations."},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":21668},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":103},"title":"","narrative":"ADAM is a more powerful alternative to the classical stochastic gradient descent. \n    It combines the best properties of the AdaGrad and the RMSProp algorithms, which makes \n    it especially well suited for sparse gradients and noisy data.\n    Adam is the most post popular among the adaptive optimizers\n    because its adaptive learning rate working so well with sparse datasets."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":22},"title":"","narrative":"Momentum is an extension to the gradient descent optimization \n    algorithm that allows the search to build inertia in a direction \n    in the search space and overcome the oscillations of noisy \n    gradients and coast across flat spots of the search space."},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSProp_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":45},"title":"","narrative":"**Root Mean Squared Propagation**, or RMSProp, is an extension of gradient \n    descent and the AdaGrad version of gradient descent that uses a \n    decaying average of partial gradients in the adaptation of the \n    step size for each parameter."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy of a tensor will be flagged as such.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":37},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":65},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Expression_Based_Tensor_Instantiation_Spec":{"executedFeatures":["A tensor can be created from a function as expression.","We can instantiate tensors from various simple string expressions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"Expression based Tensor Instantiation","narrative":"This specification defines how a tensor can be instantiated\n    using string expressions, which define operations to be executed.\n    This form of tensor instantiation is very useful to avoid boilerplate code."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.tensors.Functional_Nda_Spec":{"executedFeatures":["ND-Array mapping lambdas produce expected nd-arrays.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a nd-array using various predicate receiving methods","We can collect a stream into a nd-array.","We can find both min and max items in a tensor by providing a comparator.","We can find both min and max items in an ND-array by providing a comparator.","We can initialize an ND-Array using a filler lambda mapping indices to items.","We can use the \"filter\" method as a shortcut for \"stream().filter(..)\".","We can use the \"flatMap\" method as a shortcut for \"stream().flatMap(..)\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":907},"title":"Functional ND-Arrays","narrative":"ND-Arrays expose a powerful API for performing operations on them\n    in a functional style."},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods","We can find both min and max items in a tensor by providing a comparator.","We can initialize a tensor using a filler lambda mapping indices to items.","We can use the \"filter\" method as a shortcut for \"stream().filter(..)\".","We can use the \"flatMap\" method as a shortcut for \"stream().flatMap(..)\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":980},"title":"Functional Tensors","narrative":"Tensors expose a powerful API for performing operations on them\n    in a functional style."},"ut.tensors.Tensor_Assign_Spec":{"executedFeatures":["Assignment can be easily achieved through subscription operators.","We can assign one slice into another one.","We can use the \"mut\" API to assign the contents of one tensor into another one."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Tsr Inline Assignment","narrative":"In this specification we cover the behaviour of tensors with respect to the assignment operation\n    as well as the assignment of individual tensor items."},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":17},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Conversion_Spec":{"executedFeatures":["Tensors value type can be changed by calling \"toType(...)\".","We can change the data type of all kinds of tensors.","We turn a tensor into a scalar value or string through the \"as\" operator!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"Tensor Type Conversion","narrative":"Here we specify how a tensor can be converted to other data types\n    like for example another tensor of a different data type."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Convolution can be performed using non-quadratic matrix tensors.","Convolution can be performed using tensors with an additional dimension as batch size.","Convolution with tensors of the same shape is equivalent to a dot product.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result.","We can perform a convolution operation on a 2D tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":10,"totalFeatures":10,"passed":10,"successRate":1.0,"time":1696},"title":"Tensor Convolution","narrative":"This specification shows how convolution can be performed on tensors.\n\n    Convolution is a linear operation which is not only important for image processing but also\n    a central player in the field of machine learning (especially for computer vision).\n    It is used to extract features from images and other typically ~2 dimensional data.\n    Other than that it is extremely important in the field of signal processing."},"ut.tensors.Tensor_Device_Spec":{"executedFeatures":["Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Tensors try to migrate themselves to a device that is being added to them as component.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":11},"title":"Tensors on Devices","narrative":"This unit test specification covers \n    the expected behavior of tensors when interacting\n    with instances of implementations of the Device interface."},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","We can create a tensor of strings."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Instantiation_Spec":{"executedFeatures":["A matrix tensor can be instantiated using lists for it's shape and values.","A simple 2D vector can be instantiated using lists for it's shape and values.","Passing a seed in the form of a String to a tensor produces pseudo random items.","Scalar tensors can be created via static factory methods","Tensors can be instantiated based on arrays for both shapes and values.","Tensors can be instantiated with String seed.","Vector tensors can be instantiated via factory methods."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":26},"title":"Instantiating Tensors","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to show how a tensor can be instantiated in different ways."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Indexing after reshaping works as expected.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","The tensor data array can be modified by targeting them with an index.","We can manipulate the underlying data array of a tensor through the mut API.","We can re-populate a tensor of shorts from a single scalar value!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":85},"title":"Reading and Writing Tensor Items","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to read from and write to the state of a tensor."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":325},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Scalar broadcasting works across devices.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":12,"totalFeatures":12,"passed":12,"successRate":1.0,"time":9325},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":31},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a item type class and nested lists.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!","We can create scalar tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":41},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type and data array\n    among other things."},"ut.tensors.Tensor_Stats_Spec":{"executedFeatures":["Both the min and max operation support autograd (back-propagation).","The sum operation support autograd (back-propagation).","There is no need to use a function, we can use the min() and max() methods on tensors instead.","We can get pre-instantiated min and max functions from the library context.","We can use the \"sum\" method to sum the items of a tensor.","We can use the max operation as a function"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":618},"title":"Reducing Tensors","narrative":"Various kinds of operations reduce tensors to scalars,\n    the most common ones being the min and max operations \n    which find the smallest as well as largest number among all \n    items of a tensor.\n    Neureka exposes various different ways to achieve this,\n    all of which are also differential (autograd support)."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":213},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors from occurring unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The DeviceCleaner triggers registered cleaner actions when things are eligible for GC."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":401},"title":"How Neureka Cleans Up","narrative":"Under the hood \n    Neureka deals whith large arrays of\n    data, which are often times \n    native data arrays requiring explicit\n    memory freeing!\n    This freeing of memory can happen at any time\n    during the livetime of a nd-array, however\n    it should happen at least up until the nd-arra/tensor\n    objects representing their referenced data arrays become\n    eligible for garbage collection.\n    This specification ensures that the custom garbage\n    cleaner implementation used by Neureka fulfills this role"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":881},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":94},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1633},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":204},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Cross device system test runs successfully.","Mapping tensors works for every device (even if they are not used).","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":13107},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":581},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5027},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["The long broad integration test runs successfully."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":62},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":282},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":31},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":165},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":230},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":330},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":270},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":139},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations have expected properties.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":146},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":195},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The CPU matrix multiplication implementation works as expected.","The internal matrix multiplication test script runs!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2794},"title":"Internal CPU based Matrix Multiplication","narrative":"This specification covers library internal matrix multiplication logic,\n    specifically the CPU implementation.\n    Do not depend on the API used in this specification as it is subject to change!"},"ut.backend.core.OpenCL_Backend_Spec":{"executedFeatures":["The OpenCL backend context can load implementations."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The \"matMul\" method allows us to perform matrix multiplication.","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":47},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":35},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":83},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":74},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":94},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":32},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result","The softmax function can be applied to tensors with more than one dimension.","The tensor API has built-in methods for applying functions.","We can collect a stream into a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":222},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":0},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":47},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Devices also store slices which can also be restored just like normal tensors.","Devices expose an API for accessing (reading and writing) the data of a tensor.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","In total there are 3 different types of methods for finding device instances.","Passing a numeric array to a tensor should modify its contents!","Virtual tensors stay virtual when outsourced.","We can find Device implementations or null by passing search keys to the \"get\" method.","We can query the backend for devices by specifying both the requested type and a key word."],"ignoredFeatures":["Devices cannot store slices whose parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":175},"title":"Finding Device Types","narrative":"Neureka introduces a the concept of a `Device` which is an interface\n    that represents a computational device used for executing tensor / nd-array operations on them.\n    The `Device` interface is implemented by various classes which represent\n    different types of accelerator hardware such as `CPUs`, `GPUs`, `TPUs`, `FPGAs`, etc.\n    These various `Device` types can not be instantiated directly because they model \n    the concrete and finite hardware that is available on any given system Neureka is running on.\n    This means that they are usually instantiated lazily upon access request or \n    upfront by the library backend (usually a backend extension built fo a specific device).\n    In order to find these instances embedded in the library backend the `Device` interface\n    exposes various static methods which can be used to find a device instance by name or type."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":142},"title":"FileDevice, Storing Tensors in Files","narrative":"The `FileDevice` class, one of many implementations of the `Device` interface, \n    represents a file directory which can store and load tensors as files (`idx`, `jpg`, `png`...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel.","The OpenCLDevice produces a working optimized Function for doubles.","The OpenCLDevice produces a working optimized Function for floats."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":251},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.CPU_Kernel_Spec":{"executedFeatures":["The Reduce implementation for the CPU has realistic behaviour","The Sum implementation for the CPU has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":83},"title":"","narrative":""},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":47},"title":"","narrative":""},"ut.device.internal.OpenCL_Kernel_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour","The Reduce implementation for the OpenCLDevice has realistic behaviour","The Sum implementation for the OpenCLDevice has realistic behaviour","The Sum implementation for the OpenCLDevice has realistic behaviour for when the number of elements is a prime."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":31},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","We can get the items of an outsourced tensor as a primitive array.","We can take a look at the underlying data array of an outsourced tensor through the unsafe API."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":393},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default tensors use the `CPU` device, but sometimes we want to\n    use something more suitable for large amounts of data and a high degree of parallelization.\n    This is were the `OpenCLDevice` comes into play!\n    It is a `Device` implementation built on top of the JOCL library, a thin OpenCL API.\n    We expect the `OpenCLDevice` to store tensors as well as being able to read and write\n    data from and to stored tensors.\n    Also, an `OpenCLDevice` should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5077},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the `OpenCLDevice` class which\n    represents a single device with OpenCL support.\n    Besides that, there is also the `OpenCLContext` class which\n    represents a OpenCL contexts, platforms and multiple devices on said platforms..."},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":78},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["A matrix (rank 2 tensor) can be labeled and their labels can be used to extract slices / subsets.","A tensor can be labeled partially.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","We can add labels to tensors through lists or maps passed to the \"label()\" method."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":30},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.introductions.Tensor_NDArray_Spec":{"executedFeatures":["Tensor is a subtype of NdArray.","We can use tensors for numeric calculations (but not nd-arrays)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":0},"title":"Tensors or Nd-arrays","narrative":"*What is the difference?*\n\nIn the world of machine learning we use something called a **'tensor'** to represent data.\nThey might be called **'nd-arrays'** in some other frameworks,\nbut although they are very similar, \nthere are also some important distinctions to be made between these two concepts.\nBoth are at their core merely multidimensional arrays, however,\nthey are different in their typical usage and API.\nnd-arrays are merely used to represent any type of data as a \ncollection of elements in a multidimensional grid,  \ntensors on the other hand have additional requirements.\nThey are a type of nd-array which stores numeric data \nas well as expose various mathematical operations for said data.\nIn that sense it is actually merely a more complex kind of number.\nThis concept actually comes from the field of physics, \nwhere it is used to represent a physical quantity.\n\nNeureka models both concepts through the `Tsr` and the `Nda` interfaces.\n`Nda` is an abbreviation of `NdArray`, and `Tsr` is an abbreviation of `Tensor`.\nThe `Tsr` type is a subtype of the `Nda` type, exposing additional methods\nlike for example `plus`, `minus`, `times` and `divide`.\nBoth can be instantiated through static factory methods (and a fluent builder API)."},"ut.ndas.Nda_Assign_Spec":{"executedFeatures":["Assignment can be easily achieved through subscription operators.","We can assign one slice into another one.","We can use the \"mut\" API to assign the contents of one nd-array into another one."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":0},"title":"Nda Inline Assignment","narrative":"In this specification we cover the behaviour of nda's with respect to the assignment operation\n    as well as the assignment of individual Nda items."},"ut.ndas.Nda_Framing":{"executedFeatures":["Concatenating 2 labeled nd-arrays will produce a nd-array which is also labeled.","The slice of a labeled vector is labeled too.","We can concatenate more than 2 nd-arrays.","We can label the columns and rows of a rank 3 nd-array.","We can label the columns of a rank 2 nd-array.","We can use labels as selectors for slicing."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":16},"title":"NDA Framing","narrative":"Framing an nd-array is all about naming its axes and then using those names to\n    access, read or write its values in a more convenient and human readable way."},"ut.ndas.Nda_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","Common types of nd-arrays are best instantiated using type specific convenience methods.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndas.Nda_Mutation_Spec":{"executedFeatures":["A ND-Array can be mutated simply using the \"set\" method.","A ND-Array can be mutated using the \"at(..).set(..)\" methods.","A simple vector ND-Array can be mutated using the \"at(..).set(..)\" methods.","A simple vector ND-Array can be mutated using the \"setItemAt\" method.","We can use the subscription operator to mutate a simple vector ND-Array.","We can use the subscription operator to mutate an ND-Array."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":16},"title":"Mutating ND-Arrays","narrative":"ND-Arrays should be considered immutable, so we should prefer creating new \n    ND-Arrays from existing ones using wither methods.\n    However this is not always a good idea as it can be expensive to create new\n    ND-Arrays, especially if the ND-Array is very large.\n    The ability to mutate ND-Arrays is therefore provided, but only\n    accessible via the mutation API exposed by the `getMut()` method."},"ut.ndas.Nda_Wither_Specification":{"executedFeatures":["An Nda can be labeled.","We can create a new Nda instance with a different shape."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":0},"title":"Nda Withers","narrative":"Immutability is a core concept of the Neureka library.\n    This means that the Nda API does not expose mutability directly.\n    Instead, the API exposes methods that return new instances of Nda\n    that are derived from the original instance."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":115},"title":"Making Arrays N-Dimensional","narrative":"Under the hood Neureka implements powerful indexing \n    abstractions through the `NDConfiguration` interface and its various implementations.\n    This allows for the creation of tensors/nd-arrays with arbitrary dimensions, \n    the ability to slice them into smaller tensors/nd-arrays with the same underlying data,\n    and finally the ability to reshape their axes (like transposing them for example).\n    \n    This specification however only focuses on the behaviour of the `NDConfiguration` interface\n    which translates various types of indices."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"What it means to be N-Dimensional","narrative":"This specification covers how implementations\n    of the `NDConfiguration` interface manage to define\n    what it means to be a n-dimensional tensor/nd-array."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"Reshaping Tensors","narrative":"Reshaping a tensor means changing its shape.\n    This is a very important operation in Neureka, because it allows for the creation of new views on the same data.\n    This is very useful for example when you want to perform a matrix multiplication on a tensor which is not a matrix.\n    In this case you can reshape the tensor to a matrix and then perform the multiplication.\n\n    Reshaping a tensor is also very useful when you want to perform other kinds of linear\n    operations like for example doing 4D convolution with a tensor which is not a 4D tensor.\n    In this case you can create a reshape 4D tensor then perform the convolution.\n  \n    Reshaping is also a very cheap operation because it does not copy any data but merely\n    creates a new view on the same data with a different access pattern."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"Reshaping Slices of Tensors","narrative":"Neureka provides a convenient way to reshape tensors\n    even if they are slices of other tensors sharing the same underlying data.\n    This is possible because of the under the hood indexing \n    abstractions provided by the `NDConfiguration` interface and its various implementations."},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":22018},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":138},"title":"","narrative":"ADAM is a more powerful alternative to the classical stochastic gradient descent. \n    It combines the best properties of the AdaGrad and the RMSProp algorithms, which makes \n    it especially well suited for sparse gradients and noisy data.\n    Adam is the most post popular among the adaptive optimizers\n    because its adaptive learning rate working so well with sparse datasets."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":31},"title":"","narrative":"Momentum is an extension to the gradient descent optimization \n    algorithm that allows the search to build inertia in a direction \n    in the search space and overcome the oscillations of noisy \n    gradients and coast across flat spots of the search space."},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSProp_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":64},"title":"","narrative":"**Root Mean Squared Propagation**, or RMSProp, is an extension of gradient \n    descent and the AdaGrad version of gradient descent that uses a \n    decaying average of partial gradients in the adaptation of the \n    step size for each parameter."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy of a tensor will be flagged as such.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":47},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":84},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Expression_Based_Tensor_Instantiation_Spec":{"executedFeatures":["A tensor can be created from a function as expression.","We can instantiate tensors from various simple string expressions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Expression based Tensor Instantiation","narrative":"This specification defines how a tensor can be instantiated\n    using string expressions, which define operations to be executed.\n    This form of tensor instantiation is very useful to avoid boilerplate code."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":46},"title":"","narrative":""},"ut.tensors.Functional_Nda_Spec":{"executedFeatures":["ND-Array mapping lambdas produce expected nd-arrays.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a nd-array using various predicate receiving methods","We can collect a stream into a nd-array.","We can find both min and max items in a tensor by providing a comparator.","We can find both min and max items in an ND-array by providing a comparator.","We can initialize an ND-Array using a filler lambda mapping indices to items.","We can use the \"filter\" method as a shortcut for \"stream().filter(..)\".","We can use the \"flatMap\" method as a shortcut for \"stream().flatMap(..)\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":964},"title":"Functional ND-Arrays","narrative":"ND-Arrays expose a powerful API for performing operations on them\n    in a functional style."},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods","We can find both min and max items in a tensor by providing a comparator.","We can initialize a tensor using a filler lambda mapping indices to items.","We can use the \"filter\" method as a shortcut for \"stream().filter(..)\".","We can use the \"flatMap\" method as a shortcut for \"stream().flatMap(..)\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":1870},"title":"Functional Tensors","narrative":"Tensors expose a powerful API for performing operations on them\n    in a functional style."},"ut.tensors.Tensor_Assign_Spec":{"executedFeatures":["Assignment can be easily achieved through subscription operators.","We can assign one slice into another one.","We can use the \"mut\" API to assign the contents of one tensor into another one."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"Tsr Inline Assignment","narrative":"In this specification we cover the behaviour of tensors with respect to the assignment operation\n    as well as the assignment of individual tensor items."},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":16},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Conversion_Spec":{"executedFeatures":["Tensors value type can be changed by calling \"toType(...)\".","We can change the data type of all kinds of tensors.","We turn a tensor into a scalar value or string through the \"as\" operator!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"Tensor Type Conversion","narrative":"Here we specify how a tensor can be converted to other data types\n    like for example another tensor of a different data type."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Convolution can be performed using non-quadratic matrix tensors.","Convolution can be performed using tensors with an additional dimension as batch size.","Convolution with tensors of the same shape is equivalent to a dot product.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result.","We can perform a convolution operation on a 2D tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":10,"totalFeatures":10,"passed":10,"successRate":1.0,"time":1890},"title":"Tensor Convolution","narrative":"This specification shows how convolution can be performed on tensors.\n\n    Convolution is a linear operation which is not only important for image processing but also\n    a central player in the field of machine learning (especially for computer vision).\n    It is used to extract features from images and other typically ~2 dimensional data.\n    Other than that it is extremely important in the field of signal processing."},"ut.tensors.Tensor_Device_Spec":{"executedFeatures":["Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Tensors try to migrate themselves to a device that is being added to them as component.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":15},"title":"Tensors on Devices","narrative":"This unit test specification covers \n    the expected behavior of tensors when interacting\n    with instances of implementations of the Device interface."},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","We can create a tensor of strings."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":0},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":0},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Instantiation_Spec":{"executedFeatures":["A matrix tensor can be instantiated using lists for it's shape and values.","A simple 2D vector can be instantiated using lists for it's shape and values.","Passing a seed in the form of a String to a tensor produces pseudo random items.","Scalar tensors can be created via static factory methods","Tensors can be instantiated based on arrays for both shapes and values.","Tensors can be instantiated with String seed.","Vector tensors can be instantiated via factory methods."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":36},"title":"Instantiating Tensors","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to show how a tensor can be instantiated in different ways."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Indexing after reshaping works as expected.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","The tensor data array can be modified by targeting them with an index.","We can manipulate the underlying data array of a tensor through the mut API.","We can re-populate a tensor of shorts from a single scalar value!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":64},"title":"Reading and Writing Tensor Items","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to read from and write to the state of a tensor."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":298},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Scalar broadcasting works across devices.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":12,"totalFeatures":12,"passed":12,"successRate":1.0,"time":10902},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":32},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a item type class and nested lists.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!","We can create scalar tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":50},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type and data array\n    among other things."},"ut.tensors.Tensor_Stats_Spec":{"executedFeatures":["Both the min and max operation support autograd (back-propagation).","The sum operation support autograd (back-propagation).","There is no need to use a function, we can use the min() and max() methods on tensors instead.","We can get pre-instantiated min and max functions from the library context.","We can use the \"sum\" method to sum the items of a tensor.","We can use the max operation as a function"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":503},"title":"Reducing Tensors","narrative":"Various kinds of operations reduce tensors to scalars,\n    the most common ones being the min and max operations \n    which find the smallest as well as largest number among all \n    items of a tensor.\n    Neureka exposes various different ways to achieve this,\n    all of which are also differential (autograd support)."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":249},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors from occurring unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The DeviceCleaner triggers registered cleaner actions when things are eligible for GC."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":455},"title":"How Neureka Cleans Up","narrative":"Under the hood \n    Neureka deals whith large arrays of\n    data, which are often times \n    native data arrays requiring explicit\n    memory freeing!\n    This freeing of memory can happen at any time\n    during the livetime of a nd-array, however\n    it should happen at least up until the nd-arra/tensor\n    objects representing their referenced data arrays become\n    eligible for garbage collection.\n    This specification ensures that the custom garbage\n    cleaner implementation used by Neureka fulfills this role"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":968},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""}}