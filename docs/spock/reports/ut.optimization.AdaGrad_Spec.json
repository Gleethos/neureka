{
  "className":"ut.optimization.AdaGrad_Spec",
  "title":"",
  "narrative":"",
  "subjects":["neureka.optimization.Optimizer"],
  "statistics":{
    "runs":"1",
    "successRate":"100.0%",
    "failures":"0",
    "errors":"0",
    "skipped":"0",
    "duration":"0.019 seconds"
  },
  "headers":["\n                <h2> AdaGrad Optimizer Behavior </h2>\n                <br> \n                <p>\n                    This specification check the behavior of the AdaGrad class.        \n                </p>\n            "],"tags":{},"see":[],
  "features":[ 
    {
      "id":"AdaGrad optimizes according to expected inputs",
      "result":"PASS",
      "duration":"0.011 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":[]
      },
      "blocks":[
        {"kind":"given","text":"A new scalar gradient tensor is being created.","code":["Tsr g = Tsr.of(expectedWeight)"]},

        {"kind":"and","text":"The following input is being applied to the tensor (and internal optimizer)...","code":["w.set( Tsr.of( (double)gradient ) )","w.applyGradient()"]},

        {"kind":"expect","text":"The following state emerges:","code":["w.toString().contains(g.toString())","w.shape.hashCode()==g.shape.hashCode()","w.translation().hashCode()==g.translation().hashCode()","w.indicesMap().hashCode()==g.indicesMap().hashCode()","w.spread().hashCode()==g.spread().hashCode()","w.offset().hashCode()==g.offset().hashCode()"]},

        {"kind":"where","text":"","code":{"gradient":["-3","-3","2","-3","2","2","-4","-3","-3","2"],"expectedWeight":["0.00999","0.01707","0.01280001","0.01819","0.01481","0.01161","0.0170001","0.02075","0.02426","0.02198"]}}
      ],
      "problems":{"dataValues":[], "errors":[]}
    }
  
  ],
  "generator":"https://github.com/renatoathaydes/spock-reports"
}