<!DOCTYPE html><html>
<head>
<meta http-equiv='Content-Type' content='text/html; charset=utf-8'></meta>
<style>
body {
    font-family: Helvetica, Arial, sans-serif;
    font-weight: 300;
}

h2 {
    font-weight: 400;
}

hr {
    margin-bottom: 1.5em;
}

h3 {
    font-weight: 200;
}

table {
    margin: 7px;
    -webkit-box-shadow: 0px 0px 3px 1px rgba(0,0,0,0.75);
    -moz-box-shadow: 0px 0px 3px 1px rgba(0,0,0,0.75);
    box-shadow: 0px 0px 3px 1px rgba(0,0,0,0.75);
}

.ignored {
    color: gray;
}

div.project-header {
    margin-bottom: 10px;
    font-size: large;
}

div.project-header &gt; span.project-name {

                        }

div.project-header &gt; span.project-version {
                            padding-left: 20px;
                        }

div.date-test-ran {
    font-size: small;
    font-style: italic;
}

div.spec-title {
    padding: 10px 0px 5px 0px;
}

tr.error td, td.error {
    background-color: #F89A4F !important;
}

tr.failure td, td.failure {
    color: red;
}

div.footer {
    text-align: center;
    font-size: small;
}






.back-link {
    font-size: small;
    font-weight: bold;
}


div.date-test-ran {
    font-size: small;
    font-style: italic;
}

table.features-table {
    width: 99%;
    text-align: left;
}

table.summary-table {
    width: 99%;

    font-weight: bold;
    font-size: small;
}

table.summary-table tbody {
    width: 99%;
    text-align: left;
}

table.summary-table th {
    background: lightblue;
    padding: 6px;
}

table.summary-table td {
    background: #E0E0E0;
    padding: 6px;
}

pre.title {
    font-family: inherit;
    font-size: 24px;
    line-height: 28px;
    letter-spacing: -1px;
    color: #333;
}

pre.narrative {
    font-family: inherit;
    font-size: 18px;
    font-style: italic;
    line-height: 23px;
    letter-spacing: -1px;
    color: #333;
}

.feature-description {
    font-size: large;
    background: lightblue;
    padding: 12px;
}

.feature-toc-error {
    color: #F89A4F;
}

.feature-toc-failure {
    color: #FF8080;
}

.feature-toc-ignored {
    color: lightgray;
}

.feature-toc-pass {
    color: green;
}

.feature-description.error {
    background: #F89A4F;
}

.feature-description.failure {
    background: #FF8080;
}

.feature-description.ignored {
    background: lightgray;
}

.feature-description.ignored .reason {
    color: black;
    font-style: italic;
    font-size: small;
}

div.extra-info {
    font-size: small;
}

div.spec-headers {
    margin: 4px;
    font-style: italic;
}

div.spec-header {
}

div.issues {
    margin-top: 6px;
    padding: 10px 5px 5px 5px;
    background-color: lemonchiffon;
    color: black;
    font-weight: 500;
    font-size: small;
    max-width: 50%;
}

div.pending-feature {
    background-color: dodgerblue;
    color: white;
    margin-top: 6px;
    padding: 5px;
    text-align: center;
    font-size: small;
    max-width: 120px;
}

div.problem-description {
    padding: 10px;
    background: pink;
    border-radius: 10px;
}

div.problem-header {
    font-weight: bold;
    color: red;
}

div.problem-list {

}

table.ex-table{
    width: 98%;
}

table.ex-table th {
    background: lightblue;
    padding: 5px;
}

table.ex-table td {
    background: #E0E0E0;
    padding: 2px 5px 2px 5px;
}

table td {
    min-width: 50px;
}

col.block-kind-col {
    width: 70px;
}

span.spec-header {
    font-weight: bold;
}

div.spec-text {
    /*color: green;*/
}

div.spec-status {
    font-style: italic;
}

.ignored {
    color: gray;
}

td.ex-result {
    text-align: center;
    background: white !important;
}

.ex-pass {
    color: darkgreen;
}

.ex-fail {
    color: red;
    font-weight: bold;
}

div.block-kind {
    margin: 2px;
    font-style: italic;
}

div.block-text {

}

pre.block-source {
    background-color: whitesmoke;
    padding: 10px;
}

pre.block-source.error {
    background-color: pink;
    color: red;
    font-weight: bold;
}

pre.block-source.pre-error {

}

pre.block-source.before-error {
    margin-bottom: -14px;
}

pre.block-source.after-error {
    color: gray;
    margin-top: -14px;
}

pre.block-source.post-error {
    color: gray;
}

div.footer {
    text-align: center;
    font-size: small;
}</style>
</head>
<body>
<h2>Report for it.autograd.Autograd_Flags_Explained</h2>
<hr></hr>
<div class='back-link'>
<a href='index.html'>&lt;&lt; Back</a>
</div>
<div class='summary-report'>
<h3>Summary:</h3>
<div class='date-test-ran'>Created on Wed Nov 17 22:25:01 CET 2021 by Daglemino</div>
<table class='summary-table'>
<thead>
<tr>
<th>Executed features</th>
<th>Passed</th>
<th>Failures</th>
<th>Errors</th>
<th>Skipped</th>
<th>Success rate</th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>8</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>100.0%</td>
<td>8.533 seconds</td>
</tr>
</tbody>
</table>
</div>
<div class='spec-headers'>
<div class='spec-header'>
            <b> Autograd Advanced - Custom Autograd </b>                                                            <br>
                                                                                                                    <br>
            Neureka does not necessarily perform autograd eagerly.                                                  <br>
            If required then auto-differentiation will occur as one would expect                                    <br>
            similarly to the way PyTorch's autograd works.                                                          <br>
            However for many use cases it might make sense to use different variants                                <br>
            of auto-differentiation.                                                                                <br>
            This specification covers precisely these different autograd modes.                                     <br>
                                                                                                                    <br>
        </div>
</div>
<h3>Features:</h3>
<table class='features-table'>
<colgroup>
<col class='block-kind-col'></col>
<col class='block-text-col'></col>
</colgroup>
<tbody>
<ul id='toc'>
<li>
<a href='#1041041227' class='feature-toc-pass'>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: false, whenUse: false, doJIT: false, afterBack: .*1.*4\.5.*, afterUse: .*1.*4\.5.*, afterRqd: .*1.*4\.5.*, afterAll: .*1.*4\.5.*, #0]</a>
</li>
<li>
<a href='#1701006427' class='feature-toc-pass'>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: true, whenUse: false, doJIT: false, afterBack: .*1.*4\.5.*, afterUse: .*1.*4\.5.*, afterRqd: .*5\.5.*null.*, afterAll: .*5\.5.*null.*, #1]</a>
</li>
<li>
<a href='#-1072517419' class='feature-toc-pass'>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: false, whenUse: true, doJIT: false, afterBack: .*1.*4\.5.*, afterUse: .*5\.5.*null.*, afterRqd: .*5\.5.*null.*, afterAll: .*5\.5.*null.*, #2]</a>
</li>
<li>
<a href='#1992288653' class='feature-toc-pass'>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: true, whenUse: true, doJIT: false, afterBack: .*1.*4\.5.*, afterUse: .*1.*4\.5.*, afterRqd: .*1.*4\.5.*, afterAll: .*5\.5.*null.*, #3]</a>
</li>
<li>
<a href='#516755762' class='feature-toc-pass'>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: false, whenUse: false, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*1.*null.*, afterAll: .*1.*null.*, #4]</a>
</li>
<li>
<a href='#-1152714464' class='feature-toc-pass'>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: true, whenUse: false, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*5\.5.*null.*, afterAll: .*5\.5.*null.*, #5]</a>
</li>
<li>
<a href='#865054400' class='feature-toc-pass'>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: false, whenUse: true, doJIT: true, afterBack: .*1.*null.*, afterUse: .*5\.5.*null.*, afterRqd: .*5\.5.*null.*, afterAll: .*5\.5.*null.*, #6]</a>
</li>
<li>
<a href='#-42533674' class='feature-toc-pass'>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: true, whenUse: true, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*1.*null.*, afterAll: .*5\.5.*null.*, #7]</a>
</li>
</ul>
<tr>
<td colspan='10'>
<div class='feature-description' id='1041041227'>
<span>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: false, whenUse: false, doJIT: false, afterBack: .*1.*4\.5.*, afterUse: .*1.*4\.5.*, afterRqd: .*1.*4\.5.*, afterAll: .*1.*4\.5.*, #0]</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b>                 <br>
            This run covers the feature having the following settings :                                     <br>
                                                                                                            <br>
            <b> Neureka.instance().settings().autograd().: </b>                                             <br>
            isRetainingPendingErrorForJITProp := false                                                   <br>      
            isApplyingGradientWhenTensorIsUsed := false                                                <br>    
            isApplyingGradientWhenRequested := false                                                   <br>  
                                                                                                            <br>
            ...code producing the result : 'y*y*3'                                                        <br>                                                                                                        
                                                                                                            <br>                                                                                                       
            <b> is-Retaining-Pending-Error-For-JITProp : </b>                                               <br>
                                                                                                            <br>
            This flag enables an optimization technique which only propagates error values to               <br>
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them        <br>
            at divergent differentiation paths within the computation graph.                                <br>
            If the flag is set to true                                                                      <br>
            then error values will accumulate at such junction nodes.                                       <br>
            This technique however uses more memory but will                                                <br>
            improve performance for some networks substantially.                                            <br>
            The technique is termed JIT-Propagation.                                                        <br>
                                                                                                            <br>
                                                                                                            <br>
            <b> is-Applying-Gradient-When-Tensor-Is-Used : </b>                                             <br>
                                                                                                            <br>
            Gradients will automatically be applied (or JITed) to tensors as soon as                        <br>
            they are being used for calculation (GraphNode instantiation).                                  <br>
            This feature works well with JIT-Propagation.                                                   <br>
                                                                                                            <br>
                                                                                                            <br>      
            <b> is-Applying-Gradient-When-Requested : </b>                                                  <br>
                                                                                                            <br>
            Gradients will only be applied if requested.                                                    <br>
            Usually this happens immediately, however                                                       <br>
            if the flag 'applyGradientWhenTensorIsUsed' is set                                              <br>
            to true, then the tensor will only be updated by its                                            <br>
            gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation).         <br>
                                                                                                            <br>
            <b> Let's take a look :  </b>
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>We configure Neureka autograd:</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd
Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse
Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1).setRqsGradient(true)
def y = x + 2
Binding binding = new Binding()
binding.setVariable('x', x)
binding.setVariable('y', y)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The code snippet is being execute...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr z = new GroovyShell(binding).evaluate((code))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)
def xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterBack )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterUse )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x.setGradientApplyRequested( true )
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterRqd )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterAll )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Examples:</div>
</td>
<td>
<div class='spec-examples'>
<table class='ex-table'>
<thead>
<tr>
<th class='ex-header'>code</th>
<th class='ex-header'>whenRsd</th>
<th class='ex-header'>whenUse</th>
<th class='ex-header'>doJIT</th>
<th class='ex-header'>afterBack</th>
<th class='ex-header'>afterUse</th>
<th class='ex-header'>afterRqd</th>
<th class='ex-header'>afterAll</th>
</tr>
</thead>
<tbody>
<tr class='ex-pass'>
<td class='ex-value'>y*y*3</td>
<td class='ex-value'>false</td>
<td class='ex-value'>false</td>
<td class='ex-value'>false</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-result'>OK</td>
</tr>
</tbody>
</table>
</div>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='1701006427'>
<span>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: true, whenUse: false, doJIT: false, afterBack: .*1.*4\.5.*, afterUse: .*1.*4\.5.*, afterRqd: .*5\.5.*null.*, afterAll: .*5\.5.*null.*, #1]</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b>                 <br>
            This run covers the feature having the following settings :                                     <br>
                                                                                                            <br>
            <b> Neureka.instance().settings().autograd().: </b>                                             <br>
            isRetainingPendingErrorForJITProp := false                                                   <br>      
            isApplyingGradientWhenTensorIsUsed := false                                                <br>    
            isApplyingGradientWhenRequested := true                                                   <br>  
                                                                                                            <br>
            ...code producing the result : 'y*y*3'                                                        <br>                                                                                                        
                                                                                                            <br>                                                                                                       
            <b> is-Retaining-Pending-Error-For-JITProp : </b>                                               <br>
                                                                                                            <br>
            This flag enables an optimization technique which only propagates error values to               <br>
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them        <br>
            at divergent differentiation paths within the computation graph.                                <br>
            If the flag is set to true                                                                      <br>
            then error values will accumulate at such junction nodes.                                       <br>
            This technique however uses more memory but will                                                <br>
            improve performance for some networks substantially.                                            <br>
            The technique is termed JIT-Propagation.                                                        <br>
                                                                                                            <br>
                                                                                                            <br>
            <b> is-Applying-Gradient-When-Tensor-Is-Used : </b>                                             <br>
                                                                                                            <br>
            Gradients will automatically be applied (or JITed) to tensors as soon as                        <br>
            they are being used for calculation (GraphNode instantiation).                                  <br>
            This feature works well with JIT-Propagation.                                                   <br>
                                                                                                            <br>
                                                                                                            <br>      
            <b> is-Applying-Gradient-When-Requested : </b>                                                  <br>
                                                                                                            <br>
            Gradients will only be applied if requested.                                                    <br>
            Usually this happens immediately, however                                                       <br>
            if the flag 'applyGradientWhenTensorIsUsed' is set                                              <br>
            to true, then the tensor will only be updated by its                                            <br>
            gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation).         <br>
                                                                                                            <br>
            <b> Let's take a look :  </b>
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>We configure Neureka autograd:</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd
Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse
Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1).setRqsGradient(true)
def y = x + 2
Binding binding = new Binding()
binding.setVariable('x', x)
binding.setVariable('y', y)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The code snippet is being execute...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr z = new GroovyShell(binding).evaluate((code))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)
def xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterBack )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterUse )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x.setGradientApplyRequested( true )
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterRqd )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterAll )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Examples:</div>
</td>
<td>
<div class='spec-examples'>
<table class='ex-table'>
<thead>
<tr>
<th class='ex-header'>code</th>
<th class='ex-header'>whenRsd</th>
<th class='ex-header'>whenUse</th>
<th class='ex-header'>doJIT</th>
<th class='ex-header'>afterBack</th>
<th class='ex-header'>afterUse</th>
<th class='ex-header'>afterRqd</th>
<th class='ex-header'>afterAll</th>
</tr>
</thead>
<tbody>
<tr class='ex-pass'>
<td class='ex-value'>y*y*3</td>
<td class='ex-value'>true</td>
<td class='ex-value'>false</td>
<td class='ex-value'>false</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-result'>OK</td>
</tr>
</tbody>
</table>
</div>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='-1072517419'>
<span>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: false, whenUse: true, doJIT: false, afterBack: .*1.*4\.5.*, afterUse: .*5\.5.*null.*, afterRqd: .*5\.5.*null.*, afterAll: .*5\.5.*null.*, #2]</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b>                 <br>
            This run covers the feature having the following settings :                                     <br>
                                                                                                            <br>
            <b> Neureka.instance().settings().autograd().: </b>                                             <br>
            isRetainingPendingErrorForJITProp := false                                                   <br>      
            isApplyingGradientWhenTensorIsUsed := true                                                <br>    
            isApplyingGradientWhenRequested := false                                                   <br>  
                                                                                                            <br>
            ...code producing the result : 'y*y*3'                                                        <br>                                                                                                        
                                                                                                            <br>                                                                                                       
            <b> is-Retaining-Pending-Error-For-JITProp : </b>                                               <br>
                                                                                                            <br>
            This flag enables an optimization technique which only propagates error values to               <br>
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them        <br>
            at divergent differentiation paths within the computation graph.                                <br>
            If the flag is set to true                                                                      <br>
            then error values will accumulate at such junction nodes.                                       <br>
            This technique however uses more memory but will                                                <br>
            improve performance for some networks substantially.                                            <br>
            The technique is termed JIT-Propagation.                                                        <br>
                                                                                                            <br>
                                                                                                            <br>
            <b> is-Applying-Gradient-When-Tensor-Is-Used : </b>                                             <br>
                                                                                                            <br>
            Gradients will automatically be applied (or JITed) to tensors as soon as                        <br>
            they are being used for calculation (GraphNode instantiation).                                  <br>
            This feature works well with JIT-Propagation.                                                   <br>
                                                                                                            <br>
                                                                                                            <br>      
            <b> is-Applying-Gradient-When-Requested : </b>                                                  <br>
                                                                                                            <br>
            Gradients will only be applied if requested.                                                    <br>
            Usually this happens immediately, however                                                       <br>
            if the flag 'applyGradientWhenTensorIsUsed' is set                                              <br>
            to true, then the tensor will only be updated by its                                            <br>
            gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation).         <br>
                                                                                                            <br>
            <b> Let's take a look :  </b>
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>We configure Neureka autograd:</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd
Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse
Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1).setRqsGradient(true)
def y = x + 2
Binding binding = new Binding()
binding.setVariable('x', x)
binding.setVariable('y', y)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The code snippet is being execute...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr z = new GroovyShell(binding).evaluate((code))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)
def xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterBack )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterUse )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x.setGradientApplyRequested( true )
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterRqd )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterAll )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Examples:</div>
</td>
<td>
<div class='spec-examples'>
<table class='ex-table'>
<thead>
<tr>
<th class='ex-header'>code</th>
<th class='ex-header'>whenRsd</th>
<th class='ex-header'>whenUse</th>
<th class='ex-header'>doJIT</th>
<th class='ex-header'>afterBack</th>
<th class='ex-header'>afterUse</th>
<th class='ex-header'>afterRqd</th>
<th class='ex-header'>afterAll</th>
</tr>
</thead>
<tbody>
<tr class='ex-pass'>
<td class='ex-value'>y*y*3</td>
<td class='ex-value'>false</td>
<td class='ex-value'>true</td>
<td class='ex-value'>false</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-result'>OK</td>
</tr>
</tbody>
</table>
</div>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='1992288653'>
<span>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: true, whenUse: true, doJIT: false, afterBack: .*1.*4\.5.*, afterUse: .*1.*4\.5.*, afterRqd: .*1.*4\.5.*, afterAll: .*5\.5.*null.*, #3]</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b>                 <br>
            This run covers the feature having the following settings :                                     <br>
                                                                                                            <br>
            <b> Neureka.instance().settings().autograd().: </b>                                             <br>
            isRetainingPendingErrorForJITProp := false                                                   <br>      
            isApplyingGradientWhenTensorIsUsed := true                                                <br>    
            isApplyingGradientWhenRequested := true                                                   <br>  
                                                                                                            <br>
            ...code producing the result : 'y*y*3'                                                        <br>                                                                                                        
                                                                                                            <br>                                                                                                       
            <b> is-Retaining-Pending-Error-For-JITProp : </b>                                               <br>
                                                                                                            <br>
            This flag enables an optimization technique which only propagates error values to               <br>
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them        <br>
            at divergent differentiation paths within the computation graph.                                <br>
            If the flag is set to true                                                                      <br>
            then error values will accumulate at such junction nodes.                                       <br>
            This technique however uses more memory but will                                                <br>
            improve performance for some networks substantially.                                            <br>
            The technique is termed JIT-Propagation.                                                        <br>
                                                                                                            <br>
                                                                                                            <br>
            <b> is-Applying-Gradient-When-Tensor-Is-Used : </b>                                             <br>
                                                                                                            <br>
            Gradients will automatically be applied (or JITed) to tensors as soon as                        <br>
            they are being used for calculation (GraphNode instantiation).                                  <br>
            This feature works well with JIT-Propagation.                                                   <br>
                                                                                                            <br>
                                                                                                            <br>      
            <b> is-Applying-Gradient-When-Requested : </b>                                                  <br>
                                                                                                            <br>
            Gradients will only be applied if requested.                                                    <br>
            Usually this happens immediately, however                                                       <br>
            if the flag 'applyGradientWhenTensorIsUsed' is set                                              <br>
            to true, then the tensor will only be updated by its                                            <br>
            gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation).         <br>
                                                                                                            <br>
            <b> Let's take a look :  </b>
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>We configure Neureka autograd:</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd
Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse
Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1).setRqsGradient(true)
def y = x + 2
Binding binding = new Binding()
binding.setVariable('x', x)
binding.setVariable('y', y)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The code snippet is being execute...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr z = new GroovyShell(binding).evaluate((code))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)
def xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterBack )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterUse )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x.setGradientApplyRequested( true )
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterRqd )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterAll )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Examples:</div>
</td>
<td>
<div class='spec-examples'>
<table class='ex-table'>
<thead>
<tr>
<th class='ex-header'>code</th>
<th class='ex-header'>whenRsd</th>
<th class='ex-header'>whenUse</th>
<th class='ex-header'>doJIT</th>
<th class='ex-header'>afterBack</th>
<th class='ex-header'>afterUse</th>
<th class='ex-header'>afterRqd</th>
<th class='ex-header'>afterAll</th>
</tr>
</thead>
<tbody>
<tr class='ex-pass'>
<td class='ex-value'>y*y*3</td>
<td class='ex-value'>true</td>
<td class='ex-value'>true</td>
<td class='ex-value'>false</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*1.*4\.5.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-result'>OK</td>
</tr>
</tbody>
</table>
</div>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='516755762'>
<span>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: false, whenUse: false, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*1.*null.*, afterAll: .*1.*null.*, #4]</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b>                 <br>
            This run covers the feature having the following settings :                                     <br>
                                                                                                            <br>
            <b> Neureka.instance().settings().autograd().: </b>                                             <br>
            isRetainingPendingErrorForJITProp := true                                                   <br>      
            isApplyingGradientWhenTensorIsUsed := false                                                <br>    
            isApplyingGradientWhenRequested := false                                                   <br>  
                                                                                                            <br>
            ...code producing the result : 'y*y*3'                                                        <br>                                                                                                        
                                                                                                            <br>                                                                                                       
            <b> is-Retaining-Pending-Error-For-JITProp : </b>                                               <br>
                                                                                                            <br>
            This flag enables an optimization technique which only propagates error values to               <br>
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them        <br>
            at divergent differentiation paths within the computation graph.                                <br>
            If the flag is set to true                                                                      <br>
            then error values will accumulate at such junction nodes.                                       <br>
            This technique however uses more memory but will                                                <br>
            improve performance for some networks substantially.                                            <br>
            The technique is termed JIT-Propagation.                                                        <br>
                                                                                                            <br>
                                                                                                            <br>
            <b> is-Applying-Gradient-When-Tensor-Is-Used : </b>                                             <br>
                                                                                                            <br>
            Gradients will automatically be applied (or JITed) to tensors as soon as                        <br>
            they are being used for calculation (GraphNode instantiation).                                  <br>
            This feature works well with JIT-Propagation.                                                   <br>
                                                                                                            <br>
                                                                                                            <br>      
            <b> is-Applying-Gradient-When-Requested : </b>                                                  <br>
                                                                                                            <br>
            Gradients will only be applied if requested.                                                    <br>
            Usually this happens immediately, however                                                       <br>
            if the flag 'applyGradientWhenTensorIsUsed' is set                                              <br>
            to true, then the tensor will only be updated by its                                            <br>
            gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation).         <br>
                                                                                                            <br>
            <b> Let's take a look :  </b>
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>We configure Neureka autograd:</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd
Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse
Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1).setRqsGradient(true)
def y = x + 2
Binding binding = new Binding()
binding.setVariable('x', x)
binding.setVariable('y', y)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The code snippet is being execute...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr z = new GroovyShell(binding).evaluate((code))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)
def xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterBack )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterUse )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x.setGradientApplyRequested( true )
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterRqd )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterAll )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Examples:</div>
</td>
<td>
<div class='spec-examples'>
<table class='ex-table'>
<thead>
<tr>
<th class='ex-header'>code</th>
<th class='ex-header'>whenRsd</th>
<th class='ex-header'>whenUse</th>
<th class='ex-header'>doJIT</th>
<th class='ex-header'>afterBack</th>
<th class='ex-header'>afterUse</th>
<th class='ex-header'>afterRqd</th>
<th class='ex-header'>afterAll</th>
</tr>
</thead>
<tbody>
<tr class='ex-pass'>
<td class='ex-value'>y*y*3</td>
<td class='ex-value'>false</td>
<td class='ex-value'>false</td>
<td class='ex-value'>true</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-result'>OK</td>
</tr>
</tbody>
</table>
</div>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='-1152714464'>
<span>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: true, whenUse: false, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*5\.5.*null.*, afterAll: .*5\.5.*null.*, #5]</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b>                 <br>
            This run covers the feature having the following settings :                                     <br>
                                                                                                            <br>
            <b> Neureka.instance().settings().autograd().: </b>                                             <br>
            isRetainingPendingErrorForJITProp := true                                                   <br>      
            isApplyingGradientWhenTensorIsUsed := false                                                <br>    
            isApplyingGradientWhenRequested := true                                                   <br>  
                                                                                                            <br>
            ...code producing the result : 'y*y*3'                                                        <br>                                                                                                        
                                                                                                            <br>                                                                                                       
            <b> is-Retaining-Pending-Error-For-JITProp : </b>                                               <br>
                                                                                                            <br>
            This flag enables an optimization technique which only propagates error values to               <br>
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them        <br>
            at divergent differentiation paths within the computation graph.                                <br>
            If the flag is set to true                                                                      <br>
            then error values will accumulate at such junction nodes.                                       <br>
            This technique however uses more memory but will                                                <br>
            improve performance for some networks substantially.                                            <br>
            The technique is termed JIT-Propagation.                                                        <br>
                                                                                                            <br>
                                                                                                            <br>
            <b> is-Applying-Gradient-When-Tensor-Is-Used : </b>                                             <br>
                                                                                                            <br>
            Gradients will automatically be applied (or JITed) to tensors as soon as                        <br>
            they are being used for calculation (GraphNode instantiation).                                  <br>
            This feature works well with JIT-Propagation.                                                   <br>
                                                                                                            <br>
                                                                                                            <br>      
            <b> is-Applying-Gradient-When-Requested : </b>                                                  <br>
                                                                                                            <br>
            Gradients will only be applied if requested.                                                    <br>
            Usually this happens immediately, however                                                       <br>
            if the flag 'applyGradientWhenTensorIsUsed' is set                                              <br>
            to true, then the tensor will only be updated by its                                            <br>
            gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation).         <br>
                                                                                                            <br>
            <b> Let's take a look :  </b>
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>We configure Neureka autograd:</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd
Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse
Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1).setRqsGradient(true)
def y = x + 2
Binding binding = new Binding()
binding.setVariable('x', x)
binding.setVariable('y', y)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The code snippet is being execute...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr z = new GroovyShell(binding).evaluate((code))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)
def xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterBack )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterUse )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x.setGradientApplyRequested( true )
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterRqd )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterAll )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Examples:</div>
</td>
<td>
<div class='spec-examples'>
<table class='ex-table'>
<thead>
<tr>
<th class='ex-header'>code</th>
<th class='ex-header'>whenRsd</th>
<th class='ex-header'>whenUse</th>
<th class='ex-header'>doJIT</th>
<th class='ex-header'>afterBack</th>
<th class='ex-header'>afterUse</th>
<th class='ex-header'>afterRqd</th>
<th class='ex-header'>afterAll</th>
</tr>
</thead>
<tbody>
<tr class='ex-pass'>
<td class='ex-value'>y*y*3</td>
<td class='ex-value'>true</td>
<td class='ex-value'>false</td>
<td class='ex-value'>true</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-result'>OK</td>
</tr>
</tbody>
</table>
</div>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='865054400'>
<span>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: false, whenUse: true, doJIT: true, afterBack: .*1.*null.*, afterUse: .*5\.5.*null.*, afterRqd: .*5\.5.*null.*, afterAll: .*5\.5.*null.*, #6]</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b>                 <br>
            This run covers the feature having the following settings :                                     <br>
                                                                                                            <br>
            <b> Neureka.instance().settings().autograd().: </b>                                             <br>
            isRetainingPendingErrorForJITProp := true                                                   <br>      
            isApplyingGradientWhenTensorIsUsed := true                                                <br>    
            isApplyingGradientWhenRequested := false                                                   <br>  
                                                                                                            <br>
            ...code producing the result : 'y*y*3'                                                        <br>                                                                                                        
                                                                                                            <br>                                                                                                       
            <b> is-Retaining-Pending-Error-For-JITProp : </b>                                               <br>
                                                                                                            <br>
            This flag enables an optimization technique which only propagates error values to               <br>
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them        <br>
            at divergent differentiation paths within the computation graph.                                <br>
            If the flag is set to true                                                                      <br>
            then error values will accumulate at such junction nodes.                                       <br>
            This technique however uses more memory but will                                                <br>
            improve performance for some networks substantially.                                            <br>
            The technique is termed JIT-Propagation.                                                        <br>
                                                                                                            <br>
                                                                                                            <br>
            <b> is-Applying-Gradient-When-Tensor-Is-Used : </b>                                             <br>
                                                                                                            <br>
            Gradients will automatically be applied (or JITed) to tensors as soon as                        <br>
            they are being used for calculation (GraphNode instantiation).                                  <br>
            This feature works well with JIT-Propagation.                                                   <br>
                                                                                                            <br>
                                                                                                            <br>      
            <b> is-Applying-Gradient-When-Requested : </b>                                                  <br>
                                                                                                            <br>
            Gradients will only be applied if requested.                                                    <br>
            Usually this happens immediately, however                                                       <br>
            if the flag 'applyGradientWhenTensorIsUsed' is set                                              <br>
            to true, then the tensor will only be updated by its                                            <br>
            gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation).         <br>
                                                                                                            <br>
            <b> Let's take a look :  </b>
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>We configure Neureka autograd:</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd
Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse
Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1).setRqsGradient(true)
def y = x + 2
Binding binding = new Binding()
binding.setVariable('x', x)
binding.setVariable('y', y)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The code snippet is being execute...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr z = new GroovyShell(binding).evaluate((code))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)
def xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterBack )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterUse )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x.setGradientApplyRequested( true )
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterRqd )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterAll )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Examples:</div>
</td>
<td>
<div class='spec-examples'>
<table class='ex-table'>
<thead>
<tr>
<th class='ex-header'>code</th>
<th class='ex-header'>whenRsd</th>
<th class='ex-header'>whenUse</th>
<th class='ex-header'>doJIT</th>
<th class='ex-header'>afterBack</th>
<th class='ex-header'>afterUse</th>
<th class='ex-header'>afterRqd</th>
<th class='ex-header'>afterAll</th>
</tr>
</thead>
<tbody>
<tr class='ex-pass'>
<td class='ex-value'>y*y*3</td>
<td class='ex-value'>false</td>
<td class='ex-value'>true</td>
<td class='ex-value'>true</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-result'>OK</td>
</tr>
</tbody>
</table>
</div>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='-42533674'>
<span>Advanced backpropagation on all AD-Modes  [code: y*y*3, whenRsd: true, whenUse: true, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*1.*null.*, afterAll: .*5\.5.*null.*, #7]</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b>                 <br>
            This run covers the feature having the following settings :                                     <br>
                                                                                                            <br>
            <b> Neureka.instance().settings().autograd().: </b>                                             <br>
            isRetainingPendingErrorForJITProp := true                                                   <br>      
            isApplyingGradientWhenTensorIsUsed := true                                                <br>    
            isApplyingGradientWhenRequested := true                                                   <br>  
                                                                                                            <br>
            ...code producing the result : 'y*y*3'                                                        <br>                                                                                                        
                                                                                                            <br>                                                                                                       
            <b> is-Retaining-Pending-Error-For-JITProp : </b>                                               <br>
                                                                                                            <br>
            This flag enables an optimization technique which only propagates error values to               <br>
            gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them        <br>
            at divergent differentiation paths within the computation graph.                                <br>
            If the flag is set to true                                                                      <br>
            then error values will accumulate at such junction nodes.                                       <br>
            This technique however uses more memory but will                                                <br>
            improve performance for some networks substantially.                                            <br>
            The technique is termed JIT-Propagation.                                                        <br>
                                                                                                            <br>
                                                                                                            <br>
            <b> is-Applying-Gradient-When-Tensor-Is-Used : </b>                                             <br>
                                                                                                            <br>
            Gradients will automatically be applied (or JITed) to tensors as soon as                        <br>
            they are being used for calculation (GraphNode instantiation).                                  <br>
            This feature works well with JIT-Propagation.                                                   <br>
                                                                                                            <br>
                                                                                                            <br>      
            <b> is-Applying-Gradient-When-Requested : </b>                                                  <br>
                                                                                                            <br>
            Gradients will only be applied if requested.                                                    <br>
            Usually this happens immediately, however                                                       <br>
            if the flag 'applyGradientWhenTensorIsUsed' is set                                              <br>
            to true, then the tensor will only be updated by its                                            <br>
            gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation).         <br>
                                                                                                            <br>
            <b> Let's take a look :  </b>
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>We configure Neureka autograd:</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd
Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse
Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1).setRqsGradient(true)
def y = x + 2
Binding binding = new Binding()
binding.setVariable('x', x)
binding.setVariable('y', y)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The code snippet is being execute...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr z = new GroovyShell(binding).evaluate((code))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)
def xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterBack )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterUse )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x.setGradientApplyRequested( true )
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterRqd )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x * 2
xAsStr = x.toString()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The variable "x" contains every expected String :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>xAsStr.matches( afterAll )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Examples:</div>
</td>
<td>
<div class='spec-examples'>
<table class='ex-table'>
<thead>
<tr>
<th class='ex-header'>code</th>
<th class='ex-header'>whenRsd</th>
<th class='ex-header'>whenUse</th>
<th class='ex-header'>doJIT</th>
<th class='ex-header'>afterBack</th>
<th class='ex-header'>afterUse</th>
<th class='ex-header'>afterRqd</th>
<th class='ex-header'>afterAll</th>
</tr>
</thead>
<tbody>
<tr class='ex-pass'>
<td class='ex-value'>y*y*3</td>
<td class='ex-value'>true</td>
<td class='ex-value'>true</td>
<td class='ex-value'>true</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*1.*null.*</td>
<td class='ex-value'>.*5\.5.*null.*</td>
<td class='ex-result'>OK</td>
</tr>
</tbody>
</table>
</div>
</td>
</tr>
</tbody>
</table>
<hr></hr>
<div class='footer'>Generated by <a href='https://github.com/renatoathaydes/spock-reports'>Athaydes Spock Reports</a></div>
</body>
</html>