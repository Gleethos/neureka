<!DOCTYPE html><html>
<head>
<meta http-equiv='Content-Type' content='text/html; charset=utf-8'></meta>
<style>
body {
    font-family: Helvetica, Arial, sans-serif;
    font-weight: 300;
}

h2 {
    font-weight: 400;
}

hr {
    margin-bottom: 1.5em;
}

h3 {
    font-weight: 200;
}

table {
    margin: 7px;
    -webkit-box-shadow: 0px 0px 3px 1px rgba(0,0,0,0.75);
    -moz-box-shadow: 0px 0px 3px 1px rgba(0,0,0,0.75);
    box-shadow: 0px 0px 3px 1px rgba(0,0,0,0.75);
}

.ignored {
    color: gray;
}

div.project-header {
    margin-bottom: 10px;
    font-size: large;
}

div.project-header &gt; span.project-name {

                        }

div.project-header &gt; span.project-version {
                            padding-left: 20px;
                        }

div.date-test-ran {
    font-size: small;
    font-style: italic;
}

div.spec-title {
    padding: 10px 0px 5px 0px;
}

tr.error td, td.error {
    background-color: #F89A4F !important;
}

tr.failure td, td.failure {
    color: red;
}

div.footer {
    text-align: center;
    font-size: small;
}






.back-link {
    font-size: small;
    font-weight: bold;
}


div.date-test-ran {
    font-size: small;
    font-style: italic;
}

table.features-table {
    width: 99%;
    text-align: left;
}

table.summary-table {
    width: 99%;

    font-weight: bold;
    font-size: small;
}

table.summary-table tbody {
    width: 99%;
    text-align: left;
}

table.summary-table th {
    background: lightblue;
    padding: 6px;
}

table.summary-table td {
    background: #E0E0E0;
    padding: 6px;
}

pre.title {
    font-family: inherit;
    font-size: 24px;
    line-height: 28px;
    letter-spacing: -1px;
    color: #333;
}

pre.narrative {
    font-family: inherit;
    font-size: 18px;
    font-style: italic;
    line-height: 23px;
    letter-spacing: -1px;
    color: #333;
}

.feature-description {
    font-size: large;
    background: lightblue;
    padding: 12px;
}

.feature-toc-error {
    color: #F89A4F;
}

.feature-toc-failure {
    color: #FF8080;
}

.feature-toc-ignored {
    color: lightgray;
}

.feature-toc-pass {
    color: green;
}

.feature-description.error {
    background: #F89A4F;
}

.feature-description.failure {
    background: #FF8080;
}

.feature-description.ignored {
    background: lightgray;
}

.feature-description.ignored .reason {
    color: black;
    font-style: italic;
    font-size: small;
}

div.extra-info {
    font-size: small;
}

div.spec-headers {
    margin: 4px;
    font-style: italic;
}

div.spec-header {
}

div.issues {
    margin-top: 6px;
    padding: 10px 5px 5px 5px;
    background-color: lemonchiffon;
    color: black;
    font-weight: 500;
    font-size: small;
    max-width: 50%;
}

div.pending-feature {
    background-color: dodgerblue;
    color: white;
    margin-top: 6px;
    padding: 5px;
    text-align: center;
    font-size: small;
    max-width: 120px;
}

div.problem-description {
    padding: 10px;
    background: pink;
    border-radius: 10px;
}

div.problem-header {
    font-weight: bold;
    color: red;
}

div.problem-list {

}

table.ex-table{
    width: 98%;
}

table.ex-table th {
    background: lightblue;
    padding: 5px;
}

table.ex-table td {
    background: #E0E0E0;
    padding: 2px 5px 2px 5px;
}

table td {
    min-width: 50px;
}

col.block-kind-col {
    width: 70px;
}

span.spec-header {
    font-weight: bold;
}

div.spec-text {
    /*color: green;*/
}

div.spec-status {
    font-style: italic;
}

.ignored {
    color: gray;
}

td.ex-result {
    text-align: center;
    background: white !important;
}

.ex-pass {
    color: darkgreen;
}

.ex-fail {
    color: red;
    font-weight: bold;
}

div.block-kind {
    margin: 2px;
    font-style: italic;
}

div.block-text {

}

pre.block-source {
    background-color: whitesmoke;
    padding: 10px;
}

pre.block-source.error {
    background-color: pink;
    color: red;
    font-weight: bold;
}

pre.block-source.pre-error {

}

pre.block-source.before-error {
    margin-bottom: -14px;
}

pre.block-source.after-error {
    color: gray;
    margin-top: -14px;
}

pre.block-source.post-error {
    color: gray;
}

div.footer {
    text-align: center;
    font-size: small;
}</style>
</head>
<body>
<h2>Report for it.autograd.Autograd_Explained</h2>
<hr></hr>
<div class='back-link'>
<a href='index.html'>&lt;&lt; Back</a>
</div>
<div class='summary-report'>
<h3>Summary:</h3>
<div class='date-test-ran'>Created on Sun Apr 03 14:55:26 CEST 2022 by Daglemino</div>
<table class='summary-table'>
<thead>
<tr>
<th>Executed features</th>
<th>Passed</th>
<th>Failures</th>
<th>Errors</th>
<th>Skipped</th>
<th>Success rate</th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>100.0%</td>
<td>6.171 seconds</td>
</tr>
</tbody>
</table>
</div>
<pre class='title'>Autograd - Automatic Differentiation</pre>
<pre class='narrative'>Central to all neural networks in Neureka is the autograd package.                                      &lt;br&gt;
     The autograd package provides automatic differentiation for all default operations on Tensors.          &lt;br&gt;
     Neureka is a define-by-run library, which means that your backpropagation is defined by how             &lt;br&gt;
     your code is run, and that every single iteration can be different.                                     &lt;br&gt;
                                                                                                             &lt;br&gt;
     The class neureka.Tsr is the central class of the main package.                                         &lt;br&gt;
     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           &lt;br&gt;
     When you finish the forward pass of your network                                                        &lt;br&gt;
     you can call .backward() and have all the gradients computed                                            &lt;br&gt;
     and distributed to the tensors requiring them automatically.                                            &lt;br&gt;
                                                                                                             &lt;br&gt;
     &lt;br&gt;                                                                                                    &lt;br&gt;
     The gradient for a tensor will be accumulated into a child tensor (component) which                     &lt;br&gt;
     can be accessed via the '.getGradient()' method.                                                        &lt;br&gt;
                                                                                                             &lt;br&gt;
     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  &lt;br&gt;
     computation history, and to prevent future computation from being tracked.                              &lt;br&gt;
     &lt;br&gt;</pre>
<div class='spec-headers'>
<div class='spec-header'>
            Thereâ€™s one more class which is very important for autograd implementation : the 'GraphNode class'!     <br>
            Tsr and GraphNode instances are interconnected and build up an acyclic graph,                           <br>       
            that encodes a complete history of computation.                                                         <br>                   
            Each tensor has a .getGraphNode() attribute that references the GraphNode                               <br>             
            that has created a given Tsr instance.                                                                  <br>
            (except for Tsr created by the user or created by a "detached" Function instance... ).                  <br>
            <br>
        </div>
<div class='spec-header'>
            If you want to compute the derivatives, you can call .backward() on a Tensor.                           <br>
            If the given Tsr is a scalar (i.e. it holds one element and has shape "(1)"), you do not need to        <br>
            specify any arguments to backward(), however if it has more elements,                                   <br>
            you should specify a gradient argument that is a tensor of matching shape.                              <br>
        </div>
</div>
<h3>Features:</h3>
<table class='features-table'>
<colgroup>
<col class='block-kind-col'></col>
<col class='block-text-col'></col>
</colgroup>
<tbody>
<ul id='toc'>
<li>
<a href='#-795350263' class='feature-toc-pass'>Simple automatic differentiation and propagation.</a>
</li>
</ul>
<tr>
<td colspan='10'>
<div class='feature-description' id='-795350263'>
<span>Simple automatic differentiation and propagation.</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            How can I compute gradients with Neureka automatically?
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>
                The following flag states enable regular auto-grad (should also be the default):
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.get().settings().autograd().setIsApplyingGradientWhenRequested(false)
Neureka.get().settings().autograd().setIsApplyingGradientWhenTensorIsUsed(false)
Neureka.get().settings().autograd().setIsRetainingPendingErrorForJITProp(false)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>We create a simple tensor and set rqsGradient to true in order to track dependent computation.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def x = Tsr.of([2, 2], 1d).setRqsGradient(true)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Expect:</div>
</td>
<td>
<div class='block-text'>The tensor should look as follows : </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>x.toString().contains("(2x2):[1.0, 1.0, 1.0, 1.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The following "+" operation is being applied...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def y = x + 2</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The new tensor now contains four threes.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.toString().contains("(2x2):[3.0, 3.0, 3.0, 3.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>Because "y" was created as a result of a default operation, it now has a graph node as component.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.has( GraphNode.class )</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We do more computations on "y" ...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def z = y * y * 3</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>Any new tensor is created...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def someTensor = Tsr.newInstance()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The autograd flag will always default to "false" :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>someTensor.rqsGradient() == false</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We take a look at said property of the previously created "result" variable...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def resultRequiresGradient = z.rqsGradient()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>
                We will notice that "result" does NOT require gradients!
                Although one of it's "ancestors" does require gradients (namely: "x"),
                this variable itself will not hold any gradients except for when it
                propagates them ...
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>resultRequiresGradient == false</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(Tsr.of(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>
                The tensor which requires gradients, namely "x" now has the expected gradients :
        </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>x.toString().contains("(2x2):[1.0, 1.0, 1.0, 1.0]:g:[4.5, 4.5, 4.5, 4.5]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We now try to access the gradient...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def gradient = x.getGradient()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This given gradient is as expected !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>gradient.toString() == "(2x2):[4.5, 4.5, 4.5, 4.5]"</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has( GraphNode.class )</pre>
</td>
</tr>
</tbody>
</table>
<hr></hr>
<div class='footer'>Generated by <a href='https://github.com/renatoathaydes/spock-reports'>Athaydes Spock Reports</a></div>
</body>
</html>