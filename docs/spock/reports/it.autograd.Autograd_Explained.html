<!DOCTYPE html><html>
<head>
<meta http-equiv='Content-Type' content='text/html; charset=utf-8'></meta>
<style>body {
  font-family: Helvetica, Arial, sans-serif;
  font-weight: 300;
}

h2 {
  font-weight: 400;
}

h3 {
  font-weight: 200;
}

.back-link {
  font-size: small;
}

table {
  margin: 5px;
}

div.date-test-ran {
  font-size: small;
  font-style: italic;
}

table.features-table {
  width: 800px;
}

table.summary-table {
  width: 800px;
  text-align: left;
  font-weight: bold;
  font-size: small;
}

table.summary-table th {
  background: lightblue;
  padding: 6px;
}

table.summary-table td {
  background: #E0E0E0;
  padding: 6px;
}

pre.title {
  font-family: inherit;
  font-size: 24px;
  line-height: 28px;
  letter-spacing: -1px;
  color: #333;
}

pre.narrative {
  font-family: inherit;
  font-size: 18px;
  font-style: italic;
  line-height: 23px;
  letter-spacing: -1px;
  color: #333;
}

.feature-description {
  font-size: large;
  background: lightblue;
  padding: 12px;
}

.feature-toc-error {
  color: #F89A4F;
}

.feature-toc-failure {
  color: #FF8080;
}

.feature-toc-ignored {
  color: lightgray;
}

.feature-toc-pass {
  color: green;
}

.feature-description.error {
  background: #F89A4F;
}

.feature-description.failure {
  background: #FF8080;
}

.feature-description.ignored {
  background: lightgray;
}

.feature-description.ignored .reason {
  color: black;
  font-style: italic;
  font-size: small;
}

div.extra-info {
  font-size: small;
}

div.spec-headers {
  margin: 4px;
  font-style: italic;
}

div.spec-header {
}

div.issues {
  margin-top: 6px;
  padding: 10px 5px 5px 5px;
  background-color: lemonchiffon;
  color: black;
  font-weight: 500;
  font-size: small;
  max-width: 50%;
}

div.pending-feature {
  background-color: dodgerblue;
  color: white;
  margin-top: 6px;
  padding: 5px;
  text-align: center;
  font-size: small;
  max-width: 120px;
}

div.problem-description {
  padding: 10px;
  background: pink;
  border-radius: 10px;
}

div.problem-header {
  font-weight: bold;
  color: red;
}

div.problem-list {

}

table.ex-table th {
  background: lightblue;
  padding: 5px;
}

table.ex-table td {
  background: #E0E0E0;
  padding: 2px 5px 2px 5px;
}

table td {
  min-width: 50px;
}

col.block-kind-col {
  width: 70px;
}

span.spec-header {
  font-weight: bold;
}

div.spec-text {
  /*color: green;*/
}

div.spec-status {
  font-style: italic;
}

.ignored {
  color: gray;
}

td.ex-result {
  text-align: center;
  background: white !important;
}

.ex-pass {
  color: darkgreen;
}

.ex-fail {
  color: red;
  font-weight: bold;
}

div.block-kind {
  margin: 2px;
  font-style: italic;
}

div.block-text {

}

pre.block-source {
  background-color: whitesmoke;
  padding: 10px;
}

pre.block-source.error {
  background-color: pink;
  color: red;
  font-weight: bold;
}

pre.block-source.pre-error {

}

pre.block-source.before-error {
  margin-bottom: -14px;
}

pre.block-source.after-error {
  color: gray;
  margin-top: -14px;
}

pre.block-source.post-error {
  color: gray;
}

div.footer {
  text-align: center;
  font-size: small;
}
</style>
</head>
<body>
<h2>Report for it.autograd.Autograd_Explained</h2>
<hr></hr>
<div class='back-link'>
<a href='index.html'>&lt;&lt; Back</a>
</div>
<div class='summary-report'>
<h3>Summary:</h3>
<div class='date-test-ran'>Created on Sat Dec 19 19:58:06 CET 2020 by Daglemino</div>
<table class='summary-table'>
<thead>
<th>Executed features</th>
<th>Failures</th>
<th>Errors</th>
<th>Skipped</th>
<th>Success rate</th>
<th>Time</th>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>100.0%</td>
<td>8.890 seconds</td>
</tr>
</tbody>
</table>
</div>
<div class='spec-headers'>
<div class='spec-header'>
            <b> Autograd - Automatic Differentiation </b>                                                           <br>
            <br>
            Central to all neural networks in Neureka is the autograd package.                                      <br>
            The autograd package provides automatic differentiation for all default operations on Tensors.          <br>
            Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>
            your code is run, and that every single iteration can be different.                                     <br>
            <br>
            The class neureka.Tsr is the central class of the main package.                                         <br>
            If you set its attribute rqsGradient to True, Neureka starts to track all operations on it.             <br>
            When you finish the forward pass of your network                                                        <br>
            you can call .backward() and have all the gradients computed                                            <br>
            and distributed to the tensors requiring them automatically.                                            <br>
            <br>
            <b> Tensors and Gradients </b>                                                                          <br>
            <br>                                                                                                        <br>
            The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>
            can be accessed via the .getGradient() method.                                                          <br>
            <br>
            To stop a tensor from tracking history, you can call .detach() to detach it from the                    <br>
            computation history, and to prevent future computation from being tracked.                              <br>
            <br>                                         
        </div>
<div class='spec-header'>
            Thereâ€™s one more class which is very important for autograd implementation : the GraphNode.             <br>
            Tsr and GraphNode instances are interconnected and build up an acyclic graph,                           <br>       
            that encodes a complete history of computation.                                                         <br>                   
            Each tensor has a .getGraphNode() attribute that references the GraphNode                               <br>             
            that has created a given Tsr instance.                                                                  <br>
            (except for Tsr created by the user or created by a "detached" Function instance... ).                  <br>
            <br>
        </div>
<div class='spec-header'>
            If you want to compute the derivatives, you can call .backward() on a Tensor.                           <br>
            If the given Tsr is a scalar (i.e. it holds one element and has shape "(1)"), you do not need to        <br>
            specify any arguments to backward(), however if it has more elements,                                   <br>
            you should specify a gradient argument that is a tensor of matching shape.                              <br>
        </div>
</div>
<h3>Features:</h3>
<table class='features-table'>
<colgroup>
<col class='block-kind-col'></col>
<col class='block-text-col'></col>
</colgroup>
<tbody>
<ul id='toc'>
<li>
<a href='#-795350263' class='feature-toc-pass'>Simple automatic differentiation and propagation.</a>
</li>
</ul>
<tr>
<td colspan='10'>
<div class='feature-description' id='-795350263'>
<span>Simple automatic differentiation and propagation.</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
<div class='extra-info'>
<ul>
<li>
<div>
            How can I compute gradients with Neureka automatically?
        </div>
</li>
</ul>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>
                Because (by default) Neureka is not too eager when it comes to backpropagation
                we have to set the following flags :
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.instance().settings().autograd().setIsApplyingGradientWhenRequested(false)
Neureka.instance().settings().autograd().setIsApplyingGradientWhenTensorIsUsed(false)
Neureka.instance().settings().autograd().setIsRetainingPendingErrorForJITProp(false)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>We create a simple tensor and set rqsGradient to true in order to track dependent computation.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def x = new Tsr([2, 2], 1).setRqsGradient(true)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Expect:</div>
</td>
<td>
<div class='block-text'>The tensor should look as follows : </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>x.toString().contains("(2x2):[1.0, 1.0, 1.0, 1.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>The following "+" operation is being applied...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def y = x + 2</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The new tensor now contains four threes.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.toString().contains("(2x2):[3.0, 3.0, 3.0, 3.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>Because "y" was created as a result of a default operation, it now has a graph node as component.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.has(GraphNode.class)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We do more computations on "y" ...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def z = y * y * 3</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>As expected, this new tensor contains four times 27 :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.toString().contains("(2x2):[27.0, 27.0, 27.0, 27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We call the "mean()" method as a simple loss function!
                This produces a scalar output tensor which is ideal as entrypoint
                for the autograd algorithm.
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def result = z.mean()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This "result" tensor will be the expected scalar :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.toString().contains("(1x1):[27.0]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>Any new tensor is created...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def someTensor = new Tsr()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The autograd flag will always default to "false" :</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>someTensor.rqsGradient() == false</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We take a look at said property of the previously created "result" variable...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def resultRequiresGradient = z.rqsGradient()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>
                We will notice that "result" does NOT require gradients!
                Although one of it's "ancestors" does require gradients (namely: "x"),
                this variable itself will not hold any gradients except for when it
                propagates them ...
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>resultRequiresGradient == false</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>
                We now try to backpropagate! Because "result" contains a single scalar,
                result.backward() is equivalent to out.backward(new Tsr(1)).
            </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>z.backward(0.25)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>
                The tensor which requires gradients, namely "x" now has the expected gradients :
        </div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>x.toString().contains("(2x2):[1.0, 1.0, 1.0, 1.0]:g:[4.5, 4.5, 4.5, 4.5]")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We now try to access the gradient...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def gradient = x.getGradient()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This given gradient is as expected !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>gradient.toString() == "(2x2):[4.5, 4.5, 4.5, 4.5]"</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>It is time to free some memory because our history of computation has grown a bit...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>result.detach()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>!result.has(GraphNode.class)</pre>
</td>
</tr>
</tbody>
</table>
<hr></hr>
<div class='footer'>Generated by <a href='https://github.com/renatoathaydes/spock-reports'>Athaydes Spock Reports</a></div>
</body>
</html>