{
  "project": "Neureka",
  "version": "0.17.0",
  "created": "Mon Sep 05 15:18:29 CEST 2022",
  "statistics":{
    "runs":"86",
    "passed":"85",
    "failed":"0",
    "featureFailures":"0",
    "successRate":"1.0",
    "duration":"47371.0"
  },
  "specifications": [{
      "className":"Example_Spec.Example_Spec",
      "title":"An Introduction to writing Spock Specifications",
      "narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n\n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way.",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"56",
      "executedFeatures":[{"id":"Call me feature not unit test!","extraInfo":[]},{"id":"I am readable and also best practice!","extraInfo":[]},{"id":"Numbers to the power of two with a fancy data table!","extraInfo":[]},{"id":"Should be able to remove from list","extraInfo":[]},{"id":"iAmNotSoReadable","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"it.Calculus_Stress_Test",
      "title":"",
      "narrative":"",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"657",
      "executedFeatures":[{"id":"Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","extraInfo":[]},{"id":"Activation functions work across types.","extraInfo":[]},{"id":"Dot operation stress test runs error free and produces expected result","extraInfo":[]},{"id":"Stress test runs error free and produces expected result","extraInfo":[]},{"id":"The broadcast operation stress test runs error free and produces expected result","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"it.Cross_Device_Sliced_Tensor_System_Test",
      "title":"Cross Device Tensor Slicing",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"191",
      "executedFeatures":[{"id":"Cross device sliced tensor integration test runs without errors.","extraInfo":[]},{"id":"Slices can be created using the SliceBuilder.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"it.Cross_Device_Spec",
      "title":"Cross Device Stress Test Specification",
      "narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same...",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"15995",
      "executedFeatures":[{"id":"A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","extraInfo":[]},{"id":"Convolution can model matrix multiplications across devices.","extraInfo":[]},{"id":"Cross device system test runs successfully.","extraInfo":[]},{"id":"Mapping tensors works for every device (even if they are not used).","extraInfo":[]},{"id":"Test simple NN implementation with manual backprop","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"it.Eleven_Lines_NN_System_Spec",
      "title":"NN Code Golfing!",
      "narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    ´´´\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    ´´´",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"209",
      "executedFeatures":[{"id":"One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","extraInfo":[]},{"id":"One can write a simple float based neural network in less than 11 lines of java like code!","extraInfo":[]},{"id":"One can write a simple neural network in less than 11 lines of code!","extraInfo":[]},{"id":"One can write a simple neural network with custom back-prop in 11 lines of code!","extraInfo":[]},{"id":"The pseudo random number generator works as expected for the weights used in the 11 line NN examples!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"st.Benchmark_System_Test",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"1964",
      "executedFeatures":[{"id":"Tensor can be constructed by passing List instances.","extraInfo":[]},{"id":"Test benchmark script and simple tensor constructor.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"st.Broad_System_Test",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"35",
      "executedFeatures":[{"id":"The long broad integration test runs successfully.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.AD_And_Computation_Graph_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"172",
      "executedFeatures":[{"id":"Payloads and derivatives are null after garbage collection.","extraInfo":[]},{"id":"Reshaping produces expected computation graph and also works with reverse mode AD.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.Autograd_Explained",
      "title":"Autograd - Automatic Differentiation",
      "narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n\n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n\n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n\n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked.",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"10",
      "executedFeatures":[{"id":"Simple automatic differentiation and propagation.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.Autograd_Flags_Explained",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"105",
      "executedFeatures":[{"id":"Advanced backpropagation on all AD-Modes ","extraInfo":[]},{"id":"We can create a shallow copy of a tensor detached from the computation graph.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.Autograd_NN_Spec",
      "title":"Simple Neural Network autograd integration test",
      "narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n\n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"147",
      "executedFeatures":[{"id":"Autograd work for simple matrix multiplications.","extraInfo":[]},{"id":"Autograd works for 2 matrix multiplications in a row.","extraInfo":[]},{"id":"Autograd works in a simple convolutional dot product and float based feed forward neural network.","extraInfo":[]},{"id":"Autograd works in a simple convolutional dot product based feed forward neural network.","extraInfo":[]},{"id":"Autograd works in a simple mat-mul based feed forward neural network.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.Autograd_Tensor_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"17",
      "executedFeatures":[{"id":"A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","extraInfo":[]},{"id":"Second-Test \"x-mul\" autograd behaviour. (Not on device)","extraInfo":[]},{"id":"Test basic autograd behaviour. (Not on device)","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests",
      "title":"",
      "narrative":"",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"201",
      "executedFeatures":[{"id":"GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","extraInfo":[]},{"id":"GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","extraInfo":[]},{"id":"GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","extraInfo":[]},{"id":"GraphNode throws an exception when trying to instantiate it with the wrong context.","extraInfo":[]},{"id":"GraphNode throws exception when payload is null.","extraInfo":[]},{"id":"GraphNode throws exception when trying to instantiate it with the Function argument being null.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"17",
      "executedFeatures":[{"id":"GraphNode instantiation works as expected when the context argument is a GraphLock.","extraInfo":[]},{"id":"GraphNode instantiation works as expected when the context argument is an ExecutionCall.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"5",
      "executedFeatures":[{"id":"A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.JITProp_Autograd_Tensor_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"8",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"155",
      "executedFeatures":[{"id":"Gradient auto-apply kicks in when used AD uses JIT prop","extraInfo":[]},{"id":"Test JIT propagation variant one.","extraInfo":[]},{"id":"Test JIT propagation variant two.","extraInfo":[]},{"id":"Test autograd without JIT and auto apply.","extraInfo":[]},{"id":"Test in-differential and JIT with auto apply","extraInfo":[]},{"id":"Test no JIT prop when forward AD","extraInfo":[]},{"id":"Test no preemptive gradient apply when not requested and auto apply and JIT_prop","extraInfo":[]},{"id":"Test pending error optimization","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.Backend_Extension_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"73",
      "executedFeatures":[{"id":"Lambda properties of mock implementation interact with FunctionNode as expected.","extraInfo":[]},{"id":"Mock operation interacts with FunctionNode (AbstractFunction) instance as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.Backend_MatMul_Extension_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"74",
      "executedFeatures":[{"id":"GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","extraInfo":[]},{"id":"Test context mock for opencl reference implementations.","extraInfo":[]},{"id":"Tile parsing for kernel parameter calculation yields expected tile dimensions.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Backend_Algorithm_AD_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"125",
      "executedFeatures":[{"id":"Activation implementations behave as expected.","extraInfo":[]},{"id":"Broadcast implementations behave as expected.","extraInfo":[]},{"id":"Convolution implementations behave as expected.","extraInfo":[]},{"id":"Operator implementations behave as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Backend_Algorithm_Implementation_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"158",
      "executedFeatures":[{"id":"Activation implementations have expected Executor instances.","extraInfo":[]},{"id":"CLExecutors of Operator implementations behave as expected.","extraInfo":[]},{"id":"HostExecutors of Operator implementations behave as expected.","extraInfo":[]},{"id":"Operator implementations have expected Executor instances.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Backend_Functional_Algorithm_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"21",
      "executedFeatures":[{"id":"A functional algorithm cannot be used if it was not built properly!","extraInfo":[]},{"id":"A functional algorithm does not accept null as an answer!","extraInfo":[]},{"id":"A functional algorithm warns us when modified after it has been built!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Matrix_Multiplication_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"1367",
      "executedFeatures":[{"id":"The CPU matrix multiplication implementation works as expected.","extraInfo":[]},{"id":"The internal matrix multiplication test script runs!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Randomization_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"26",
      "executedFeatures":[{"id":"Randomization is in essence the same algorithm as JDKs \"Random\".","extraInfo":[]},{"id":"The Randomization class can fill various types of arrays with pseudo random numbers.","extraInfo":[]},{"id":"We can make slices of tensors random.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.Matrix_Multiplication_Spec",
      "title":"Matrix Multiplication",
      "narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"19",
      "executedFeatures":[{"id":"The \"matMul\" method allows us to perform matrix multiplication.","extraInfo":[]},{"id":"The simple CPU matrix multiplication implementation works as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.BackendContext_Spec",
      "title":"The BackendContext is a cloneable context which can run Tasks.",
      "narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"15",
      "executedFeatures":[{"id":"BackendContext instances can be created by cloning from Singleton instance.","extraInfo":[]},{"id":"BackendContext instances return Runner instances for easy visiting with return values.","extraInfo":[]},{"id":"BackendContext instances return Runner instances for easy visiting.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Calculus_Exception_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"4",
      "executedFeatures":[{"id":"Function throws exception when arity does not match input number.","extraInfo":[]},{"id":"Function throws exception when not enough inputs provided.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Calculus_Function_Spec",
      "title":"Testing Default Methods on Functions",
      "narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"44",
      "executedFeatures":[{"id":"Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","extraInfo":[]},{"id":"Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","extraInfo":[]},{"id":"Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","extraInfo":[]},{"id":"The library context exposes a set of useful functions.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Calculus_Parsing_Spec",
      "title":"Parsing Expressions into Functions",
      "narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"24",
      "executedFeatures":[{"id":"Functions can derive themselves according to the provided index of the input which ought to be derived.","extraInfo":[]},{"id":"Parsed equations throw expected error messages.","extraInfo":[]},{"id":"Test parsed equations when building Function instances.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Calculus_Scalar_Spec",
      "title":"Functions for Scalars",
      "narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values.",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"25",
      "executedFeatures":[{"id":"Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","extraInfo":[]},{"id":"Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","extraInfo":[]},{"id":"Function \"1/I[0]\" instance returns expected scalar results.","extraInfo":[]},{"id":"Function \"I[0]+1/I[0]\" instance returns expected scalar results.","extraInfo":[]},{"id":"Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","extraInfo":[]},{"id":"Test scalar results of various Function instances.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.ConCat_Spec",
      "title":"Merging Tensors",
      "narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors.",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"27",
      "executedFeatures":[{"id":"We can concatenate 2 float tensors alongside a specified axis!","extraInfo":[]},{"id":"We can concatenate 2 string tensors alongside a specified axis!","extraInfo":[]},{"id":"We can concatenate 2 tensors alongside a specified axis!","extraInfo":[]},{"id":"We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","extraInfo":[]},{"id":"We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Tensor_Function_Spec",
      "title":"Applying Functions to Tensors",
      "narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you.",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"90",
      "executedFeatures":[{"id":"Executed tensors are intermediate tensors.","extraInfo":[]},{"id":"Reshaping on 3D tensors works by instantiate a Function instance built from a String.","extraInfo":[]},{"id":"Tensor results of various Function instances return expected results.","extraInfo":[]},{"id":"The \"DimTrim\" operation works forward as well as backward!","extraInfo":[]},{"id":"The optimization function for the SGD algorithm produces the expected result","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.CLFunctionCompiler_Spec",
      "title":"OpenCLDevice Function Optimization Integration Tests",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"126",
      "executedFeatures":[{"id":"The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler).","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.CPU_Spec",
      "title":"The CPU device, an API for CPU based execution",
      "narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"360",
      "executedFeatures":[{"id":"CPU knows the current number of available processor cores!","extraInfo":[]},{"id":"The CPU exposes a non null API for executing workloads in parallel.","extraInfo":[]},{"id":"Thread pool executes given workload in parallel","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.Cross_Device_IO_Spec",
      "title":"Devices manage the states of the tensors they store!",
      "narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"23",
      "executedFeatures":[{"id":"We can use the access device API to read from a tensor.","extraInfo":[]},{"id":"We can use the access device API to write to a tensor","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.Cross_Device_Type_Spec",
      "title":"Cross Device-Type Unit Tests",
      "narrative":"",
      "featureCount":"10",
      "failures":"0",
      "errors":"0",
      "skipped":"1" ,
      "successRate":"1.0",
      "duration":"83",
      "executedFeatures":[{"id":"Advanced device querying methods query as expected!","extraInfo":[]},{"id":"Devices store slices which can also be restored.","extraInfo":[]},{"id":"Devices store tensors which can also be restored.","extraInfo":[]},{"id":"Execution calls containing null arguments will cause an exception to be thrown in device instances.","extraInfo":[]},{"id":"Passing a numeric array to a tensor should modify its content!","extraInfo":[]},{"id":"Querying for Device implementations works as expected.","extraInfo":[]},{"id":"Tensor data can be fetched from device if the tensor is stored on it...","extraInfo":[]},{"id":"The simpler device querying methods query as expected!","extraInfo":[]},{"id":"Virtual tensors stay virtual when outsourced.","extraInfo":[]}],
      "ignoredFeatures":[{"id":"Devices cannot store slices which parents are not already stored.","extraInfo":[]}]
    },{
      "className":"ut.device.FileDevice_Spec",
      "title":"FileDevice, Storing Tensors in Files",
      "narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...).",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"84",
      "executedFeatures":[{"id":"A file device stores tensors in idx files by default.","extraInfo":[]},{"id":"A file device stores tensors in various file formats.","extraInfo":[]},{"id":"The file device can load known files in a directory.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.internal.CLFunctionCompiler_Spec",
      "title":"Turning functions into kernels.",
      "narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"14",
      "executedFeatures":[{"id":"The CLFunctionCompiler produces an operation which properly integrates to the backend.","extraInfo":[]},{"id":"The CLFunctionCompiler produces the expected \"ad hoc\" kernel.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.internal.CPU_Kernel_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"46",
      "executedFeatures":[{"id":"The Reduce implementation for the CPU has realistic behaviour","extraInfo":[]},{"id":"The Sum implementation for the CPU has realistic behaviour","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.internal.OpenCL_Data_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"28",
      "executedFeatures":[{"id":"The \"Data\" class can represent various OpenCL data types.","extraInfo":[]},{"id":"The OpenCLDevice specific Data class represents JVM data for OpenCL.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.internal.OpenCL_Kernel_Unit_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"16",
      "executedFeatures":[{"id":"The GEMM implementation for the OpenCLDevice has realistic behaviour","extraInfo":[]},{"id":"The Reduce implementation for the OpenCLDevice has realistic behaviour","extraInfo":[]},{"id":"The Sum implementation for the OpenCLDevice has realistic behaviour","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.OpenCLDevice_Exception_Spec",
      "title":"OpenCLDevice Exception Handling",
      "narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"215",
      "executedFeatures":[{"id":"Ad hoc compilation produces expected exceptions when duplication is found.","extraInfo":[]},{"id":"Ad hoc compilation produces expected exceptions.","extraInfo":[]},{"id":"An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","extraInfo":[]},{"id":"Trying to restore a tensor which is not on a device raises exception.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.OpenCLDevice_Spec",
      "title":"The OpenCLDevice Specification",
      "narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly...",
      "featureCount":"8",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"4230",
      "executedFeatures":[{"id":"Ad hoc compilation produces executable kernel.","extraInfo":[]},{"id":"Ad hoc compilation works for WIP general purpose matrix multiplication.","extraInfo":[]},{"id":"Ad hoc compilation works for custom column major based tiled matrix multiplication.","extraInfo":[]},{"id":"Ad hoc compilation works for custom simple row major based matrix multiplication.","extraInfo":[]},{"id":"Ad hoc matrix multiplication works for multiple of 16 matrices.","extraInfo":[]},{"id":"An OpenCLDevice loads tensors in a provided lambda temporarily.","extraInfo":[]},{"id":"The \"getData()\" method of an outsourced tensor will return null when outsourced.","extraInfo":[]},{"id":"The \"getValue()\" method of an outsourced tensor will return the expected array type.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.OpenCL_Spec",
      "title":"Working with OpenCL",
      "narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices...",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"2473",
      "executedFeatures":[{"id":"A given OpenCL context can be disposed!","extraInfo":[]},{"id":"An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","extraInfo":[]},{"id":"First found OpenCLDevice will have realistic numeric properties.","extraInfo":[]},{"id":"First found OpenCLDevice will have realistic properties inside summary query.","extraInfo":[]},{"id":"First found OpenCLDevice will have realistic text properties.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.dtype.DataType_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"9",
      "executedFeatures":[{"id":"DataType multi-ton instances behave as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.dtype.NumericType_Spec",
      "title":"The NumericType and its implementations model their respective numeric data types.",
      "narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"33",
      "executedFeatures":[{"id":"Conversion goes both ways and produces expected numeric values.","extraInfo":[]},{"id":"NumericType conversion to holder types yields expected results.","extraInfo":[]},{"id":"NumericType implementations behave as expected.","extraInfo":[]},{"id":"NumericType implementations return their expected properties.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.framing.Tensor_Framing_Spec",
      "title":"Naming Tensors and their Dimensions.",
      "narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"205",
      "executedFeatures":[{"id":"A tensor can be labeled partially.","extraInfo":[]},{"id":"Added labels to tensors are accessible through the \"index()\" method.","extraInfo":[]},{"id":"Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","extraInfo":[]},{"id":"Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.introductions.Tensor_NDArray_Spec",
      "title":"Tensors or Nd-arrays",
      "narrative":"*What is the difference?*\n\nIn the world of machine learning we use something called a **'tensor'** to represent data.\nThey might be called **'nd-arrays'** in some other frameworks,\nbut although they are very similar, \nthere are also some important distinctions to be made between these two concepts.\nBoth are at their core merely multidimensional arrays, however,\nthey are different in their typical usage and API.\nnd-arrays are merely used to represent any type of data as a \ncollection of elements in a multidimensional grid,  \ntensors on the other hand have additional requirements.\nThey are a type of nd-array which stores numeric data \nas well as expose various mathematical operations for said data.\nIn that sense it is actually merely a more complex kind of number.\nThis concept actually comes from the field of physics, \nwhere it is used to represent a physical quantity.\n\nNeureka models both concepts through the `Tsr` and the `Nda` interfaces.\n`Nda` is an abbreviation of `NdArray`, and `Tsr` is an abbreviation of `Tensor`.\nThe `Tsr` type is a subtype of the `Nda` type, exposing additional methods\nlike for example `plus`, `minus`, `times` and `divide`.\nBoth can be instantiated through static factory methods (and a fluent builder API).",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"4",
      "executedFeatures":[{"id":"Tensor is a subtype of NdArray.","extraInfo":[]},{"id":"We can use tensors for numeric calculations (but not nd-arrays).","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndas.NDA_Instantiation_Spec",
      "title":"ND-Array Instantiation",
      "narrative":"In this specification we cover how ND-arrays can be instantiated.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"5",
      "executedFeatures":[{"id":"A vector can be created from an array of values through the \"of\" method.","extraInfo":[]},{"id":"ND-arrays can be created fluently.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndim.NDConfiguration_Spec",
      "title":"Making Arrays N-Dimensional",
      "narrative":"Under the hood Neureka implements powerful indexing \n    abstractions through the `NDConfiguration` interface and its various implementations.\n    This allows for the creation of tensors/nd-arrays with arbitrary dimensions, \n    the ability to slice them into smaller tensors/nd-arrays with the same underlying data,\n    and finally the ability to reshape their axes (like transposing them for example).\n\n    This specification however only focuses on the behaviour of the `NDConfiguration` interface\n    which translates various types of indices.",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"46",
      "executedFeatures":[{"id":"Various NDConfigurations behaviour exactly as their general purpose implementation.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndim.Tensor_NDConfiguration_Spec",
      "title":"What it means to be N-Dimensional",
      "narrative":"This specification covers how implementations\n    of the `NDConfiguration` interface manage to define\n    what it means to be a n-dimensional tensor/nd-array.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"8",
      "executedFeatures":[{"id":"NDConfiguration instances of tensors have expected state and behaviour.","extraInfo":[]},{"id":"NDConfiguration instances of tensors have expected state.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndim.Tensor_Reshape_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"2",
      "executedFeatures":[{"id":"When matrices are transpose, they will change their layout type.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndim.Tensor_Slice_Reshape_Spec",
      "title":"Reshaping Slices of Tensors",
      "narrative":"Neureka provides a convenient way to reshape tensors\n    even if they are slices of other tensors sharing the same underlying data.\n    This is possible because of the under the hood indexing \n    abstractions provided by the `NDConfiguration` interface and its various implementations.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"6",
      "executedFeatures":[{"id":"A slice of a tensor changes as expected when reshaping it.","extraInfo":[]},{"id":"Reshaping a slice works as expected.","extraInfo":[]},{"id":"Two slices of one big tensor perform matrix multiplication flawless.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.neureka.Neureka_Spec",
      "title":"The Neureka context can be used and configured as expected.",
      "narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general.",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"5536",
      "executedFeatures":[{"id":"Backend related library objects adhere to the same toString formatting convention!","extraInfo":[]},{"id":"Every Thread instance has their own Neureka instance.","extraInfo":[]},{"id":"Neureka class instance has expected behaviour.","extraInfo":[]},{"id":"Neureka settings class can be locked causing its properties to be immutable.","extraInfo":[]},{"id":"OpenCL related library objects adhere to the same toString formatting convention!","extraInfo":[]},{"id":"Various library objects adhere to the same toString formatting convention!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.AdaGrad_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"18",
      "executedFeatures":[{"id":"AdaGrad optimizes according to expected inputs","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.ADAM_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"62",
      "executedFeatures":[{"id":"ADAM optimizes according to expected inputs","extraInfo":[]},{"id":"Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","extraInfo":[]},{"id":"Equations used by ADAM return expected result.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.Momentum_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"10",
      "executedFeatures":[{"id":"Momentum optimizes according to expected inputs","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.Optimizer_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"1" ,
      "successRate":"1.0",
      "duration":"0",
      "executedFeatures":[],
      "ignoredFeatures":[{"id":"Dot based feed forward and activation produces expected result.","extraInfo":[]}]
    },{
      "className":"ut.optimization.RMSprop_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"19",
      "executedFeatures":[{"id":"RMSprop optimizes according to expected inputs","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Copy_Spec",
      "title":"To Copy or Not to Copy",
      "narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well.",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"21",
      "executedFeatures":[{"id":"A deep copy of a slice tensor is also a deep copy of the underlying data array.","extraInfo":[]},{"id":"A deep copy of a tensor is also a deep copy of the underlying data array.","extraInfo":[]},{"id":"A shallow copy of a tensor will be flagged as such.","extraInfo":[]},{"id":"A shallow copy will share the same underlying data as its original tensor.","extraInfo":[]},{"id":"We can deep copy various types of tensors.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.exceptions.Tensor_Delete_Exception_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"7",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"4",
      "executedFeatures":[{"id":"A deleted tensor will tell you that it has been deleted.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when accessing its configuration.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when accessing its data type.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when accessing its data.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when modifying its data type.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when trying to modify its data.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when trying to set its configuration.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.exceptions.Tensor_Exception_Spec",
      "title":"Tensors Exception Behavior",
      "narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse.",
      "featureCount":"8",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"40",
      "executedFeatures":[{"id":"Building a tensor with \"null\" as shape argument throws an exception.","extraInfo":[]},{"id":"Building a tensor with 0 shape arguments throws an exception.","extraInfo":[]},{"id":"Casting a tensor as something unusual will cuas an exception to be thrown.","extraInfo":[]},{"id":"Out of dimension bound causes descriptive exception!","extraInfo":[]},{"id":"Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","extraInfo":[]},{"id":"Passing an invalid object into Tsr constructor causes descriptive exception.","extraInfo":[]},{"id":"Passing null to various methods of the tensor API will throw exceptions.","extraInfo":[]},{"id":"Trying to inject an empty tensor into another causes fitting exception.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Expression_Based_Tensor_Instantiation_Spec",
      "title":"Expression based Tensor Instantiation",
      "narrative":"This specification defines how a tensor can be instantiated\n    using string expressions, which define operations to be executed.\n    This form of tensor instantiation is very useful to avoid boilerplate code.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"5",
      "executedFeatures":[{"id":"A tensor can be created from a function as expression.","extraInfo":[]},{"id":"We can instantiate tensors from various simple string expressions.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Fluent_Tensor_Creation_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"7",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"21",
      "executedFeatures":[{"id":"Initialization lambda based tensors can be created fluently.","extraInfo":[]},{"id":"Range based tensors can be created fluently.","extraInfo":[]},{"id":"Scalars can be created fluently.","extraInfo":[]},{"id":"Seed based tensors can be created fluently.","extraInfo":[]},{"id":"Tensors can be created fluently.","extraInfo":[]},{"id":"Value based tensors can be created fluently.","extraInfo":[]},{"id":"Vectors can be created fluently.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Functional_Nda_Spec",
      "title":"Functional ND-Arrays",
      "narrative":"ND-Arrays expose a powerful API for performing operations on them\n    in a functional style.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"147",
      "executedFeatures":[{"id":"We can find both min and max items in an ND-array by providing a comparator.","extraInfo":[]},{"id":"We can initialize an ND-Array using a filler lambda mapping indices to items.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Functional_Tensor_Spec",
      "title":"Functional Tensors",
      "narrative":"Tensors expose a powerful API for performing operations on them\n    in a functional style.",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"646",
      "executedFeatures":[{"id":"Tensor mapping lambdas produce expected tensors.","extraInfo":[]},{"id":"The \"map\" method is a shorter convenience method for mapping to the same type.","extraInfo":[]},{"id":"We can analyse the values of a tensor using various predicate receiving methods","extraInfo":[]},{"id":"We can find both min and max items in a tensor by providing a comparator.","extraInfo":[]},{"id":"We can initialize a tensor using a filler lambda mapping indices to items.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_As_Container_Spec",
      "title":"Why not have a tensor of words?",
      "narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things...",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"15",
      "executedFeatures":[{"id":"More tensor operations translate to custom data type \"ComplexNumber\".","extraInfo":[]},{"id":"Plus operator on String tensors works element-wise.","extraInfo":[]},{"id":"Tensor operations translate to custom data type \"ComplexNumber\".","extraInfo":[]},{"id":"We can apply predicates on the values of a tensor.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Conversion_Spec",
      "title":"Tensor Type Conversion",
      "narrative":"Here we specify how a tensor can be converted to other data types\n    like for example another tensor of a different data type.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"14",
      "executedFeatures":[{"id":"Tensors value type can be changed by calling \"toType(...)\".","extraInfo":[]},{"id":"We can change the data type of all kinds of tensors.","extraInfo":[]},{"id":"We turn a tensor into a scalar value or string through the \"as\" operator!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Convolution_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"558",
      "executedFeatures":[{"id":"Autograd works with simple 2D convolution.","extraInfo":[]},{"id":"Manual convolution produces expected result.","extraInfo":[]},{"id":"Sime convolution works as expected eith autograd.","extraInfo":[]},{"id":"Tensors have the correct layout after convolution.","extraInfo":[]},{"id":"The \"x\" (convolution) operator produces expected results (On the CPU).","extraInfo":[]},{"id":"Very simple manual convolution produces expected result.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Device_Spec",
      "title":"Tensors on Devices",
      "narrative":"This unit test specification covers \n    the expected behavior of tensors when interacting\n    with instances of implementations of the Device interface.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"9",
      "executedFeatures":[{"id":"Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","extraInfo":[]},{"id":"Tensors try to migrate themselves to a device that is being added to them as component.","extraInfo":[]},{"id":"The device of a tensor can be accessed via the \"device()\" method.","extraInfo":[]},{"id":"When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Generics_Spec",
      "title":"Tensors as Generic Containers",
      "narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"5",
      "executedFeatures":[{"id":"1D tensors can be created from primitive arrays.","extraInfo":[]},{"id":"Anonymous tensor instance has the default datatype class as defined in Neureka settings.","extraInfo":[]},{"id":"String tensor instance discovers expected class.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Gradient_Spec",
      "title":"Gradients are Tensors which are Components of other Tensors",
      "narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to...",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"4",
      "executedFeatures":[{"id":"Gradient of tensor is being applies regardless of the tensor requiring gradient or not","extraInfo":[]},{"id":"Tensors can have gradients but not require them.","extraInfo":[]},{"id":"Tensors that have gradients but do not require them still print them.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Instantiation_Spec",
      "title":"Instantiating Tensors",
      "narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to show how a tensor can be instantiated in different ways.",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"15",
      "executedFeatures":[{"id":"Passing String seed to tensor produces expected values.","extraInfo":[]},{"id":"Scalar tensors can be created via static factory methods","extraInfo":[]},{"id":"Tensors can be instantiated based on arrays for both shapes and values.","extraInfo":[]},{"id":"Tensors can be instantiated based on lists for both shapes and values.","extraInfo":[]},{"id":"Tensors can be instantiated with String seed.","extraInfo":[]},{"id":"Vector tensors can be instantiated via factory methods.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Interop_Spec",
      "title":"Tensors play well with other data structures!",
      "narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"11",
      "executedFeatures":[{"id":"Not all tensor can be converted to images.","extraInfo":[]},{"id":"Tensor can be converted to buffered images.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_IO_Spec",
      "title":"Reading and Writing Tensor Items",
      "narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to read from and write to the state of a tensor.",
      "featureCount":"8",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"29",
      "executedFeatures":[{"id":"A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","extraInfo":[]},{"id":"Indexing after reshaping works as expected.","extraInfo":[]},{"id":"Tensor value type can not be changed by passing float or double arrays to it.","extraInfo":[]},{"id":"Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","extraInfo":[]},{"id":"The tensor data array can be modified by targeting them with an index.","extraInfo":[]},{"id":"We can manipulate the underlying data array of a tensor through the unsafe API.","extraInfo":[]},{"id":"We can re-populate a tensor of shorts from a single scalar value!","extraInfo":[]},{"id":"When we try to manipulate the underlying data array of a virtual tensor then it will become actual.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Layout_Spec",
      "title":"Row or Column Major. Why not both?",
      "narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"402",
      "executedFeatures":[{"id":"A new transposed version of a given tensor will be returned by the \"T()\" method.","extraInfo":[]},{"id":"Matrix multiplication works for both column and row major matrices across devices.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Operation_Spec",
      "title":"Running Tensors through operations",
      "narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid.",
      "featureCount":"11",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"8251",
      "executedFeatures":[{"id":"Activation functions work across types on slices and non sliced tensors.","extraInfo":[]},{"id":"Auto reshaping and broadcasting works and the result can be back propagated.","extraInfo":[]},{"id":"New method \"asFunction\" of String added at runtime is callable by groovy and also works.","extraInfo":[]},{"id":"New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","extraInfo":[]},{"id":"Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","extraInfo":[]},{"id":"Overloaded operation methods on tensors produce expected results when called.","extraInfo":[]},{"id":"Simple slice addition produces expected result.","extraInfo":[]},{"id":"The \"dot\" operation reshapes and produces valid \"x\" operation result.","extraInfo":[]},{"id":"The \"matMul\" operation produces the expected result.","extraInfo":[]},{"id":"The \"random\" function/operation populates tensors randomly.","extraInfo":[]},{"id":"The values of a randomly populated tensor seems to adhere to a gaussian distribution.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Slicing_Spec",
      "title":"Tensors within Tensors",
      "narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing.",
      "featureCount":"9",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"27",
      "executedFeatures":[{"id":"A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","extraInfo":[]},{"id":"A tensor can be sliced by passing ranges in the form of primitive arrays.","extraInfo":[]},{"id":"Normal slicing will try to do autograd.","extraInfo":[]},{"id":"Slicing is also a Function with autograd support!","extraInfo":[]},{"id":"The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","extraInfo":[]},{"id":"The slice builder also supports slicing with custom step sizes.","extraInfo":[]},{"id":"We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","extraInfo":[]},{"id":"We can slice a scalar tensor from a larger tensor of rank 4.","extraInfo":[]},{"id":"When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_State_Spec",
      "title":"The Tensor Initialization and State Specification",
      "narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things.",
      "featureCount":"7",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"23",
      "executedFeatures":[{"id":"A tensor can be instantiated from a item type class and nested lists.","extraInfo":[]},{"id":"Numeric tensors as String can be formatted on an entry based level.","extraInfo":[]},{"id":"Tensor created from shape and datatype has expected state.","extraInfo":[]},{"id":"Tensors as String can be formatted depending on shape.","extraInfo":[]},{"id":"Tensors as String can be formatted on an entry based level.","extraInfo":[]},{"id":"The data and the value of a tensor a 2 different things!","extraInfo":[]},{"id":"We can create scalar tensors.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Stats_Spec",
      "title":"Reducing Tensors",
      "narrative":"Various kinds of operations reduce tensors to scalars,\n    the most common ones being the min and max operations \n    which find the smallest as well as largest number among all \n    items of a tensor.\n    Neureka exposes various different ways to achieve this,\n    all of which are also differential (autograd support).",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"395",
      "executedFeatures":[{"id":"Both the min and max operation support autograd (back-propagation).","extraInfo":[]},{"id":"The sum operation support autograd (back-propagation).","extraInfo":[]},{"id":"There is no need to use a function, we can use the min() and max() methods on tensors instead.","extraInfo":[]},{"id":"We can get pre-instantiated min and max functions from the library context.","extraInfo":[]},{"id":"We can use the \"sum\" method to sum the items of a tensor.","extraInfo":[]},{"id":"We can use the max operation as a function","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Version_Spec",
      "title":"Tensor (Data Array) Version",
      "narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n\n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors from occurring unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n\n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"143",
      "executedFeatures":[{"id":"Inline operations cause illegal state exceptions.","extraInfo":[]},{"id":"Inline operations causes version incrementation.","extraInfo":[]},{"id":"Non-inline operations do not cause version incrementation.","extraInfo":[]},{"id":"Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically).","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.Cleaner_Testing",
      "title":"How Neureka Cleans Up",
      "narrative":"Under the hood \n    Neureka deals whith large arrays of\n    data, which are often times \n    native data arrays requiring explicit\n    memory freeing!\n    This freeing of memory can happen at any time\n    during the livetime of a nd-array, however\n    it should happen at least up until the nd-arra/tensor\n    objects representing their referenced data arrays become\n    eligible for garbage collection.\n    This specification ensures that the custom garbage\n    cleaner implementation used by Neureka fulfills this role",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"407",
      "executedFeatures":[{"id":"The DeviceCleaner triggers registersd cleaner actions when things are eligable for GC.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.DataConverter_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"4",
      "executedFeatures":[{"id":"An array of any type of object may be converted to a array of primitives.","extraInfo":[]},{"id":"The DataConverter can convert the given array data.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.FileHandle_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"470",
      "executedFeatures":[{"id":"Fully labeled tenors will be stored with their labels included when saving them as CSV.","extraInfo":[]},{"id":"Partially labeled tenors will be stored with their labels included when saving them as CSV.","extraInfo":[]},{"id":"Test reading IDX file format.","extraInfo":[]},{"id":"Test writing IDX file format.","extraInfo":[]},{"id":"The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","extraInfo":[]},{"id":"We can load image files as tensors.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.ListReader_Exception_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"4",
      "executedFeatures":[{"id":"The ListReader will detect inconsistent degrees of nesting in the provided data.","extraInfo":[]},{"id":"The ListReader will detect inconsistent types in the provided data.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.ListReader_Spec",
      "title":"The Internal ListReader turning lists into flat arrays with shape and type data",
      "narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\".",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"5",
      "executedFeatures":[{"id":"The ListReader can interpret nested lists into a shape list and value list.","extraInfo":[]},{"id":"The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","extraInfo":[]},{"id":"The ListReader can interpret nested lists resembling a matrix into a shape list and value list.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.Utility_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"7",
      "executedFeatures":[{"id":"Object arrays can be converted to primitive arrays.","extraInfo":[]}],
      "ignoredFeatures":[]
    }
  ],
  "generator":"https://github.com/renatoathaydes/spock-reports"
}