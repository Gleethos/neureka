{
  "project": "Neureka",
  "version": "0.17.0",
  "created": "Thu Aug 11 21:54:28 CEST 2022",
  "statistics":{
    "runs":"79",
    "passed":"78",
    "failed":"0",
    "featureFailures":"0",
    "successRate":"1.0",
    "duration":"127161.0"
  },
  "specifications": [{
      "className":"ut.ndas.NDA_Instantiation_Spec",
      "title":"ND-Array Instantiation",
      "narrative":"In this specification we cover how ND-arrays can be instantiated.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"56",
      "executedFeatures":[{"id":"A vector can be created from an array of values through the \"of\" method.","extraInfo":[]},{"id":"ND-arrays can be created fluently.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"Example_Spec.Example_Spec",
      "title":"An Introduction to writing Spock Specifications",
      "narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n\n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way.",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"387",
      "executedFeatures":[{"id":"Call me feature not unit test!","extraInfo":[]},{"id":"I am readable and also best practice!","extraInfo":[]},{"id":"Numbers to the power of two with a fancy data table!","extraInfo":[]},{"id":"Should be able to remove from list","extraInfo":[]},{"id":"iAmNotSoReadable","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.AD_And_Computation_Graph_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"458",
      "executedFeatures":[{"id":"Payloads and derivatives are null after garbage collection.","extraInfo":[]},{"id":"Reshaping produces expected computation graph and also works with reverse mode AD.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"36",
      "executedFeatures":[{"id":"GraphNode instantiation works as expected when the context argument is a GraphLock.","extraInfo":[]},{"id":"GraphNode instantiation works as expected when the context argument is an ExecutionCall.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.Backend_MatMul_Extension_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"559",
      "executedFeatures":[{"id":"GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","extraInfo":[]},{"id":"Test context mock for opencl reference implementations.","extraInfo":[]},{"id":"Tile parsing for kernel parameter calculation yields expected tile dimensions.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndim.Tensor_Slice_Reshape_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"46",
      "executedFeatures":[{"id":"A slice of a tensor changes as expected when reshaping it.","extraInfo":[]},{"id":"Reshaping a slice works as expected.","extraInfo":[]},{"id":"Two slices of one big tensor perform matrix multiplication flawless.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Functional_Tensor_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"3629",
      "executedFeatures":[{"id":"Tensor initialization lambdas produce expected tensors.","extraInfo":[]},{"id":"Tensor mapping lambdas produce expected tensors.","extraInfo":[]},{"id":"The \"map\" method is a shorter convenience method for mapping to the same type.","extraInfo":[]},{"id":"We can analyse the values of a tensor using various predicate receiving methods","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Generics_Spec",
      "title":"Tensors as Generic Containers",
      "narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"15",
      "executedFeatures":[{"id":"1D tensors can be created from primitive arrays.","extraInfo":[]},{"id":"Anonymous tensor instance has the default datatype class as defined in Neureka settings.","extraInfo":[]},{"id":"String tensor instance discovers expected class.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_IO_Spec",
      "title":"The Tensor state Input and Output Specification",
      "narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!",
      "featureCount":"16",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"230",
      "executedFeatures":[{"id":"A tensor produced by a function has expected properties.","extraInfo":[]},{"id":"A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","extraInfo":[]},{"id":"Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","extraInfo":[]},{"id":"Indexing after reshaping works as expected.","extraInfo":[]},{"id":"Passing String seed to tensor produces expected values.","extraInfo":[]},{"id":"Smart tensor constructors yield expected results.","extraInfo":[]},{"id":"Tensor value type can not be changed by passing float or double arrays to it.","extraInfo":[]},{"id":"Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","extraInfo":[]},{"id":"Tensors can be instantiated with String seed.","extraInfo":[]},{"id":"Tensors value type can be changed by calling \"toType(...)\".","extraInfo":[]},{"id":"The tensor data array can be modified by targeting them with an index.","extraInfo":[]},{"id":"Vector tensors can be instantiated via factory methods.","extraInfo":[]},{"id":"We can manipulate the underlying data array of a tensor through the unsafe API.","extraInfo":[]},{"id":"We can re-populate a tensor of shorts from a single scalar value!","extraInfo":[]},{"id":"We turn a tensor into a scalar value or string through the \"as\" operator!","extraInfo":[]},{"id":"When we try to manipulate the underlying data array of a virtual tensor then it will become actual.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Layout_Spec",
      "title":"Row or Column Major. Why not both?",
      "narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"762",
      "executedFeatures":[{"id":"A new transposed version of a given tensor will be returned by the \"T()\" method.","extraInfo":[]},{"id":"Matrix multiplication works for both column and row major matrices across devices.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.DataConverter_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"200",
      "executedFeatures":[{"id":"An array of any type of object may be converted to a array of primitives.","extraInfo":[]},{"id":"The DataConverter can convert the given array data.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Backend_Functional_Algorithm_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"547",
      "executedFeatures":[{"id":"A functional algorithm cannot be used if it was not built properly!","extraInfo":[]},{"id":"A functional algorithm does not accept null as an answer!","extraInfo":[]},{"id":"A functional algorithm warns us when modified after it has been built!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.BackendContext_Spec",
      "title":"The BackendContext is a cloneable context which can run Tasks.",
      "narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"169",
      "executedFeatures":[{"id":"BackendContext instances can be created by cloning from Singleton instance.","extraInfo":[]},{"id":"BackendContext instances return Runner instances for easy visiting with return values.","extraInfo":[]},{"id":"BackendContext instances return Runner instances for easy visiting.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.internal.OpenCL_Data_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"182",
      "executedFeatures":[{"id":"The \"Data\" class can represent various OpenCL data types.","extraInfo":[]},{"id":"The OpenCLDevice specific Data class represents JVM data for OpenCL.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndim.Tensor_Reshape_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"16",
      "executedFeatures":[{"id":"When matrices are transpose, they will change their layout type.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.RMSprop_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"117",
      "executedFeatures":[{"id":"RMSprop optimizes according to expected inputs","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Device_Mock_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"30",
      "executedFeatures":[{"id":"Tensors try to migrate themselves to a device that is being added to them as component.","extraInfo":[]},{"id":"Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","extraInfo":[]},{"id":"The device of a tensor can be accessed via the \"device()\" method.","extraInfo":[]},{"id":"When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"it.Calculus_Stress_Test",
      "title":"",
      "narrative":"",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"5267",
      "executedFeatures":[{"id":"Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","extraInfo":[]},{"id":"Activation functions work across types.","extraInfo":[]},{"id":"Dot operation stress test runs error free and produces expected result","extraInfo":[]},{"id":"Stress test runs error free and produces expected result","extraInfo":[]},{"id":"The broadcast operation stress test runs error free and produces expected result","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.Autograd_Tensor_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"84",
      "executedFeatures":[{"id":"A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","extraInfo":[]},{"id":"Second-Test \"x-mul\" autograd behaviour. (Not on device)","extraInfo":[]},{"id":"Test basic autograd behaviour. (Not on device)","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndim.Tensor_NDConfiguration_Spec",
      "title":"What it means to be N-Dimensional",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"56",
      "executedFeatures":[{"id":"NDConfiguration instances of tensors have expected state and behaviour.","extraInfo":[]},{"id":"NDConfiguration instances of tensors have expected state.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Slicing_Spec",
      "title":"Tensors within Tensors",
      "narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing.",
      "featureCount":"9",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"88",
      "executedFeatures":[{"id":"A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","extraInfo":[]},{"id":"A tensor can be sliced by passing ranges in the form of primitive arrays.","extraInfo":[]},{"id":"Normal slicing will try to do autograd.","extraInfo":[]},{"id":"Slicing is also a Function with autograd support!","extraInfo":[]},{"id":"The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","extraInfo":[]},{"id":"The slice builder also supports slicing with custom step sizes.","extraInfo":[]},{"id":"We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","extraInfo":[]},{"id":"We can slice a scalar tensor from a larger tensor of rank 4.","extraInfo":[]},{"id":"When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.Autograd_Flags_Explained",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"675",
      "executedFeatures":[{"id":"Advanced backpropagation on all AD-Modes ","extraInfo":[]},{"id":"We can create a shallow copy of a tensor detached from the computation graph.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.Autograd_NN_Spec",
      "title":"Simple Neural Network autograd integration test",
      "narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n\n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"2083",
      "executedFeatures":[{"id":"Autograd work for simple matrix multiplications.","extraInfo":[]},{"id":"Autograd works for 2 matrix multiplications in a row.","extraInfo":[]},{"id":"Autograd works in a simple convolutional dot product and float based feed forward neural network.","extraInfo":[]},{"id":"Autograd works in a simple convolutional dot product based feed forward neural network.","extraInfo":[]},{"id":"Autograd works in a simple mat-mul based feed forward neural network.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.JITProp_Autograd_Tensor_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"8",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"250",
      "executedFeatures":[{"id":"Gradient auto-apply kicks in when used AD uses JIT prop","extraInfo":[]},{"id":"Test JIT propagation variant one.","extraInfo":[]},{"id":"Test JIT propagation variant two.","extraInfo":[]},{"id":"Test autograd without JIT and auto apply.","extraInfo":[]},{"id":"Test in-differential and JIT with auto apply","extraInfo":[]},{"id":"Test no JIT prop when forward AD","extraInfo":[]},{"id":"Test no preemptive gradient apply when not requested and auto apply and JIT_prop","extraInfo":[]},{"id":"Test pending error optimization","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Randomization_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"35",
      "executedFeatures":[{"id":"Randomization is in essence the same algorithm as JDKs \"Random\".","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.OpenCLDevice_Exception_Spec",
      "title":"OpenCLDevice Exception Handling",
      "narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"209",
      "executedFeatures":[{"id":"Ad hoc compilation produces expected exceptions when duplication is found.","extraInfo":[]},{"id":"Ad hoc compilation produces expected exceptions.","extraInfo":[]},{"id":"An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","extraInfo":[]},{"id":"Trying to restore a tensor which is not on a device raises exception.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.ndim.NDConfiguration_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"439",
      "executedFeatures":[{"id":"Various NDConfigurations behaviour exactly as their general purpose implementation.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Copy_Spec",
      "title":"To Copy or Not to Copy",
      "narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"133",
      "executedFeatures":[{"id":"A deep copy of a slice tensor is also a deep copy of the underlying data array.","extraInfo":[]},{"id":"A deep copy of a tensor is also a deep copy of the underlying data array.","extraInfo":[]},{"id":"A shallow copy will share the same underlying data as its original tensor.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.ListReader_Exception_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"25",
      "executedFeatures":[{"id":"The ListReader will detect inconsistent degrees of nesting in the provided data.","extraInfo":[]},{"id":"The ListReader will detect inconsistent types in the provided data.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"it.Cross_Device_Sliced_Tensor_System_Test",
      "title":"Cross Device Tensor Slicing",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"2177",
      "executedFeatures":[{"id":"Cross device sliced tensor integration test runs without errors.","extraInfo":[]},{"id":"Slices can be created using the SliceBuilder.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Calculus_Exception_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"10",
      "executedFeatures":[{"id":"Function throws exception when arity does not match input number.","extraInfo":[]},{"id":"Function throws exception when not enough inputs provided.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Tensor_Function_Spec",
      "title":"Applying Functions to Tensors",
      "narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you.",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"707",
      "executedFeatures":[{"id":"Executed tensors are intermediate tensors.","extraInfo":[]},{"id":"Reshaping on 3D tensors works by instantiate a Function instance built from a String.","extraInfo":[]},{"id":"Tensor results of various Function instances return expected results.","extraInfo":[]},{"id":"The \"DimTrim\" operation works forward as well as backward!","extraInfo":[]},{"id":"The optimization function for the SGD algorithm produces the expected result","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.Cross_Device_IO_Spec",
      "title":"Devices manage the states of the tensors they store!",
      "narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"116",
      "executedFeatures":[{"id":"We can use the access device API to read from a tensor.","extraInfo":[]},{"id":"We can use the access device API to write to a tensor","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.FileDevice_Spec",
      "title":"FileDevice, Storing Tensors in Files",
      "narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...).",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"206",
      "executedFeatures":[{"id":"A file device stores tensors in idx files by default.","extraInfo":[]},{"id":"A file device stores tensors in various file formats.","extraInfo":[]},{"id":"The file device can load known files in a directory.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.internal.OpenCL_GEMM_Unit_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"1063",
      "executedFeatures":[{"id":"The GEMM implementation for the OpenCLDevice has realistic behaviour","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_As_Container_Spec",
      "title":"Why not have a tensor of words?",
      "narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things...",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"69",
      "executedFeatures":[{"id":"More tensor operations translate to custom data type \"ComplexNumber\".","extraInfo":[]},{"id":"Plus operator on String tensors works element-wise.","extraInfo":[]},{"id":"Tensor operations translate to custom data type \"ComplexNumber\".","extraInfo":[]},{"id":"We can apply predicates on the values of a tensor.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_State_Spec",
      "title":"The Tensor Initialization and State Specification",
      "narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things.",
      "featureCount":"8",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"140",
      "executedFeatures":[{"id":"A tensor can be instantiated from a target type and nested lists.","extraInfo":[]},{"id":"Newly instantiated and unmodified scalar tensor has expected state.","extraInfo":[]},{"id":"Newly instantiated and unmodified vector tensor has expected state.","extraInfo":[]},{"id":"Numeric tensors as String can be formatted on an entry based level.","extraInfo":[]},{"id":"Tensor created from shape and datatype has expected state.","extraInfo":[]},{"id":"Tensors as String can be formatted depending on shape.","extraInfo":[]},{"id":"Tensors as String can be formatted on an entry based level.","extraInfo":[]},{"id":"The data and the value of a tensor a 2 different things!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.Cleaner_Testing",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"474",
      "executedFeatures":[{"id":"The default DeviceCleaner works","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.ListReader_Spec",
      "title":"The Internal ListReader turning lists into flat arrays with shape and type data",
      "narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\".",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"31",
      "executedFeatures":[{"id":"The ListReader can interpret nested lists into a shape list and value list.","extraInfo":[]},{"id":"The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","extraInfo":[]},{"id":"The ListReader can interpret nested lists resembling a matrix into a shape list and value list.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.Utility_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"43",
      "executedFeatures":[{"id":"Object arrays can be converted to primitive arrays.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Backend_Algorithm_AD_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"385",
      "executedFeatures":[{"id":"Activation implementations behave as expected.","extraInfo":[]},{"id":"Broadcast implementations behave as expected.","extraInfo":[]},{"id":"Convolution implementations behave as expected.","extraInfo":[]},{"id":"Operator implementations behave as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Calculus_Function_Spec",
      "title":"Testing Default Methods on Functions",
      "narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"366",
      "executedFeatures":[{"id":"Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","extraInfo":[]},{"id":"Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","extraInfo":[]},{"id":"Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","extraInfo":[]},{"id":"The library context exposes a set of useful functions.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.Cross_Device_Type_Spec",
      "title":"Cross Device-Type Unit Tests",
      "narrative":"",
      "featureCount":"10",
      "failures":"0",
      "errors":"0",
      "skipped":"1" ,
      "successRate":"1.0",
      "duration":"3609",
      "executedFeatures":[{"id":"Advanced device querying methods query as expected!","extraInfo":[]},{"id":"Devices store slices which can also be restored.","extraInfo":[]},{"id":"Devices store tensors which can also be restored.","extraInfo":[]},{"id":"Execution calls containing null arguments will cause an exception to be thrown in device instances.","extraInfo":[]},{"id":"Passing a numeric array to a tensor should modify its content!","extraInfo":[]},{"id":"Querying for Device implementations works as expected.","extraInfo":[]},{"id":"Tensor data can be fetched from device if the tensor is stored on it...","extraInfo":[]},{"id":"The simpler device querying methods query as expected!","extraInfo":[]},{"id":"Virtual tensors stay virtual when outsourced.","extraInfo":[]}],
      "ignoredFeatures":[{"id":"Devices cannot store slices which parents are not already stored.","extraInfo":[]}]
    },{
      "className":"ut.device.OpenCLDevice_Spec",
      "title":"The OpenCLDevice Specification",
      "narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly...",
      "featureCount":"8",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"614",
      "executedFeatures":[{"id":"Ad hoc compilation produces executable kernel.","extraInfo":[]},{"id":"Ad hoc compilation works for WIP general purpose matrix multiplication.","extraInfo":[]},{"id":"Ad hoc compilation works for custom column major based tiled matrix multiplication.","extraInfo":[]},{"id":"Ad hoc compilation works for custom simple row major based matrix multiplication.","extraInfo":[]},{"id":"Ad hoc matrix multiplication works for multiple of 16 matrices.","extraInfo":[]},{"id":"An OpenCLDevice loads tensors in a provided lambda temporarily.","extraInfo":[]},{"id":"The \"getData()\" method of an outsourced tensor will return null when outsourced.","extraInfo":[]},{"id":"The \"getValue()\" method of an outsourced tensor will return the expected array type.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.ADAM_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"864",
      "executedFeatures":[{"id":"ADAM optimizes according to expected inputs","extraInfo":[]},{"id":"Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","extraInfo":[]},{"id":"Equations used by ADAM return expected result.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.utility.FileHandle_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"1375",
      "executedFeatures":[{"id":"Fully labeled tenors will be stored with their labels included when saving them as CSV.","extraInfo":[]},{"id":"Partially labeled tenors will be stored with their labels included when saving them as CSV.","extraInfo":[]},{"id":"Test reading IDX file format.","extraInfo":[]},{"id":"Test writing IDX file format.","extraInfo":[]},{"id":"The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","extraInfo":[]},{"id":"We can load image files as tensors.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"st.Benchmark_System_Test",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"8876",
      "executedFeatures":[{"id":"Tensor can be constructed by passing List instances.","extraInfo":[]},{"id":"Test benchmark script and simple tensor constructor.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Matrix_Multiplication_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"2481",
      "executedFeatures":[{"id":"The internal matrix multiplication test script runs!","extraInfo":[]},{"id":"The simple CPU matrix multiplication implementation works as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Calculus_Scalar_Spec",
      "title":"Functions for Scalars",
      "narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values.",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"321",
      "executedFeatures":[{"id":"Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","extraInfo":[]},{"id":"Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","extraInfo":[]},{"id":"Function \"1/I[0]\" instance returns expected scalar results.","extraInfo":[]},{"id":"Function \"I[0]+1/I[0]\" instance returns expected scalar results.","extraInfo":[]},{"id":"Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","extraInfo":[]},{"id":"Test scalar results of various Function instances.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.internal.CLFunctionCompiler_Spec",
      "title":"Turning functions into kernels.",
      "narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"57",
      "executedFeatures":[{"id":"The CLFunctionCompiler produces an operation which properly integrates to the backend.","extraInfo":[]},{"id":"The CLFunctionCompiler produces the expected \"ad hoc\" kernel.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.dtype.DataType_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"61",
      "executedFeatures":[{"id":"DataType multi-ton instances behave as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.neureka.Neureka_Spec",
      "title":"The Neureka context can be used and configured as expected.",
      "narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general.",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"13650",
      "executedFeatures":[{"id":"Backend related library objects adhere to the same toString formatting convention!","extraInfo":[]},{"id":"Every Thread instance has their own Neureka instance.","extraInfo":[]},{"id":"Neureka class instance has expected behaviour.","extraInfo":[]},{"id":"Neureka settings class can be locked causing its properties to be immutable.","extraInfo":[]},{"id":"OpenCL related library objects adhere to the same toString formatting convention!","extraInfo":[]},{"id":"Various library objects adhere to the same toString formatting convention!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.Momentum_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"111",
      "executedFeatures":[{"id":"Momentum optimizes according to expected inputs","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Convolution_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"2423",
      "executedFeatures":[{"id":"Autograd works with simple 2D convolution.","extraInfo":[]},{"id":"Manual convolution produces expected result.","extraInfo":[]},{"id":"Sime convolution works as expected eith autograd.","extraInfo":[]},{"id":"Tensors have the correct layout after convolution.","extraInfo":[]},{"id":"The \"x\" (convolution) operator produces expected results (On the CPU).","extraInfo":[]},{"id":"Very simple manual convolution produces expected result.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Interop_Spec",
      "title":"Tensors play well with other data structures!",
      "narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements.",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"227",
      "executedFeatures":[{"id":"Not all tensor can be converted to images.","extraInfo":[]},{"id":"Tensor can be converted to buffered images.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"it.Eleven_Lines_NN_System_Spec",
      "title":"NN Code Golfing!",
      "narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    ´´´\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    ´´´",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"3625",
      "executedFeatures":[{"id":"One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","extraInfo":[]},{"id":"One can write a simple float based neural network in less than 11 lines of java like code!","extraInfo":[]},{"id":"One can write a simple neural network in less than 11 lines of code!","extraInfo":[]},{"id":"One can write a simple neural network with custom back-prop in 11 lines of code!","extraInfo":[]},{"id":"The pseudo random number generator works as expected for the weights used in the 11 line NN examples!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"st.Broad_System_Test",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"311",
      "executedFeatures":[{"id":"Test integration broadly.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests",
      "title":"",
      "narrative":"",
      "featureCount":"6",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"1268",
      "executedFeatures":[{"id":"GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","extraInfo":[]},{"id":"GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","extraInfo":[]},{"id":"GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","extraInfo":[]},{"id":"GraphNode throws an exception when trying to instantiate it with the wrong context.","extraInfo":[]},{"id":"GraphNode throws exception when payload is null.","extraInfo":[]},{"id":"GraphNode throws exception when trying to instantiate it with the Function argument being null.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.Backend_Extension_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"2",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"2988",
      "executedFeatures":[{"id":"Lambda properties of mock implementation interact with FunctionNode as expected.","extraInfo":[]},{"id":"Mock operation interacts with FunctionNode (AbstractFunction) instance as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.Matrix_Multiplication_Spec",
      "title":"Matrix Multiplication",
      "narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"124",
      "executedFeatures":[{"id":"The simple CPU matrix multiplication implementation works as expected.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.CPU_Spec",
      "title":"The CPU device, an API for CPU based execution",
      "narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"32",
      "executedFeatures":[{"id":"CPU knows the current number of available processor cores!","extraInfo":[]},{"id":"The CPU exposes a non null API for executing workloads in parallel.","extraInfo":[]},{"id":"Thread pool executes given workload in parallel","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.dtype.NumericType_Spec",
      "title":"The NumericType and its implementations model their respective numeric data types.",
      "narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"177",
      "executedFeatures":[{"id":"Conversion goes both ways and produces expected numeric values.","extraInfo":[]},{"id":"NumericType conversion to holder types yields expected results.","extraInfo":[]},{"id":"NumericType implementations behave as expected.","extraInfo":[]},{"id":"NumericType implementations return their expected properties.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.framing.Tensor_Framing_Spec",
      "title":"Naming Tensors and their Dimensions.",
      "narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"396",
      "executedFeatures":[{"id":"Added labels to tensors are accessible through the \"index()\" method.","extraInfo":[]},{"id":"Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","extraInfo":[]},{"id":"Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.AdaGrad_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"93",
      "executedFeatures":[{"id":"AdaGrad optimizes according to expected inputs","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.optimization.Optimizer_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"1" ,
      "successRate":"1.0",
      "duration":"3",
      "executedFeatures":[],
      "ignoredFeatures":[{"id":"Conv dot based feed forward and activation produces expected result.","extraInfo":[]}]
    },{
      "className":"ut.tensors.exceptions.Tensor_Delete_Exception_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"7",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"12",
      "executedFeatures":[{"id":"A deleted tensor will tell you that it has been deleted.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when accessing its configuration.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when accessing its data type.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when accessing its data.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when modifying its data type.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when trying to modify its data.","extraInfo":[]},{"id":"A deleted tensor will throw an exception when trying to set its configuration.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Operation_Spec",
      "title":"Running Tensors through operations",
      "narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid.",
      "featureCount":"11",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"14696",
      "executedFeatures":[{"id":"Activation functions work across types on slices and non sliced tensors.","extraInfo":[]},{"id":"Auto reshaping and broadcasting works and the result can be back propagated.","extraInfo":[]},{"id":"New method \"asFunction\" of String added at runtime is callable by groovy and also works.","extraInfo":[]},{"id":"New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","extraInfo":[]},{"id":"Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","extraInfo":[]},{"id":"Overloaded operation methods on tensors produce expected results when called.","extraInfo":[]},{"id":"Simple slice addition produces expected result.","extraInfo":[]},{"id":"The \"dot\" operation reshapes and produces valid \"x\" operation result.","extraInfo":[]},{"id":"The \"matMul\" operation produces the expected result.","extraInfo":[]},{"id":"The \"random\" function/operation populates tensors randomly.","extraInfo":[]},{"id":"The values of a randomly populated tensor seems to adhere to a gaussian distribution.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"it.Cross_Device_Spec",
      "title":"Cross Device Stress Test Specification",
      "narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same...",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"35213",
      "executedFeatures":[{"id":"A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","extraInfo":[]},{"id":"Convolution can model matrix multiplications across devices.","extraInfo":[]},{"id":"Mapping tensors works for every device (even if they are not used).","extraInfo":[]},{"id":"Test cross device system test runs successfully.","extraInfo":[]},{"id":"Test simple NN implementation with manual backprop","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.Autograd_Explained",
      "title":"Autograd - Automatic Differentiation",
      "narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n\n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n\n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n\n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked.",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"36",
      "executedFeatures":[{"id":"Simple automatic differentiation and propagation.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests",
      "title":"",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"566",
      "executedFeatures":[{"id":"A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.backend.core.Backend_Algorithm_Implementation_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"2172",
      "executedFeatures":[{"id":"Activation implementations have expected Executor instances.","extraInfo":[]},{"id":"CLExecutors of Operator implementations behave as expected.","extraInfo":[]},{"id":"HostExecutors of Operator implementations behave as expected.","extraInfo":[]},{"id":"Operator implementations have expected Executor instances.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.Calculus_Parsing_Spec",
      "title":"Parsing Expressions into Functions",
      "narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations.",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"111",
      "executedFeatures":[{"id":"Functions can derive themselves according to the provided index of the input which ought to be derived.","extraInfo":[]},{"id":"Parsed equations throw expected error messages.","extraInfo":[]},{"id":"Test parsed equations when building Function instances.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.calculus.ConCat_Spec",
      "title":"Merging Tensors",
      "narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors.",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"209",
      "executedFeatures":[{"id":"We can concatenate 2 float tensors alongside a specified axis!","extraInfo":[]},{"id":"We can concatenate 2 string tensors alongside a specified axis!","extraInfo":[]},{"id":"We can concatenate 2 tensors alongside a specified axis!","extraInfo":[]},{"id":"We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","extraInfo":[]},{"id":"We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.CLFunctionCompiler_Spec",
      "title":"OpenCLDevice Function Optimization Integration Tests",
      "narrative":"",
      "featureCount":"1",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"58",
      "executedFeatures":[{"id":"The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler).","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.device.OpenCL_Spec",
      "title":"Working with OpenCL",
      "narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices...",
      "featureCount":"5",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"6492",
      "executedFeatures":[{"id":"A given OpenCL context can be disposed!","extraInfo":[]},{"id":"An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","extraInfo":[]},{"id":"First found OpenCLDevice will have realistic numeric properties.","extraInfo":[]},{"id":"First found OpenCLDevice will have realistic properties inside summary query.","extraInfo":[]},{"id":"First found OpenCLDevice will have realistic text properties.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.exceptions.Tensor_Exception_Spec",
      "title":"Tensors Exception Behavior",
      "narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse.",
      "featureCount":"8",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"220",
      "executedFeatures":[{"id":"Building a tensor with \"null\" as shape argument throws an exception.","extraInfo":[]},{"id":"Building a tensor with 0 shape arguments throws an exception.","extraInfo":[]},{"id":"Casting a tensor as something unusual will cuas an exception to be thrown.","extraInfo":[]},{"id":"Out of dimension bound causes descriptive exception!","extraInfo":[]},{"id":"Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","extraInfo":[]},{"id":"Passing an invalid object into Tsr constructor causes descriptive exception.","extraInfo":[]},{"id":"Passing null to various methods of the tensor API will throw exceptions.","extraInfo":[]},{"id":"Trying to inject an empty tensor into another causes fitting exception.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Fluent_Tensor_Creation_Spec",
      "title":"",
      "narrative":"",
      "featureCount":"7",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"131",
      "executedFeatures":[{"id":"Initialization lambda based tensors can be created fluently.","extraInfo":[]},{"id":"Range based tensors can be created fluently.","extraInfo":[]},{"id":"Scalars can be created fluently.","extraInfo":[]},{"id":"Seed based tensors can be created fluently.","extraInfo":[]},{"id":"Tensors can be created fluently.","extraInfo":[]},{"id":"Value based tensors can be created fluently.","extraInfo":[]},{"id":"Vectors can be created fluently.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Gradient_Spec",
      "title":"Gradients are Tensors which are Components of other Tensors",
      "narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to...",
      "featureCount":"3",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"14",
      "executedFeatures":[{"id":"Gradient of tensor is being applies regardless of the tensor requiring gradient or not","extraInfo":[]},{"id":"Tensors can have gradients but not require them.","extraInfo":[]},{"id":"Tensors that have gradients but do not require them still print them.","extraInfo":[]}],
      "ignoredFeatures":[]
    },{
      "className":"ut.tensors.Tensor_Version_Spec",
      "title":"Tensor (Data Array) Version",
      "narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n\n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n\n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes.",
      "featureCount":"4",
      "failures":"0",
      "errors":"0",
      "skipped":"0" ,
      "successRate":"1.0",
      "duration":"975",
      "executedFeatures":[{"id":"Inline operations cause illegal state exceptions.","extraInfo":[]},{"id":"Inline operations causes version incrementation.","extraInfo":[]},{"id":"Non-inline operations do not cause version incrementation.","extraInfo":[]},{"id":"Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically).","extraInfo":[]}],
      "ignoredFeatures":[]
    }
  ],
  "generator":"https://github.com/renatoathaydes/spock-reports"
}