{
  "className":"ut.autograd.Autograd_Flags_Explained",
  "statistics":{
    "runs":"9",
    "successRate":"100.0%",
    "failures":"0",
    "errors":"0",
    "skipped":"0",
    "duration":"1.352 seconds"
  },

  "title":"",
  "narrative":"",
  "headers":["\\u000a <b> Autograd Advanced - Custom Autograd </b> <br>\\u000a <br>\\u000a Neureka does not necessarily perform autograd eagerly. <br>\\u000a If required then auto-differentiation will occur as one would expect <br>\\u000a similarly to the way PyTorch's autograd works. <br>\\u000a However for many use cases it might make sense to use different variants <br>\\u000a of auto-differentiation. <br>\\u000a This specification covers precisely these different autograd modes. <br>\\u000a <br>\\u000a"],"tags":{},"see":[],
  "features":[ 
    {
      "id":"Advanced backpropagation on all AD-Modes [code: y*y*3, whenRsd: false, whenUse: false, doJIT: false, afterBack: .*1.*4\\.5.*, afterUse: .*1.*4\\.5.*, afterRqd: .*1.*4\\.5.*, afterAll: .*1.*4\\.5.*, #0]",
      "result":"PASS",
      "duration":"0.675 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":["\\u000a What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b> <br>\\u000a This run covers the feature having the following settings : <br>\\u000a <br>\\u000a <b> Neureka.instance().settings().autograd().: </b> <br>\\u000a isRetainingPendingErrorForJITProp := false <br> \\u000a isApplyingGradientWhenTensorIsUsed := false <br> \\u000a isApplyingGradientWhenRequested := false <br> \\u000a <br>\\u000a ...code producing the result : 'y*y*3' <br> \\u000a <br> \\u000a <b> is-Retaining-Pending-Error-For-JITProp : </b> <br>\\u000a <br>\\u000a This flag enables an optimization technique which only propagates error values to <br>\\u000a gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them <br>\\u000a at divergent differentiation paths within the computation graph. <br>\\u000a If the flag is set to true <br>\\u000a then error values will accumulate at such junction nodes. <br>\\u000a This technique however uses more memory but will <br>\\u000a improve performance for some networks substantially. <br>\\u000a The technique is termed JIT-Propagation. <br>\\u000a <br>\\u000a <br>\\u000a <b> is-Applying-Gradient-When-Tensor-Is-Used : </b> <br>\\u000a <br>\\u000a Gradients will automatically be applied (or JITed) to tensors as soon as <br>\\u000a they are being used for calculation (GraphNode instantiation). <br>\\u000a This feature works well with JIT-Propagation. <br>\\u000a <br>\\u000a <br> \\u000a <b> is-Applying-Gradient-When-Requested : </b> <br>\\u000a <br>\\u000a Gradients will only be applied if requested. <br>\\u000a Usually this happens immediately, however <br>\\u000a if the flag 'applyGradientWhenTensorIsUsed' is set <br>\\u000a to true, then the tensor will only be updated by its <br>\\u000a gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation). <br>\\u000a <br>\\u000a <b> Let's take a look : </b>\\u000a"]
      },
      "blocks":[
        {"kind":"given","text":"We configure Neureka autograd:","code":["Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd","Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse","Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT"]},

        {"kind":"and","text":"","code":["def x = Tsr.of([2, 2], 1d).setRqsGradient(true)","def y = x + 2","Binding binding = new Binding()","binding.setVariable('x', x)","binding.setVariable('y', y)"]},

        {"kind":"when","text":"The code snippet is being execute...","code":["Tsr z = new GroovyShell(binding).evaluate((code))"]},

        {"kind":"then","text":"As expected, this new tensor contains four times 27 :","code":["z.toString().contains(\"(2x2):[27.0, 27.0, 27.0, 27.0]\")"]},

        {"kind":"when","text":"\u000a We call the \"mean()\" method as a simple loss function!\u000a This produces a scalar output tensor which is ideal as entrypoint\u000a for the autograd algorithm.\u000a","code":["def result = z.mean()"]},

        {"kind":"then","text":"This \"result\" tensor will be the expected scalar :","code":["result.toString().contains(\"(1x1):[27.0]\")"]},

        {"kind":"when","text":"\u000a We now try to backpropagate! Because \"result\" contains a single scalar,\u000a result.backward() is equivalent to out.backward(Tsr.of(1)).\u000a","code":["z.backward(0.25)","def xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterBack )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterUse )"]},

        {"kind":"when","text":"","code":["x.setGradientApplyRequested( true )","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterRqd )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterAll )"]},

        {"kind":"when","text":"It is time to free some memory because our history of computation has grown a bit...","code":["result.unsafe.detach()"]},

        {"kind":"then","text":"Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !","code":["!result.has( GraphNode.class )"]},

        {"kind":"where","text":"","code":{"code":["y*y*3"],"whenRsd":["false"],"whenUse":["false"],"doJIT":["false"],"afterBack":[".*1.*4\\.5.*"],"afterUse":[".*1.*4\\.5.*"],"afterRqd":[".*1.*4\\.5.*"],"afterAll":[".*1.*4\\.5.*"]}}
      ],
      "problems":"[]"
    },
  
    {
      "id":"Advanced backpropagation on all AD-Modes [code: y*y*3, whenRsd: true, whenUse: false, doJIT: false, afterBack: .*1.*4\\.5.*, afterUse: .*1.*4\\.5.*, afterRqd: .*5\\.5.*null.*, afterAll: .*5\\.5.*null.*, #1]",
      "result":"PASS",
      "duration":"0.074 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":["\\u000a What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b> <br>\\u000a This run covers the feature having the following settings : <br>\\u000a <br>\\u000a <b> Neureka.instance().settings().autograd().: </b> <br>\\u000a isRetainingPendingErrorForJITProp := false <br> \\u000a isApplyingGradientWhenTensorIsUsed := false <br> \\u000a isApplyingGradientWhenRequested := true <br> \\u000a <br>\\u000a ...code producing the result : 'y*y*3' <br> \\u000a <br> \\u000a <b> is-Retaining-Pending-Error-For-JITProp : </b> <br>\\u000a <br>\\u000a This flag enables an optimization technique which only propagates error values to <br>\\u000a gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them <br>\\u000a at divergent differentiation paths within the computation graph. <br>\\u000a If the flag is set to true <br>\\u000a then error values will accumulate at such junction nodes. <br>\\u000a This technique however uses more memory but will <br>\\u000a improve performance for some networks substantially. <br>\\u000a The technique is termed JIT-Propagation. <br>\\u000a <br>\\u000a <br>\\u000a <b> is-Applying-Gradient-When-Tensor-Is-Used : </b> <br>\\u000a <br>\\u000a Gradients will automatically be applied (or JITed) to tensors as soon as <br>\\u000a they are being used for calculation (GraphNode instantiation). <br>\\u000a This feature works well with JIT-Propagation. <br>\\u000a <br>\\u000a <br> \\u000a <b> is-Applying-Gradient-When-Requested : </b> <br>\\u000a <br>\\u000a Gradients will only be applied if requested. <br>\\u000a Usually this happens immediately, however <br>\\u000a if the flag 'applyGradientWhenTensorIsUsed' is set <br>\\u000a to true, then the tensor will only be updated by its <br>\\u000a gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation). <br>\\u000a <br>\\u000a <b> Let's take a look : </b>\\u000a"]
      },
      "blocks":[
        {"kind":"given","text":"We configure Neureka autograd:","code":["Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd","Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse","Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT"]},

        {"kind":"and","text":"","code":["def x = Tsr.of([2, 2], 1d).setRqsGradient(true)","def y = x + 2","Binding binding = new Binding()","binding.setVariable('x', x)","binding.setVariable('y', y)"]},

        {"kind":"when","text":"The code snippet is being execute...","code":["Tsr z = new GroovyShell(binding).evaluate((code))"]},

        {"kind":"then","text":"As expected, this new tensor contains four times 27 :","code":["z.toString().contains(\"(2x2):[27.0, 27.0, 27.0, 27.0]\")"]},

        {"kind":"when","text":"\u000a We call the \"mean()\" method as a simple loss function!\u000a This produces a scalar output tensor which is ideal as entrypoint\u000a for the autograd algorithm.\u000a","code":["def result = z.mean()"]},

        {"kind":"then","text":"This \"result\" tensor will be the expected scalar :","code":["result.toString().contains(\"(1x1):[27.0]\")"]},

        {"kind":"when","text":"\u000a We now try to backpropagate! Because \"result\" contains a single scalar,\u000a result.backward() is equivalent to out.backward(Tsr.of(1)).\u000a","code":["z.backward(0.25)","def xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterBack )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterUse )"]},

        {"kind":"when","text":"","code":["x.setGradientApplyRequested( true )","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterRqd )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterAll )"]},

        {"kind":"when","text":"It is time to free some memory because our history of computation has grown a bit...","code":["result.unsafe.detach()"]},

        {"kind":"then","text":"Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !","code":["!result.has( GraphNode.class )"]},

        {"kind":"where","text":"","code":{"code":["y*y*3"],"whenRsd":["true"],"whenUse":["false"],"doJIT":["false"],"afterBack":[".*1.*4\\.5.*"],"afterUse":[".*1.*4\\.5.*"],"afterRqd":[".*5\\.5.*null.*"],"afterAll":[".*5\\.5.*null.*"]}}
      ],
      "problems":"[]"
    },
  
    {
      "id":"Advanced backpropagation on all AD-Modes [code: y*y*3, whenRsd: false, whenUse: true, doJIT: false, afterBack: .*1.*4\\.5.*, afterUse: .*5\\.5.*null.*, afterRqd: .*5\\.5.*null.*, afterAll: .*5\\.5.*null.*, #2]",
      "result":"PASS",
      "duration":"0.136 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":["\\u000a What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b> <br>\\u000a This run covers the feature having the following settings : <br>\\u000a <br>\\u000a <b> Neureka.instance().settings().autograd().: </b> <br>\\u000a isRetainingPendingErrorForJITProp := false <br> \\u000a isApplyingGradientWhenTensorIsUsed := true <br> \\u000a isApplyingGradientWhenRequested := false <br> \\u000a <br>\\u000a ...code producing the result : 'y*y*3' <br> \\u000a <br> \\u000a <b> is-Retaining-Pending-Error-For-JITProp : </b> <br>\\u000a <br>\\u000a This flag enables an optimization technique which only propagates error values to <br>\\u000a gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them <br>\\u000a at divergent differentiation paths within the computation graph. <br>\\u000a If the flag is set to true <br>\\u000a then error values will accumulate at such junction nodes. <br>\\u000a This technique however uses more memory but will <br>\\u000a improve performance for some networks substantially. <br>\\u000a The technique is termed JIT-Propagation. <br>\\u000a <br>\\u000a <br>\\u000a <b> is-Applying-Gradient-When-Tensor-Is-Used : </b> <br>\\u000a <br>\\u000a Gradients will automatically be applied (or JITed) to tensors as soon as <br>\\u000a they are being used for calculation (GraphNode instantiation). <br>\\u000a This feature works well with JIT-Propagation. <br>\\u000a <br>\\u000a <br> \\u000a <b> is-Applying-Gradient-When-Requested : </b> <br>\\u000a <br>\\u000a Gradients will only be applied if requested. <br>\\u000a Usually this happens immediately, however <br>\\u000a if the flag 'applyGradientWhenTensorIsUsed' is set <br>\\u000a to true, then the tensor will only be updated by its <br>\\u000a gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation). <br>\\u000a <br>\\u000a <b> Let's take a look : </b>\\u000a"]
      },
      "blocks":[
        {"kind":"given","text":"We configure Neureka autograd:","code":["Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd","Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse","Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT"]},

        {"kind":"and","text":"","code":["def x = Tsr.of([2, 2], 1d).setRqsGradient(true)","def y = x + 2","Binding binding = new Binding()","binding.setVariable('x', x)","binding.setVariable('y', y)"]},

        {"kind":"when","text":"The code snippet is being execute...","code":["Tsr z = new GroovyShell(binding).evaluate((code))"]},

        {"kind":"then","text":"As expected, this new tensor contains four times 27 :","code":["z.toString().contains(\"(2x2):[27.0, 27.0, 27.0, 27.0]\")"]},

        {"kind":"when","text":"\u000a We call the \"mean()\" method as a simple loss function!\u000a This produces a scalar output tensor which is ideal as entrypoint\u000a for the autograd algorithm.\u000a","code":["def result = z.mean()"]},

        {"kind":"then","text":"This \"result\" tensor will be the expected scalar :","code":["result.toString().contains(\"(1x1):[27.0]\")"]},

        {"kind":"when","text":"\u000a We now try to backpropagate! Because \"result\" contains a single scalar,\u000a result.backward() is equivalent to out.backward(Tsr.of(1)).\u000a","code":["z.backward(0.25)","def xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterBack )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterUse )"]},

        {"kind":"when","text":"","code":["x.setGradientApplyRequested( true )","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterRqd )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterAll )"]},

        {"kind":"when","text":"It is time to free some memory because our history of computation has grown a bit...","code":["result.unsafe.detach()"]},

        {"kind":"then","text":"Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !","code":["!result.has( GraphNode.class )"]},

        {"kind":"where","text":"","code":{"code":["y*y*3"],"whenRsd":["false"],"whenUse":["true"],"doJIT":["false"],"afterBack":[".*1.*4\\.5.*"],"afterUse":[".*5\\.5.*null.*"],"afterRqd":[".*5\\.5.*null.*"],"afterAll":[".*5\\.5.*null.*"]}}
      ],
      "problems":"[]"
    },
  
    {
      "id":"Advanced backpropagation on all AD-Modes [code: y*y*3, whenRsd: true, whenUse: true, doJIT: false, afterBack: .*1.*4\\.5.*, afterUse: .*1.*4\\.5.*, afterRqd: .*1.*4\\.5.*, afterAll: .*5\\.5.*null.*, #3]",
      "result":"PASS",
      "duration":"0.072 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":["\\u000a What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b> <br>\\u000a This run covers the feature having the following settings : <br>\\u000a <br>\\u000a <b> Neureka.instance().settings().autograd().: </b> <br>\\u000a isRetainingPendingErrorForJITProp := false <br> \\u000a isApplyingGradientWhenTensorIsUsed := true <br> \\u000a isApplyingGradientWhenRequested := true <br> \\u000a <br>\\u000a ...code producing the result : 'y*y*3' <br> \\u000a <br> \\u000a <b> is-Retaining-Pending-Error-For-JITProp : </b> <br>\\u000a <br>\\u000a This flag enables an optimization technique which only propagates error values to <br>\\u000a gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them <br>\\u000a at divergent differentiation paths within the computation graph. <br>\\u000a If the flag is set to true <br>\\u000a then error values will accumulate at such junction nodes. <br>\\u000a This technique however uses more memory but will <br>\\u000a improve performance for some networks substantially. <br>\\u000a The technique is termed JIT-Propagation. <br>\\u000a <br>\\u000a <br>\\u000a <b> is-Applying-Gradient-When-Tensor-Is-Used : </b> <br>\\u000a <br>\\u000a Gradients will automatically be applied (or JITed) to tensors as soon as <br>\\u000a they are being used for calculation (GraphNode instantiation). <br>\\u000a This feature works well with JIT-Propagation. <br>\\u000a <br>\\u000a <br> \\u000a <b> is-Applying-Gradient-When-Requested : </b> <br>\\u000a <br>\\u000a Gradients will only be applied if requested. <br>\\u000a Usually this happens immediately, however <br>\\u000a if the flag 'applyGradientWhenTensorIsUsed' is set <br>\\u000a to true, then the tensor will only be updated by its <br>\\u000a gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation). <br>\\u000a <br>\\u000a <b> Let's take a look : </b>\\u000a"]
      },
      "blocks":[
        {"kind":"given","text":"We configure Neureka autograd:","code":["Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd","Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse","Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT"]},

        {"kind":"and","text":"","code":["def x = Tsr.of([2, 2], 1d).setRqsGradient(true)","def y = x + 2","Binding binding = new Binding()","binding.setVariable('x', x)","binding.setVariable('y', y)"]},

        {"kind":"when","text":"The code snippet is being execute...","code":["Tsr z = new GroovyShell(binding).evaluate((code))"]},

        {"kind":"then","text":"As expected, this new tensor contains four times 27 :","code":["z.toString().contains(\"(2x2):[27.0, 27.0, 27.0, 27.0]\")"]},

        {"kind":"when","text":"\u000a We call the \"mean()\" method as a simple loss function!\u000a This produces a scalar output tensor which is ideal as entrypoint\u000a for the autograd algorithm.\u000a","code":["def result = z.mean()"]},

        {"kind":"then","text":"This \"result\" tensor will be the expected scalar :","code":["result.toString().contains(\"(1x1):[27.0]\")"]},

        {"kind":"when","text":"\u000a We now try to backpropagate! Because \"result\" contains a single scalar,\u000a result.backward() is equivalent to out.backward(Tsr.of(1)).\u000a","code":["z.backward(0.25)","def xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterBack )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterUse )"]},

        {"kind":"when","text":"","code":["x.setGradientApplyRequested( true )","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterRqd )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterAll )"]},

        {"kind":"when","text":"It is time to free some memory because our history of computation has grown a bit...","code":["result.unsafe.detach()"]},

        {"kind":"then","text":"Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !","code":["!result.has( GraphNode.class )"]},

        {"kind":"where","text":"","code":{"code":["y*y*3"],"whenRsd":["true"],"whenUse":["true"],"doJIT":["false"],"afterBack":[".*1.*4\\.5.*"],"afterUse":[".*1.*4\\.5.*"],"afterRqd":[".*1.*4\\.5.*"],"afterAll":[".*5\\.5.*null.*"]}}
      ],
      "problems":"[]"
    },
  
    {
      "id":"Advanced backpropagation on all AD-Modes [code: y*y*3, whenRsd: false, whenUse: false, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*1.*null.*, afterAll: .*1.*null.*, #4]",
      "result":"PASS",
      "duration":"0.075 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":["\\u000a What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b> <br>\\u000a This run covers the feature having the following settings : <br>\\u000a <br>\\u000a <b> Neureka.instance().settings().autograd().: </b> <br>\\u000a isRetainingPendingErrorForJITProp := true <br> \\u000a isApplyingGradientWhenTensorIsUsed := false <br> \\u000a isApplyingGradientWhenRequested := false <br> \\u000a <br>\\u000a ...code producing the result : 'y*y*3' <br> \\u000a <br> \\u000a <b> is-Retaining-Pending-Error-For-JITProp : </b> <br>\\u000a <br>\\u000a This flag enables an optimization technique which only propagates error values to <br>\\u000a gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them <br>\\u000a at divergent differentiation paths within the computation graph. <br>\\u000a If the flag is set to true <br>\\u000a then error values will accumulate at such junction nodes. <br>\\u000a This technique however uses more memory but will <br>\\u000a improve performance for some networks substantially. <br>\\u000a The technique is termed JIT-Propagation. <br>\\u000a <br>\\u000a <br>\\u000a <b> is-Applying-Gradient-When-Tensor-Is-Used : </b> <br>\\u000a <br>\\u000a Gradients will automatically be applied (or JITed) to tensors as soon as <br>\\u000a they are being used for calculation (GraphNode instantiation). <br>\\u000a This feature works well with JIT-Propagation. <br>\\u000a <br>\\u000a <br> \\u000a <b> is-Applying-Gradient-When-Requested : </b> <br>\\u000a <br>\\u000a Gradients will only be applied if requested. <br>\\u000a Usually this happens immediately, however <br>\\u000a if the flag 'applyGradientWhenTensorIsUsed' is set <br>\\u000a to true, then the tensor will only be updated by its <br>\\u000a gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation). <br>\\u000a <br>\\u000a <b> Let's take a look : </b>\\u000a"]
      },
      "blocks":[
        {"kind":"given","text":"We configure Neureka autograd:","code":["Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd","Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse","Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT"]},

        {"kind":"and","text":"","code":["def x = Tsr.of([2, 2], 1d).setRqsGradient(true)","def y = x + 2","Binding binding = new Binding()","binding.setVariable('x', x)","binding.setVariable('y', y)"]},

        {"kind":"when","text":"The code snippet is being execute...","code":["Tsr z = new GroovyShell(binding).evaluate((code))"]},

        {"kind":"then","text":"As expected, this new tensor contains four times 27 :","code":["z.toString().contains(\"(2x2):[27.0, 27.0, 27.0, 27.0]\")"]},

        {"kind":"when","text":"\u000a We call the \"mean()\" method as a simple loss function!\u000a This produces a scalar output tensor which is ideal as entrypoint\u000a for the autograd algorithm.\u000a","code":["def result = z.mean()"]},

        {"kind":"then","text":"This \"result\" tensor will be the expected scalar :","code":["result.toString().contains(\"(1x1):[27.0]\")"]},

        {"kind":"when","text":"\u000a We now try to backpropagate! Because \"result\" contains a single scalar,\u000a result.backward() is equivalent to out.backward(Tsr.of(1)).\u000a","code":["z.backward(0.25)","def xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterBack )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterUse )"]},

        {"kind":"when","text":"","code":["x.setGradientApplyRequested( true )","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterRqd )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterAll )"]},

        {"kind":"when","text":"It is time to free some memory because our history of computation has grown a bit...","code":["result.unsafe.detach()"]},

        {"kind":"then","text":"Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !","code":["!result.has( GraphNode.class )"]},

        {"kind":"where","text":"","code":{"code":["y*y*3"],"whenRsd":["false"],"whenUse":["false"],"doJIT":["true"],"afterBack":[".*1.*null.*"],"afterUse":[".*1.*null.*"],"afterRqd":[".*1.*null.*"],"afterAll":[".*1.*null.*"]}}
      ],
      "problems":"[]"
    },
  
    {
      "id":"Advanced backpropagation on all AD-Modes [code: y*y*3, whenRsd: true, whenUse: false, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*5\\.5.*null.*, afterAll: .*5\\.5.*null.*, #5]",
      "result":"PASS",
      "duration":"0.067 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":["\\u000a What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b> <br>\\u000a This run covers the feature having the following settings : <br>\\u000a <br>\\u000a <b> Neureka.instance().settings().autograd().: </b> <br>\\u000a isRetainingPendingErrorForJITProp := true <br> \\u000a isApplyingGradientWhenTensorIsUsed := false <br> \\u000a isApplyingGradientWhenRequested := true <br> \\u000a <br>\\u000a ...code producing the result : 'y*y*3' <br> \\u000a <br> \\u000a <b> is-Retaining-Pending-Error-For-JITProp : </b> <br>\\u000a <br>\\u000a This flag enables an optimization technique which only propagates error values to <br>\\u000a gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them <br>\\u000a at divergent differentiation paths within the computation graph. <br>\\u000a If the flag is set to true <br>\\u000a then error values will accumulate at such junction nodes. <br>\\u000a This technique however uses more memory but will <br>\\u000a improve performance for some networks substantially. <br>\\u000a The technique is termed JIT-Propagation. <br>\\u000a <br>\\u000a <br>\\u000a <b> is-Applying-Gradient-When-Tensor-Is-Used : </b> <br>\\u000a <br>\\u000a Gradients will automatically be applied (or JITed) to tensors as soon as <br>\\u000a they are being used for calculation (GraphNode instantiation). <br>\\u000a This feature works well with JIT-Propagation. <br>\\u000a <br>\\u000a <br> \\u000a <b> is-Applying-Gradient-When-Requested : </b> <br>\\u000a <br>\\u000a Gradients will only be applied if requested. <br>\\u000a Usually this happens immediately, however <br>\\u000a if the flag 'applyGradientWhenTensorIsUsed' is set <br>\\u000a to true, then the tensor will only be updated by its <br>\\u000a gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation). <br>\\u000a <br>\\u000a <b> Let's take a look : </b>\\u000a"]
      },
      "blocks":[
        {"kind":"given","text":"We configure Neureka autograd:","code":["Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd","Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse","Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT"]},

        {"kind":"and","text":"","code":["def x = Tsr.of([2, 2], 1d).setRqsGradient(true)","def y = x + 2","Binding binding = new Binding()","binding.setVariable('x', x)","binding.setVariable('y', y)"]},

        {"kind":"when","text":"The code snippet is being execute...","code":["Tsr z = new GroovyShell(binding).evaluate((code))"]},

        {"kind":"then","text":"As expected, this new tensor contains four times 27 :","code":["z.toString().contains(\"(2x2):[27.0, 27.0, 27.0, 27.0]\")"]},

        {"kind":"when","text":"\u000a We call the \"mean()\" method as a simple loss function!\u000a This produces a scalar output tensor which is ideal as entrypoint\u000a for the autograd algorithm.\u000a","code":["def result = z.mean()"]},

        {"kind":"then","text":"This \"result\" tensor will be the expected scalar :","code":["result.toString().contains(\"(1x1):[27.0]\")"]},

        {"kind":"when","text":"\u000a We now try to backpropagate! Because \"result\" contains a single scalar,\u000a result.backward() is equivalent to out.backward(Tsr.of(1)).\u000a","code":["z.backward(0.25)","def xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterBack )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterUse )"]},

        {"kind":"when","text":"","code":["x.setGradientApplyRequested( true )","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterRqd )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterAll )"]},

        {"kind":"when","text":"It is time to free some memory because our history of computation has grown a bit...","code":["result.unsafe.detach()"]},

        {"kind":"then","text":"Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !","code":["!result.has( GraphNode.class )"]},

        {"kind":"where","text":"","code":{"code":["y*y*3"],"whenRsd":["true"],"whenUse":["false"],"doJIT":["true"],"afterBack":[".*1.*null.*"],"afterUse":[".*1.*null.*"],"afterRqd":[".*5\\.5.*null.*"],"afterAll":[".*5\\.5.*null.*"]}}
      ],
      "problems":"[]"
    },
  
    {
      "id":"Advanced backpropagation on all AD-Modes [code: y*y*3, whenRsd: false, whenUse: true, doJIT: true, afterBack: .*1.*null.*, afterUse: .*5\\.5.*null.*, afterRqd: .*5\\.5.*null.*, afterAll: .*5\\.5.*null.*, #6]",
      "result":"PASS",
      "duration":"0.025 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":["\\u000a What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b> <br>\\u000a This run covers the feature having the following settings : <br>\\u000a <br>\\u000a <b> Neureka.instance().settings().autograd().: </b> <br>\\u000a isRetainingPendingErrorForJITProp := true <br> \\u000a isApplyingGradientWhenTensorIsUsed := true <br> \\u000a isApplyingGradientWhenRequested := false <br> \\u000a <br>\\u000a ...code producing the result : 'y*y*3' <br> \\u000a <br> \\u000a <b> is-Retaining-Pending-Error-For-JITProp : </b> <br>\\u000a <br>\\u000a This flag enables an optimization technique which only propagates error values to <br>\\u000a gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them <br>\\u000a at divergent differentiation paths within the computation graph. <br>\\u000a If the flag is set to true <br>\\u000a then error values will accumulate at such junction nodes. <br>\\u000a This technique however uses more memory but will <br>\\u000a improve performance for some networks substantially. <br>\\u000a The technique is termed JIT-Propagation. <br>\\u000a <br>\\u000a <br>\\u000a <b> is-Applying-Gradient-When-Tensor-Is-Used : </b> <br>\\u000a <br>\\u000a Gradients will automatically be applied (or JITed) to tensors as soon as <br>\\u000a they are being used for calculation (GraphNode instantiation). <br>\\u000a This feature works well with JIT-Propagation. <br>\\u000a <br>\\u000a <br> \\u000a <b> is-Applying-Gradient-When-Requested : </b> <br>\\u000a <br>\\u000a Gradients will only be applied if requested. <br>\\u000a Usually this happens immediately, however <br>\\u000a if the flag 'applyGradientWhenTensorIsUsed' is set <br>\\u000a to true, then the tensor will only be updated by its <br>\\u000a gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation). <br>\\u000a <br>\\u000a <b> Let's take a look : </b>\\u000a"]
      },
      "blocks":[
        {"kind":"given","text":"We configure Neureka autograd:","code":["Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd","Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse","Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT"]},

        {"kind":"and","text":"","code":["def x = Tsr.of([2, 2], 1d).setRqsGradient(true)","def y = x + 2","Binding binding = new Binding()","binding.setVariable('x', x)","binding.setVariable('y', y)"]},

        {"kind":"when","text":"The code snippet is being execute...","code":["Tsr z = new GroovyShell(binding).evaluate((code))"]},

        {"kind":"then","text":"As expected, this new tensor contains four times 27 :","code":["z.toString().contains(\"(2x2):[27.0, 27.0, 27.0, 27.0]\")"]},

        {"kind":"when","text":"\u000a We call the \"mean()\" method as a simple loss function!\u000a This produces a scalar output tensor which is ideal as entrypoint\u000a for the autograd algorithm.\u000a","code":["def result = z.mean()"]},

        {"kind":"then","text":"This \"result\" tensor will be the expected scalar :","code":["result.toString().contains(\"(1x1):[27.0]\")"]},

        {"kind":"when","text":"\u000a We now try to backpropagate! Because \"result\" contains a single scalar,\u000a result.backward() is equivalent to out.backward(Tsr.of(1)).\u000a","code":["z.backward(0.25)","def xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterBack )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterUse )"]},

        {"kind":"when","text":"","code":["x.setGradientApplyRequested( true )","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterRqd )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterAll )"]},

        {"kind":"when","text":"It is time to free some memory because our history of computation has grown a bit...","code":["result.unsafe.detach()"]},

        {"kind":"then","text":"Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !","code":["!result.has( GraphNode.class )"]},

        {"kind":"where","text":"","code":{"code":["y*y*3"],"whenRsd":["false"],"whenUse":["true"],"doJIT":["true"],"afterBack":[".*1.*null.*"],"afterUse":[".*5\\.5.*null.*"],"afterRqd":[".*5\\.5.*null.*"],"afterAll":[".*5\\.5.*null.*"]}}
      ],
      "problems":"[]"
    },
  
    {
      "id":"Advanced backpropagation on all AD-Modes [code: y*y*3, whenRsd: true, whenUse: true, doJIT: true, afterBack: .*1.*null.*, afterUse: .*1.*null.*, afterRqd: .*1.*null.*, afterAll: .*5\\.5.*null.*, #7]",
      "result":"PASS",
      "duration":"0.070 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":["\\u000a What is JIT-Prop and how does it affect autograd ? <b> Let's take a look ! </b> <br>\\u000a This run covers the feature having the following settings : <br>\\u000a <br>\\u000a <b> Neureka.instance().settings().autograd().: </b> <br>\\u000a isRetainingPendingErrorForJITProp := true <br> \\u000a isApplyingGradientWhenTensorIsUsed := true <br> \\u000a isApplyingGradientWhenRequested := true <br> \\u000a <br>\\u000a ...code producing the result : 'y*y*3' <br> \\u000a <br> \\u000a <b> is-Retaining-Pending-Error-For-JITProp : </b> <br>\\u000a <br>\\u000a This flag enables an optimization technique which only propagates error values to <br>\\u000a gradients if needed by a tensor (the tensor is used again) and otherwise accumulate them <br>\\u000a at divergent differentiation paths within the computation graph. <br>\\u000a If the flag is set to true <br>\\u000a then error values will accumulate at such junction nodes. <br>\\u000a This technique however uses more memory but will <br>\\u000a improve performance for some networks substantially. <br>\\u000a The technique is termed JIT-Propagation. <br>\\u000a <br>\\u000a <br>\\u000a <b> is-Applying-Gradient-When-Tensor-Is-Used : </b> <br>\\u000a <br>\\u000a Gradients will automatically be applied (or JITed) to tensors as soon as <br>\\u000a they are being used for calculation (GraphNode instantiation). <br>\\u000a This feature works well with JIT-Propagation. <br>\\u000a <br>\\u000a <br> \\u000a <b> is-Applying-Gradient-When-Requested : </b> <br>\\u000a <br>\\u000a Gradients will only be applied if requested. <br>\\u000a Usually this happens immediately, however <br>\\u000a if the flag 'applyGradientWhenTensorIsUsed' is set <br>\\u000a to true, then the tensor will only be updated by its <br>\\u000a gradient if requested AND the tensor is used fo calculation! (GraphNode instantiation). <br>\\u000a <br>\\u000a <b> Let's take a look : </b>\\u000a"]
      },
      "blocks":[
        {"kind":"given","text":"We configure Neureka autograd:","code":["Neureka.get().settings().autograd().isApplyingGradientWhenRequested = whenRsd","Neureka.get().settings().autograd().isApplyingGradientWhenTensorIsUsed = whenUse","Neureka.get().settings().autograd().isRetainingPendingErrorForJITProp = doJIT"]},

        {"kind":"and","text":"","code":["def x = Tsr.of([2, 2], 1d).setRqsGradient(true)","def y = x + 2","Binding binding = new Binding()","binding.setVariable('x', x)","binding.setVariable('y', y)"]},

        {"kind":"when","text":"The code snippet is being execute...","code":["Tsr z = new GroovyShell(binding).evaluate((code))"]},

        {"kind":"then","text":"As expected, this new tensor contains four times 27 :","code":["z.toString().contains(\"(2x2):[27.0, 27.0, 27.0, 27.0]\")"]},

        {"kind":"when","text":"\u000a We call the \"mean()\" method as a simple loss function!\u000a This produces a scalar output tensor which is ideal as entrypoint\u000a for the autograd algorithm.\u000a","code":["def result = z.mean()"]},

        {"kind":"then","text":"This \"result\" tensor will be the expected scalar :","code":["result.toString().contains(\"(1x1):[27.0]\")"]},

        {"kind":"when","text":"\u000a We now try to backpropagate! Because \"result\" contains a single scalar,\u000a result.backward() is equivalent to out.backward(Tsr.of(1)).\u000a","code":["z.backward(0.25)","def xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterBack )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterUse )"]},

        {"kind":"when","text":"","code":["x.setGradientApplyRequested( true )","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterRqd )"]},

        {"kind":"when","text":"","code":["x * 2","xAsStr = x.toString()"]},

        {"kind":"then","text":"The variable \"x\" contains every expected String :","code":["xAsStr.matches( afterAll )"]},

        {"kind":"when","text":"It is time to free some memory because our history of computation has grown a bit...","code":["result.unsafe.detach()"]},

        {"kind":"then","text":"Our latest tensor will now longer have a strong reference to a soon to be garbage collected past !","code":["!result.has( GraphNode.class )"]},

        {"kind":"where","text":"","code":{"code":["y*y*3"],"whenRsd":["true"],"whenUse":["true"],"doJIT":["true"],"afterBack":[".*1.*null.*"],"afterUse":[".*1.*null.*"],"afterRqd":[".*1.*null.*"],"afterAll":[".*5\\.5.*null.*"]}}
      ],
      "problems":"[]"
    },
  
    {
      "id":"We can create a shallow copy of a tensor detached from the computation graph.",
      "result":"PASS",
      "duration":"0.034 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":[]
      },
      "blocks":[
        {"kind":"given","text":"","code":["var a = Tsr.ofFloats().withShape(2).andFill(-3, 1).setRqsGradient(true)"]},

        {"kind":"when","text":"","code":["var b = a * 2","var c = b.detached()"]},

        {"kind":"then","text":"","code":["c !== b","c === c.detached()"]},

        {"kind":"and","text":"","code":["b.isBranch()","!c.isBranch()"]},

        {"kind":"and","text":"","code":["b.has(GraphNode)","!c.has(GraphNode)"]}
      ],
      "problems":"[]"
    }
  
  ],
  "generator":"https://github.com/renatoathaydes/spock-reports"
}