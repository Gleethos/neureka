<!DOCTYPE html><html>
<head>
<meta http-equiv='Content-Type' content='text/html; charset=utf-8'></meta>
<link rel="stylesheet" href="../css/custom.css">
</head>
<body>
<h2>Report for it.autograd.Autograd_Tensor_Integration_Tests</h2>
<hr></hr>
<div class='back-link'>
<a href='index.html'>&lt;&lt; Back</a>
</div>
<div class='summary-report'>
<h3>Summary:</h3>
<div class='date-test-ran'>Created on Tue Sep 01 23:06:37 CEST 2020 by Daniel</div>
<table class='summary-table'>
<thead>
<th>Executed features</th>
<th>Failures</th>
<th>Errors</th>
<th>Skipped</th>
<th>Success rate</th>
<th>Time</th>
</thead>
<tbody>
<tr>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>100,0%</td>
<td>11,130 seconds</td>
</tr>
</tbody>
</table>
</div>
<h3>Features:</h3>
<table class='features-table'>
<colgroup>
<col class='block-kind-col'></col>
<col class='block-text-col'></col>
</colgroup>
<tbody>
<ul id='toc'>
<li>
<a href='#-2033378661' class='feature-toc-pass'>Test basic autograd behaviour. (Not on device)</a>
</li>
<li>
<a href='#2117392955' class='feature-toc-pass'>Second-Test "x-mul" autograd behaviour. (Not on device)</a>
</li>
<li>
<a href='#701942685' class='feature-toc-pass'>A tensor used as derivative within a computation graph will throw exception when trying to deleting it.</a>
</li>
</ul>
<tr>
<td colspan='10'>
<div class='feature-description' id='-2033378661'>
<span>Test basic autograd behaviour. (Not on device)</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>The Neurka instance is being reset.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.instance().reset()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>Gradient auto apply for tensors in ue is set to false.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.instance().settings().autograd().setIsApplyingGradientWhenTensorIsUsed(false)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>Tensor legacy view is set to true.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.instance().settings().view().setIsUsingLegacyView(true)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>Three scalar tensors "x", "b", "w" are being instantiated, and "x" requires gradients.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr x = new Tsr(new int[]{1}, 3).setRqsGradient(true)
Tsr b = new Tsr(new int[]{1}, -4)
Tsr w = new Tsr(new int[]{1}, 2)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>A new tensor is being calculated by the equation "((i0+i1)*i2)^2".</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr y = new Tsr(new Tsr[]{x, b, w}, "((i0+i1)*i2)^2")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The resulting tensor should contain "[1]:(4.0); -&gt;d[1]:(-8.0), " where the last part is a derivative.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.toString().contains("[1]:(4.0); -&gt;d[1]:(-8.0), ")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We call the "backward" method on this tensor...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.backward(new Tsr(2))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The source tensor which requires gradients will have the gradient "-16".</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>x.toString().contains("-16.0")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We create a new tensor via the same equation but applied in a different way...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y = new Tsr("(","(",x,"+",b,")","*",w,")^2")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The will produce the same result once again.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.toString().contains("[1]:(4.0); -&gt;d[1]:(-8.0), ")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>Whe also call the "backward" method again...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.backward(new Tsr(1))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>Then the accumulated gradient in the source tensor which requires gradients will be as expected.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>x.toString().contains("-24.0")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We execute the same equation once more...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y = new Tsr("((",x,"+",b,")*",w,")^2")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>The result will be as expected.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.toString().contains("[1]:(4.0); -&gt;d[1]:(-8.0), ")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>We call "backward" with -1 as error...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>y.backward(new Tsr(-1))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>This will change the gradient of "x" accordingly.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>x.toString().contains("-16.0")</pre>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='2117392955'>
<span>Second-Test "x-mul" autograd behaviour. (Not on device)</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>The Neurka instance is being reset.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.instance().reset()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>Gradient auto apply for tensors in ue is set to false.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.instance().settings().autograd().setIsApplyingGradientWhenTensorIsUsed(false)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>Tensor legacy view is set to true.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.instance().settings().view().setIsUsingLegacyView(true)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>The Neureka instance is set to legacy indexing.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Neureka.instance().settings().indexing().setIsUsingLegacyIndexing(true)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>def x = new Tsr(
        new int[]{3, 3},
        new double[]{
                1, 2, 5,
                -1, 4, -2,
                -2, 3, 4,
        }
)
def y = new Tsr(
new int[]{2, 2},
new double[]{
        -1, 3,
        2, 3,
}).setRqsGradient(true)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>y.toString().contains(":g:(null)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>def z = new Tsr(new Tsr[]{x, y}, "I0xi1")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>z.toString().contains("[2x2]:(15.0, 15.0, 18.0, 8.0)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>z = new Tsr(new Object[]{x, "x", y})</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>z.toString().contains("[2x2]:(15.0, 15.0, 18.0, 8.0)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>z.backward(new Tsr(new int[]{2, 2}, 1))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>y.toString().contains("[2x2]:(-1.0, 3.0, 2.0, 3.0):g:(6.0, 9.0, 4.0, 9.0)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>Neureka.instance().settings().indexing().setIsUsingLegacyIndexing(false)
x = new Tsr(
        new int[]{3, 3},
        new double[]{
                1, 2, 5,
                -1, 4, -2,
                -2, 3, 4,
        }
)
y = new Tsr(
new int[]{2, 2},
new double[]{
        -1, 3,
        2, 3,
}).setRqsGradient(true)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>y.toString().contains(":g:(null)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>z = new Tsr(new Tsr[]{x, y}, "I0xi1")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>z.toString().contains("[2x2]:(15.0, 15.0, 18.0, 8.0)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>z = new Tsr(new Object[]{x, "x", y})</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>z.toString().contains("[2x2]:(15.0, 15.0, 18.0, 8.0)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>z.backward(new Tsr(new int[]{2, 2}, 1))</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>y.toString().contains("[2x2]:(-1.0, 3.0, 2.0, 3.0):g:(6.0, 9.0, 4.0, 9.0)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x = new Tsr(new int[]{1}, 3)
Tsr b = new Tsr(new int[]{1}, -5)
Tsr w = new Tsr(new int[]{1}, -2)
z = new Tsr(new Tsr[]{x, b, w}, "I0*i1*i2")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>z.toString().contains("[1]:(30.0)")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<pre class='block-source'>x = new Tsr(new int[]{1}, 4).setRqsGradient(true)
b = new Tsr(new int[]{1}, 0.5)
w = new Tsr(new int[]{1}, 0.5)
y = new Tsr(new Tsr[]{x, b, w}, "(2^i0^i1^i2^2")</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<pre class='block-source'>y.toString().contains("[1]:(4.0);")
y.toString().contains(" -&gt;d[1]:(1.38629E0), ")</pre>
</td>
</tr>
<tr>
<td colspan='10'>
<div class='feature-description' id='701942685'>
<span>A tensor used as derivative within a computation graph will throw exception when trying to deleting it.</span>
<span style='float: right; font-size: 60%;'>
<a href='#toc'>Return</a>
</span>
</div>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Given:</div>
</td>
<td>
<div class='block-text'>A tensor "a" requiring autograd.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr a = new Tsr(1).setRqsGradient(true)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>A second tensor "b".</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr b = new Tsr(2)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>When:</div>
</td>
<td>
<div class='block-text'>Both tensors are being multiplied.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>Tsr c = a.dot(b)</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>And:</div>
</td>
<td>
<div class='block-text'>One tries to delete tensor "b"...</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>b.delete()</pre>
</td>
</tr>
<tr>
<td>
<div class='block-kind'>Then:</div>
</td>
<td>
<div class='block-text'>An exception is being thrown.</div>
</td>
</tr>
<tr>
<td></td>
<td>
<pre class='block-source'>def exception = thrown(IllegalStateException)
exception.message == "Cannot delete a tensor which is used as derivative by the AD computation graph!"</pre>
</td>
</tr>
</tbody>
</table>
<hr></hr>
<div class='footer'>Generated by <a href='https://github.com/renatoathaydes/spock-reports'>Athaydes Spock Reports</a></div>
</body>
</html>