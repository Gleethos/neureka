{
  "className":"st.Broad_System_Test",
  "title":"",
  "narrative":"",
  "subjects":[],
  "statistics":{
    "runs":"4",
    "successRate":"100.0%",
    "failures":"0",
    "errors":"0",
    "skipped":"0",
    "duration":"0.083 seconds"
  },
  "headers":[],"tags":{},"see":[],
  "features":[ 
    {
      "id":"The long broad integration test runs successfully.",
      "result":"PASS",
      "duration":"0.040 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":[]
      },
      "blocks":[
        {"kind":"expect","text":"The integration test runs without exceptions or assertion errors.","code":["BroadSystemTest.on() // This is the actual test."]}
      ],
      "problems":{"dataValues":[], "errors":[]}
    },
  
    {
      "id":"A function with expression \"softplus((I[0]xI[1])*-100)\" can be backpropagated.",
      "result":"PASS",
      "duration":"0.011 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":[]
      },
      "blocks":[
        {"kind":"given","text":"","code":["Neureka.get().settings().view().getNDPrintSettings().setIsLegacy(true);","Tensor<Double> tensor1 = Tensor.of(Shape.of(1, 3), 2d);","Tensor<Double> tensor2 = Tensor.of(Double).withShape(2, 1).all(-1.0);","tensor1.setRqsGradient(true);","tensor2.setRqsGradient(true);"]},

        {"kind":"when","text":"","code":["Tensor<Double> result1 = Tensor.of(\"softplus((I[0]xI[1])*-100)\", [tensor1, tensor2]);","Tensor<Double> result2 = (Tensor.of(\"i0 x i1\", tensor1, tensor2)*-100).softplus();"]},

        {"kind":"then","text":"","code":["result1.toString() == \"[2x3]:(200.0, 200.0, 200.0, 200.0, 200.0, 200.0); ->d[2x3]:(-100.0, -100.0, -100.0, -100.0, -100.0, -100.0)\"","result2.toString() == \"[2x3]:(200.0, 200.0, 200.0, 200.0, 200.0, 200.0); ->d[2x3]:(-100.0, -100.0, -100.0, -100.0, -100.0, -100.0)\""]},

        {"kind":"when","text":"We perform a backwards pass of a gradient of `-0.1`:","code":["result1.backward( -0.1 );"]},

        {"kind":"then","text":"","code":["tensor1.gradient.get().toString() == \"[1x3]:(-20.0, -20.0, -20.0)\"","tensor2.gradient.get().toString() == \"[2x1]:(60.0, 60.0)\""]},

        {"kind":"when","text":"We perform a backwards pass of a gradient of `-0.1`:","code":["result2.backward( -0.1 );"]},

        {"kind":"then","text":"","code":["tensor1.gradient.get().toString() == \"[1x3]:(-40.0, -40.0, -40.0)\"","tensor2.gradient.get().toString() == \"[2x1]:(120.0, 120.0)\""]}
      ],
      "problems":{"dataValues":[], "errors":[]}
    },
  
    {
      "id":"A function with expression \"softplus(tanh(I[0]*I[1]*2)*I[1])\" can be backpropagated.",
      "result":"PASS",
      "duration":"0.016 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":[]
      },
      "blocks":[
        {"kind":"given","text":"","code":["Neureka.get().settings().view().getNDPrintSettings().setIsLegacy(true);","Tensor<Double> tensor1 = Tensor.of(Shape.of(2), 2d);","Tensor<Double> tensor2 = Tensor.of(Shape.of(2), 4d);","tensor1.setRqsGradient(true);","tensor2.setRqsGradient(true);"]},

        {"kind":"when","text":"","code":["Tensor<Double> result1 = Tensor.of(\"softplus(tanh(I[0]*I[1]*2)*I[1])\", [tensor1, tensor2]);","Tensor<Double> result2 = ((tensor1 * tensor2 * 2).tanh()*tensor2).softplus();"]},

        {"kind":"then","text":"","code":["result1.toString({it.hasDerivatives=false}) == \"[2]:(4.01815, 4.01815)\"","result2.toString({it.hasDerivatives=false}) == \"[2]:(4.01815, 4.01815)\""]},

        {"kind":"when","text":"We perform a backwards pass of a gradient of `100`:","code":["result1.backward( 100 );"]},

        {"kind":"then","text":"","code":["tensor1.gradient.get().toString() == \"[2]:(159.09e-12, 159.09e-12)\"","tensor2.gradient.get().toString() == \"[2]:(98.2014, 98.2014)\""]},

        {"kind":"when","text":"We perform a backwards pass of a gradient of `100`:","code":["result2.backward( 100 );"]},

        {"kind":"then","text":"","code":["tensor1.gradient.get().toString() == \"[2]:(318.18e-12, 318.18e-12)\"","tensor2.gradient.get().toString() == \"[2]:(196.403, 196.403)\""]}
      ],
      "problems":{"dataValues":[], "errors":[]}
    },
  
    {
      "id":"A function with expression \"(-3*(2*(i0*-1)))*(-1*i0)\" can be backpropagated.",
      "result":"PASS",
      "duration":"0.010 seconds",
      "iterations":{
      "tags":{},"see":[],"extraInfo":[]
      },
      "blocks":[
        {"kind":"given","text":"","code":["Neureka.get().settings().view().getNDPrintSettings().setIsLegacy(true);","Tensor<Double> tensor1 = Tensor.of(Shape.of(1), 2d);//-2*4 = 8 | *3 = -24","tensor1.setRqsGradient(true);"]},

        {"kind":"when","text":"","code":["Tensor<Double> result1 = Tensor.of(\"(-3*(2*(i0*-1)))*(-1*i0)\", [tensor1]);","Tensor<Double> result2 = ((((tensor1*-1)*2)*-3)*(tensor1*-1));"]},

        {"kind":"then","text":"","code":["result1.toString({it.hasDerivatives=false}) == \"[1]:(-24.0)\"","result2.toString({it.hasDerivatives=false}) == \"[1]:(-24.0)\""]},

        {"kind":"when","text":"We perform a backwards pass of a gradient of `2`:","code":["result1.backward( 2 );"]},

        {"kind":"then","text":"","code":["tensor1.gradient.get().toString() == \"[1]:(-48.0)\""]},

        {"kind":"when","text":"We perform a backwards pass of a gradient of `2`:","code":["result2.backward( 2 );"]},

        {"kind":"then","text":"","code":["tensor1.gradient.get().toString() == \"[1]:(-96.0)\""]}
      ],
      "problems":{"dataValues":[], "errors":[]}
    }
  
  ],
  "generator":"https://github.com/renatoathaydes/spock-reports"
}
