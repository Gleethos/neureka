{"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":250},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":51},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":291},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1374},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":275},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":44},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1938},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":33},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":248},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1074},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":29},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":838},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6435},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":318},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":196},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":168},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":504},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":90},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":7008},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":117},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":51},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":106},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1482},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1683},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":328},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":97},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2020},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":218},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":41},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5947},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":648},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":135},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":339},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1204},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":57},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":91},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":228},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":18},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3865},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":534},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":875},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7246},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":166},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":709},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10375},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":838},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":136},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":273},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7264},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":718},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2600},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":409},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2328},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":347},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":101},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":590},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":158},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":333},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11056},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25869},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":634},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":364},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":43},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":68},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":162},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2659},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":46},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":62},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":234},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":511},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":712},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":218},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":69},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1705},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":187},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":37},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1117},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6839},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":215},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":268},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":171},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":414},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":171},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5992},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":44},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":138},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":151},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":681},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2147},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":358},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":121},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1783},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":29},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":272},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":699},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5017},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":86},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":563},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":178},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":268},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1155},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":55},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":85},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":245},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":25},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3280},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":288},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":744},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7558},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":190},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":621},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":954},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":337},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1325},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":132},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":64},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":434},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":174},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":269},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":109},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":13452},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9135},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":730},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":90},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":289},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7338},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":721},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":37},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24445},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":553},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":294},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":36},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":55},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":135},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2586},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":42},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":48},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":207},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":136},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":92},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":78},"title":"What it means to be N-Dimensional","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":343},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":142},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1920},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":840},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2298},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1341},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":53},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":244},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7506},"title":"","narrative":""},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":422},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2253},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":164},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2337},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":412},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":741},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":280},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":581},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":136},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2517},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1238},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4484},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":483},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":224},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":105},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":256},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":93},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5390},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":128},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":48},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":178},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1324},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":684},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":313},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1917},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":236},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":59},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":200},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":928},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1736},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":304},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":85},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2251},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":31},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":311},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":854},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4696},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":550},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":181},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":374},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1381},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":84},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":121},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":303},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":61},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":890},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":97},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":284},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":10892},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":281},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1086},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8890},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":880},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":143},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":416},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":40},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7504},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":724},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3070},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":170},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2033},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":202},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":118},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":511},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":225},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":250},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11124},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25722},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":583},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":302},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":37},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":62},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":136},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2545},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":44},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":11},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":217},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":964},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4918},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":457},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":120},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":135},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":329},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":95},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6266},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":134},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":59},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":131},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":889},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1583},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":308},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":62},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1972},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":185},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":57},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":21},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":158},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":970},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1995},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":458},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":79},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2176},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":26},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":303},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":753},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5515},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":54},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":549},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":157},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":282},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1379},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":41},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":132},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":247},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":18},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":842},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":127},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":909},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":11850},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":185},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":763},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9546},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":770},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":149},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":412},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":42},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7359},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":732},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":60},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3194},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":233},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1885},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":315},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":69},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":640},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":177},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":275},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":74},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":30},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10938},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27265},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":574},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":312},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":41},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":58},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2497},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":44},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":261},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1002},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5888},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":283},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":184},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":168},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":332},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":110},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5946},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":128},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":46},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":301},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1286},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1433},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":436},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":189},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2193},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":244},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":43},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":181},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":747},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2046},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":563},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":88},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2124},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":296},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":759},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5357},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":69},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":657},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":218},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":364},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1322},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":76},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":113},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":254},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3824},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":347},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1049},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7531},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":156},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":616},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7479},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":969},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":157},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":448},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":61},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8678},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":33},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":788},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":40},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2461},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":177},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2705},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":282},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":112},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":536},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":188},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":333},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":72},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10749},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25646},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":544},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":333},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":40},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":52},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":148},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2552},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":48},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":59},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":281},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":319},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":387},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":42},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1873},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1364},"title":"","narrative":""},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":458},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":208},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":151},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6743},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":134},"title":"","narrative":""},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":231},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":48},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1231},"title":"","narrative":""},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":254},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3662},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":310},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10831},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":744},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":263},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2586},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1405},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5113},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":337},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":145},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":144},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":400},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":102},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6230},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":168},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":47},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":193},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1352},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":997},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":392},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":96},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2210},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":186},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":42},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":20},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":71},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":774},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2229},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":416},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":165},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2246},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":46},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":282},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":788},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":20},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4774},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":614},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":157},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":318},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1382},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":48},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":143},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":318},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":18},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2830},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":338},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":969},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7808},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":189},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":636},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7363},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1146},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":190},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":479},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":85},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8368},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":754},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2204},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":186},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2644},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":316},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":96},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":608},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":143},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":278},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":63},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10648},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25504},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":584},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":307},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":36},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":65},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":128},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2506},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":44},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":252},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":907},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":117},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":29},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":566},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":990},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7830},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":246},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":215},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":169},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":375},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":78},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":56},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":599},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1919},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1808},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":55},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2094},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":54},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":439},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1022},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1366},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":828},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":308},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":65},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1869},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":183},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":28},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":6147},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":40},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":622},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":331},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":210},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1159},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":61},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":94},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":241},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":26},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3628},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":297},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":825},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7649},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":165},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":635},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10058},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":807},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":147},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":320},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":33},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7448},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":726},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":58},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2726},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":187},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2127},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":303},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":90},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":711},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":165},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":252},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":76},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10982},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24698},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":599},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":301},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":39},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":56},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":132},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2535},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":44},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":249},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":772},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5052},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":540},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":161},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":125},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":335},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":94},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5986},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":227},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":43},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":379},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":938},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1157},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":335},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1989},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":216},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":53},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1038},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":58},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":608},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":281},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":559},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1663},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":86},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":109},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":309},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":56},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":83},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":826},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2053},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":568},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":91},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2116},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":45},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":377},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":736},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3248},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":238},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1133},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7502},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":217},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":606},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7686},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":994},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":121},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":403},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":57},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8117},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":707},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":41},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2813},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":183},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2277},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":319},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":77},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":516},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":188},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":300},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":90},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10339},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":23412},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":14},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":608},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":310},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":40},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":60},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":130},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2543},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":42},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":12},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":218},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1451},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5216},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":632},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":158},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":149},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":477},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":111},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6306},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":229},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":65},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":154},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1470},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":915},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":306},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":68},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2229},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":178},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":57},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":69},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":834},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2526},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":547},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":78},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2454},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":31},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":336},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":794},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4622},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":711},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":217},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":262},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1505},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":63},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":101},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":317},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":29},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3892},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":465},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":673},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7599},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":176},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":589},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7591},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1004},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":182},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":555},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":71},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8686},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":728},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":63},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3040},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":297},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2101},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":288},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":92},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":657},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":174},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":263},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":65},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10712},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27183},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":14},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":552},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":307},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":37},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":73},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":144},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2550},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":42},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":10},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":225},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":919},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6844},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":494},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":305},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":155},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":289},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":101},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5372},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":186},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":64},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":164},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1537},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1398},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":270},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":87},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1845},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":166},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":45},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":24},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2439},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":709},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":227},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":337},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1463},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":65},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":147},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":414},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":66},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":148},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":628},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2285},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":380},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":112},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1945},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":37},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":356},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":822},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2887},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":397},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":593},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7866},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":215},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":658},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9559},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":782},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":162},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":535},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":68},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7476},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":734},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":41},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":2076},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":336},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2254},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":379},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":113},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":546},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":138},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":260},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":81},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11395},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27963},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":613},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":320},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":41},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":61},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":137},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2582},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":48},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":11},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":260},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":863},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":223},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":226},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1409},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":76},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":90},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":247},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4389},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":83},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":588},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1055},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2073},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":260},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":86},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1853},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":172},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":38},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5280},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1553},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":82},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":118},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":23},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":91},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":30},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1254},"title":"Cross Device Tensor Slicing","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":233},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1931},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":451},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1612},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":29},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":215},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":565},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":481},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2664},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1019},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":124},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":278},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":512},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3831},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":59},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":68},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":13},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":294},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3110},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":36},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2024},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":208},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":342},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":5558},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":104},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":554},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":763},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2237},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":359},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":231},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":539},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":124},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":234},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":91},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10137},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24968},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2231},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":588},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":46},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":193},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5681},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":605},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":34},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":921},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5771},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":367},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":252},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":404},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":109},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3244},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":175},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":62},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":287},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1172},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1396},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":280},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":82},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2087},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":210},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":38},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":83},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":712},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2229},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":365},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":89},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2394},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":42},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":335},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":857},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5323},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":587},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":214},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":302},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1364},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":63},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":160},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":280},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":25},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":69},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":841},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":310},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":414},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":9984},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":406},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":946},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9295},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":800},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":145},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":350},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":57},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7596},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":45},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":749},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":62},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2201},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":197},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1987},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":277},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":78},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":710},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":176},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":252},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":88},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11568},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25647},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":14},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":558},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":316},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":52},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":56},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":131},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2494},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":43},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":54},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":232},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":831},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6360},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":410},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":239},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":197},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":538},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":95},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5204},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":175},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":47},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":127},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1150},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1970},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":308},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":120},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1930},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":271},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":43},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":73},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":500},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2270},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":463},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":114},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2247},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":55},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":474},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":883},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5713},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":492},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":188},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":315},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1247},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":42},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":108},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":246},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":54},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2545},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":353},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":945},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":8040},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":248},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":632},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":821},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":231},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1337},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":215},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":82},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":508},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":96},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":228},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":15078},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9865},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":865},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":176},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":285},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7824},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":719},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":65},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26470},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":562},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":318},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":39},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":78},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":133},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2559},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":44},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":10},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":291},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":463},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1021},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":221},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":63},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1485},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":162},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":35},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5218},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":97},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":314},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":355},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1667},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":346},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":349},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2138},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":131},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":116},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":294},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":34},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6944},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1221},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":66},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":106},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":30},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":100},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":34},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5141},"title":"Cross Device Tensor Slicing","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":351},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2026},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":193},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1385},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":37},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":162},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":591},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":501},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1821},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1357},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":122},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":353},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":454},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4317},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":78},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":71},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":31},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":457},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3006},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":83},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2497},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":296},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":330},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":6602},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":112},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":612},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1028},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2009},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":305},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":367},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":603},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":247},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":323},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":86},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11463},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26545},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2215},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":614},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":45},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":188},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":6054},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":631},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":35},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":58},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":721},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":161},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":17421},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":215},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2112},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":193},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":187},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":191},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":77},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":120},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":148},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":576},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":45},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":27},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":99},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":126},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":414},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":32},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":103},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":85},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":228},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4073},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2464},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":34},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":207},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5347},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":23},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":29},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":475},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":14},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":632},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":45},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":324},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7646},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":38},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":25},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":153},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":420},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":491},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":5,"passed":1,"successRate":1.0,"time":3787},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":54},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":748},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":161},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":15804},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":240},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2053},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":186},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      <br>\n     The autograd package provides automatic differentiation for all default operations on Tensors.          <br>\n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             <br>\n     your code is run, and that every single iteration can be different.                                     <br>\n                                                                                                             <br>\n     The class neureka.Tsr is the central class of the main package.                                         <br>\n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           <br>\n     When you finish the forward pass of your network                                                        <br>\n     you can call .backward() and have all the gradients computed                                            <br>\n     and distributed to the tensors requiring them automatically.                                            <br>\n                                                                                                             <br>\n     <br>                                                                                                    <br>\n     The gradient for a tensor will be accumulated into a child tensor (component) which                     <br>\n     can be accessed via the '.getGradient()' method.                                                        <br>\n                                                                                                             <br>\n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  <br>\n     computation history, and to prevent future computation from being tracked.                              <br>\n     <br>"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":122},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":126},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":188},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":23},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":159},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":89},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":129},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":152},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":590},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":47},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":27},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":28},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":91},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":130},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":351},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":36},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":99},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":174},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":207},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3979},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2399},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":33},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":205},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5299},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":30},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":472},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":15},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":604},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":39},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":312},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7503},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":31},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":23},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":149},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":389},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":495},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":51},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":689},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":141},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":17652},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":207},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1979},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":187},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n     The autograd package provides automatic differentiation for all default operations on Tensors.          \n     Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n     your code is run, and that every single iteration can be different.                                     \n                                                                                                             \n     The class neureka.Tsr is the central class of the main package.                                         \n     If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n     When you finish the forward pass of your network                                                        \n     you can call .backward() and have all the gradients computed                                            \n     and distributed to the tensors requiring them automatically.                                            \n                                                                                                             \n     The gradient for a tensor will be accumulated into a child tensor (component) which                     \n     can be accessed via the '.getGradient()' method.                                                        \n                                                                                                             \n     To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n     computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":122},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":184},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":22},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":150},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":76},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":80},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":124},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":148},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":557},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":28},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":105},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":131},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":402},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":33},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":83},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":81},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":217},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3947},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2420},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":33},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":194},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5321},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":22},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":10},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":33},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":498},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":14},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":629},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":43},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":308},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7667},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":32},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":25},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":165},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":396},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":465},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":59},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":762},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":162},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16748},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":211},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2045},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":191},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":107},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":127},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":190},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":151},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":78},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":79},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":125},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":154},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":568},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":29},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":97},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":145},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":423},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":108},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":84},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":208},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4073},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2463},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":31},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":203},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":58},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5362},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":28},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":492},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":15},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":632},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":42},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":319},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7592},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":36},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":25},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":153},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":412},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":474},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":53},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":687},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":139},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16553},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":213},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2020},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":46},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":185},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":105},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":125},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":193},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":158},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":72},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":76},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":144},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":554},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":27},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":23},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":90},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":125},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":382},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":30},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":84},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":85},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":199},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3958},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2416},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":32},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":203},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5325},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":30},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":492},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":15},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":615},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":43},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":313},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7558},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":30},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":25},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":150},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":387},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":450},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":58},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":688},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":237},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16367},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":206},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2008},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":199},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":121},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":119},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":195},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":161},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":79},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":115},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":142},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":568},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":44},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":28},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":85},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":131},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":379},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":29},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":89},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":82},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":205},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4056},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2397},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":31},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":187},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5447},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":54},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":29},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":496},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":14},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":617},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":40},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":318},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7626},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":33},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":27},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":155},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":417},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":457},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":54},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":726},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":141},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16045},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":215},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2018},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":181},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":120},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":176},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":177},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":71},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":147},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":563},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":26},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":97},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":127},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":398},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":31},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":86},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":81},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":205},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3927},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2398},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":34},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":192},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5370},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":29},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":504},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":15},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":617},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":38},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":330},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7470},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":35},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":24},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":187},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":416},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":463},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":53},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":681},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":143},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":17863},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":227},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2029},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":42},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":187},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":115},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":170},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":172},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":77},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":107},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":137},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":550},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":42},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":26},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":89},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":128},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":372},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":31},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":80},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":78},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":204},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3935},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2391},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":30},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":198},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5226},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":32},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":479},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":14},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":603},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":36},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":315},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7443},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":31},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":25},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":150},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":392},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":463},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":54},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":646},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":139},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":15796},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":209},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1978},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":46},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":182},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":120},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":180},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":162},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":74},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":117},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":144},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":554},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":27},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":23},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":90},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":137},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":386},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":30},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":81},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":80},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":204},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3909},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2371},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":33},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":202},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5270},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":22},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":29},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":520},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":14},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":623},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":41},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":314},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7553},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":29},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":24},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":137},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":393},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":457},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":57},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":704},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":143},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16204},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":208},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2127},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":186},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":104},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":115},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":181},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":155},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":77},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":72},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":150},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":554},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":27},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":93},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":391},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":33},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":84},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":81},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":201},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3900},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2396},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":52},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":195},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5339},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":30},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":23},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":539},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":15},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":638},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":39},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":324},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7702},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":32},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":25},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":156},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":382},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":458},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":53},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":718},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":139},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16429},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":194},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2037},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":179},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":97},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":113},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":186},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":152},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":72},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":69},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":121},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":149},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":551},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":26},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":22},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":98},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":125},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":389},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":29},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":82},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":77},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":194},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3878},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2380},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":53},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":207},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":53},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5273},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":29},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":23},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":538},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":14},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":629},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":38},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":317},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7493},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":29},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":25},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":146},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":380},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":455},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":53},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":681},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":140},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":15911},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":208},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1999},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":179},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":122},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":189},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":152},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":71},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":72},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":117},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":143},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":550},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":40},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":28},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":89},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":124},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":374},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":30},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":81},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":76},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":194},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3891},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2375},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":36},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":193},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5361},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":28},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":492},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":13},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":616},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":3},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":42},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":314},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7549},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":29},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":25},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":137},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":387},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":461},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["\n            When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly.\n            Hi wassup!\n   ","A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":9,"passed":1,"successRate":1.0,"time":3890},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3190},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3083},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3088},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3104},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3099},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3109},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3146},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3128},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3078},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":3054},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":52},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":709},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":141},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16181},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":207},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2136},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":46},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":183},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":94},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":122},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":182},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":149},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":90},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":126},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":143},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":562},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":25},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":92},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":126},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":373},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":37},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":103},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":171},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":200},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4004},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2393},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":32},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":205},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5287},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":29},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":478},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":15},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":601},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":41},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":309},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7495},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":30},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":24},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":158},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":391},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":481},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":71},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":688},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":141},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16146},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":194},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2016},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":183},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":91},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":117},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":179},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":152},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":74},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":120},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":567},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":26},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":87},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":126},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":383},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":30},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":88},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":78},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":210},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3966},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2400},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":33},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":209},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5400},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":27},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":479},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":16},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":626},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":43},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":313},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7582},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":31},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":24},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":146},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":412},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":464},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":57},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":715},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":148},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":16427},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":221},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2110},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":187},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":102},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":113},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":190},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":164},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":70},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":74},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":138},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":46},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":556},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":44},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":25},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":22},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":90},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":122},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":367},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":42},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":82},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":81},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":249},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4045},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2384},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":35},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":201},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":106},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5433},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":53},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":32},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":481},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":13},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":606},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":39},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":325},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":8158},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":29},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":24},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":179},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":498},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":483},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":54},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":689},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":144},"title":"Cross Device Tensor Slicing","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":17929},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":216},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1964},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":184},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":101},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":114},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":185},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":22},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":159},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":74},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":79},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":126},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":151},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":564},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":27},"title":"A Function as such!","narrative":"This specification defines the expected behaviour of the Function API\n    with respect to receiving simple scalar values as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also be fed standalone scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":23},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":89},"title":"Applying Functions on Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":129},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":362},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":29},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":83},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":82},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":211},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":3940},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2401},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":34},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":205},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"What it means to be N-Dimensional","narrative":""},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5251},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":0},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":32},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":474},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":14},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":600},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":37},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":317},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":7442},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":33},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":24},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":164},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":388},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":459},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":799},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5428},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":199},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":235},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":97},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":345},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":89},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6806},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":137},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":48},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":148},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1321},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1742},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":352},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":91},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2289},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":198},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":39},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":165},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":920},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1767},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":480},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":146},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2304},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":54},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":323},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":938},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5993},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":664},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":220},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":515},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1312},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":83},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":108},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":298},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":29},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1637},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":206},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1037},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":10957},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":208},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":838},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10226},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":991},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":139},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":370},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":47},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7847},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":778},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":64},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2402},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":240},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2018},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":390},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":132},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":612},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":167},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":278},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":110},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12423},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27099},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":636},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":308},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":44},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":55},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":133},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2506},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":43},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":55},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":10},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":239},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1099},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5733},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":220},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":267},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":385},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":107},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6696},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":177},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":52},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":161},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":83},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":638},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2785},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":315},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":78},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2791},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":40},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":419},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":841},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":666},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2998},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":294},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":103},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2483},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":235},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":61},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5700},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":696},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":277},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":312},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1211},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":61},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":3,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":5,"successRate":0.625,"time":148},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":227},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3171},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":318},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":770},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":8926},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":187},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":829},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8270},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1041},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":169},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":469},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":42},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":9084},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":48},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":857},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":57},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2143},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":234},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2322},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":324},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":129},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":845},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":203},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":263},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":67},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11772},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27478},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":555},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":310},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":49},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":60},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":156},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2576},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":49},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":53},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":267},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":8,"passed":0,"successRate":0.0,"time":283},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":8,"passed":1,"successRate":1.0,"time":235},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":8,"passed":1,"successRate":1.0,"time":198},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3514},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":202},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":52},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":151},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":835},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":8471},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":325},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":207},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":156},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":311},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":60},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1444},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":968},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":339},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":87},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1895},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":452},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":77},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":179},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":806},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2273},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":495},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":101},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2387},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":54},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":356},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":925},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1693},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":855},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":336},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":323},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1491},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":81},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":189},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":304},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":47},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3119},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":403},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":737},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":9007},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":225},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":713},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9158},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1130},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":133},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":466},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8347},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":803},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":80},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":2649},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":210},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2029},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":232},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":92},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":745},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":220},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":291},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":114},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12258},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26149},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":18},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":648},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":330},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":46},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":64},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":155},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2646},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":51},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":55},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":10},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":240},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5787},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":24},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":91},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1027},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":925},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":323},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":66},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2036},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":300},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":55},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":203},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":1993},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":256},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":347},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2083},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":76},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":166},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":318},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":60},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":2,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":0,"successRate":0.0,"time":770},"title":"Cross Device Tensor Slicing","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":294},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2089},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":380},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":63},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":2,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":2,"successRate":0.5,"time":2042},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":43},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":15,"successRate":0.9375,"time":296},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":1730},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":6037},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1349},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":80},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":196},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":22},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":2219},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":29},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2400},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":326},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":1,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":8,"successRate":0.8888888888888888,"time":357},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":1,"errors":4,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":3,"successRate":0.375,"time":7740},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":138},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":668},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":549},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2203},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1324},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":107},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":278},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":297},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4605},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":88},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":101},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":384},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":557},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2429},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":233},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":348},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":855},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":236},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":270},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":191},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":10,"successRate":0.9090909090909091,"time":12733},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":2,"errors":2,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":1,"successRate":0.2,"time":8648},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":2274},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":818},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":84},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":283},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":27},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7459},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":5,"successRate":0.8333333333333334,"time":808},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":33},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":849},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2222},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1103},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":443},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":81},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":137},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":683},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":179},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":328},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":119},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1250},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":6147},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2225},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":533},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2161},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":314},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":80},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":119},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5340},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1959},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":121},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":225},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":275},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":71},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":227},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5765},"title":"Cross Device Tensor Slicing","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":3566},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1140},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":228},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":51},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":607},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2449},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":8759},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":338},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":249},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":16926},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":176},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":906},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":93},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":170},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":174},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":112},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":5033},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":424},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":417},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":90},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":11475},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":63},"title":"","narrative":""},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":73},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":220},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":25},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9976},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":14632},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":7},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":44},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":112},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":218},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":30180},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":153},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":148},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":410},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":33},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":26},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":11},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5471},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":30},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":123},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1783},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":797},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":324},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":81},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1866},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":205},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":60},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":23},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6984},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1170},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":60},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":146},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":25},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":77},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":239},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2195},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":277},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":349},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1902},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":85},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":155},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":333},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":236},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2589},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":915},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":156},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":474},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":619},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4525},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":69},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":88},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":368},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5384},"title":"Cross Device Tensor Slicing","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":333},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1338},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":273},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1166},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":25},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":187},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":520},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3251},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":44},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2634},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":242},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":378},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":5995},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":140},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":664},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":667},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2287},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":343},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":429},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":637},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":209},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":247},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":100},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11584},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25368},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2452},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":588},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":46},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":189},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":6074},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":601},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":34},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":2,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":3,"successRate":0.6,"time":888},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":64},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":24},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":5,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":4,"successRate":0.4444444444444444,"time":84},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1499},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5451},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":557},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":217},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":180},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":476},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":85},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":2,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":3,"successRate":0.6,"time":781},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":139},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":40},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":5,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":4,"successRate":0.4444444444444444,"time":351},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1423},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4866},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":550},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":212},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":198},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":456},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":76},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1622},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":1192},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":277},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":53},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1981},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":203},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":69},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":2,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":0,"successRate":0.0,"time":2557},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":40},"title":"","narrative":""},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":2,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":0,"successRate":0.0,"time":195},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":292},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1768},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":63},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":141},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":297},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":77},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":148},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":910},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2513},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":317},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":78},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":1,"errors":1,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":2,"successRate":0.5,"time":2276},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":44},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":15,"successRate":0.9375,"time":331},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":582},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":4027},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":331},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":4,"errors":2,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":3,"successRate":0.3333333333333333,"time":693},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":1,"errors":6,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":1,"successRate":0.125,"time":5881},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":233},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":710},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":62},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1314},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":660},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":118},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":109},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":0,"successRate":0.0,"time":275},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3429},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":66},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":65},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":11},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":3,"successRate":0.75,"time":328},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":7993},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1120},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":261},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":436},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":48},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":9074},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":33},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":5,"successRate":0.8333333333333334,"time":874},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3721},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":408},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1762},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":228},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":74},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":567},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":190},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":282},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":113},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":2,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":9,"successRate":0.8181818181818182,"time":11957},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":0,"successRate":0.0,"time":365},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":223},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":1,"successRate":0.3333333333333333,"time":217},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1305},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":92},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":3,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":5,"successRate":0.625,"time":82},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":273},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":828},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6683},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":393},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":259},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":250},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":446},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":106},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":3,"successRate":0.6,"time":7387},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":187},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":39},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":421},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":148},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":943},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2282},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":495},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":83},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":2,"successRate":0.5,"time":2065},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":40},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":285},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":462},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1622},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":806},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":327},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":106},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":4158},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":188},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":37},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3928},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":427},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":1,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":8,"successRate":0.8888888888888888,"time":930},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":6,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":2,"successRate":0.25,"time":10052},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":223},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":671},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":8889},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":956},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":157},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":349},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8263},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":5,"successRate":0.8333333333333334,"time":822},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":40},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1269},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4890},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":397},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":241},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":155},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":307},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":96},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6457},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":218},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":54},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":145},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1219},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1166},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":421},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":89},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2385},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":247},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":58},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5531},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":55},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":656},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":195},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":1,"successRate":0.3333333333333333,"time":309},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1446},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":61},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":3,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":5,"successRate":0.625,"time":95},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":283},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":28},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":127},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":741},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2213},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":607},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":82},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2326},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":39},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":356},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":776},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3656},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":385},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":803},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":7,"successRate":0.875,"time":7923},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":218},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":700},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8333},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":950},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":207},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":481},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":9440},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":829},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":65},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1985},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":242},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2173},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":305},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":121},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":731},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":203},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":330},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":89},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":33},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12172},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27793},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":550},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":306},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":42},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":52},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":124},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2572},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":44},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":54},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":10},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":3,"successRate":0.75,"time":252},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":2,"successRate":0.6666666666666666,"time":347},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":623},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":167},"title":"","narrative":""},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":197},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":97},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":890},"title":"","narrative":""},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":3,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":5,"successRate":0.625,"time":902},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2694},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":932},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":167},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":291},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1135},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1900},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":274},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":157},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":49},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":207},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":337},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":38},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":86},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":7536},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":228},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2304},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":355},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":104},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":633},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":36},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":98},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":3,"successRate":0.75,"time":2566},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":10},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":286},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2020},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":345},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4891},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":144},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":270},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":16135},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":703},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":35},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":229},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1239},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":218},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":689},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3860},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":268},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":937},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":10648},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":94},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1130},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":181},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10513},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":867},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":130},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":356},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":213},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":198},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":63},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12290},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":29231},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":618},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":318},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":45},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":55},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":137},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":26},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":101},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":353},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":337},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":647},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":537},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":27},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2119},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":54},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":290},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1388},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":2,"successRate":0.6666666666666666,"time":1131},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":678},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6106},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":745},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":250},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1400},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11044},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":764},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":81},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":298},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":99},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":27},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":4299},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":321},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":784},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4073},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":143},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":86},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":26},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6628},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":365},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2378},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":286},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":128},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":881},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":36},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":127},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":288},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":18},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":48},"title":"","narrative":""},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":3,"successRate":0.75,"time":2026},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":12},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":327},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2174},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":367},"title":"","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":7228},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":159},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6906},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":209},"title":"","narrative":""},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":137},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":95},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1878},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1067},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":295},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":67},"title":"","narrative":""},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":305},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":330},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":91},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11520},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25392},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":627},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":323},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":62},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":60},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":128},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":25},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":6136},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":694},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":40},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1593},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5139},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":499},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":134},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":133},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":310},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":86},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2755},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":182},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":68},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":162},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":155},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1232},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1934},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":332},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":57},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2181},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":44},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":330},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":641},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1079},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":109},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1386},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":247},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":265},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1425},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":78},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":171},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":355},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":72},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1679},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":753},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":270},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1905},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":290},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":38},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":22},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":864},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":274},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":825},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":11044},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":235},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":717},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9827},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1005},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":147},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":325},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":45},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7821},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":784},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":58},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2826},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":197},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1744},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":172},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":100},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":554},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":261},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":280},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":77},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11590},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26182},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":627},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":303},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":53},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":66},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":133},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2618},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":43},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":11},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":3,"successRate":0.75,"time":270},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":135},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":97},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":120},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":985},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":7345},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":608},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":165},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":175},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":362},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":93},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4822},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":175},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":75},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":164},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1049},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":985},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":295},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":69},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1932},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":210},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":40},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":212},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":657},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2307},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":353},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":95},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1986},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":28},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":384},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":793},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3215},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":768},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":239},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":288},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1483},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":53},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":179},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":328},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":65},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2436},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":353},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":805},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":8344},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":259},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":801},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8845},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":859},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":212},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":470},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":72},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7638},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":849},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":54},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2312},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":258},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2349},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":265},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":81},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":443},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":175},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":278},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":102},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12415},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26050},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":603},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":336},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":45},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":69},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":140},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2610},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":45},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":283},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":5,"passed":0,"successRate":0.0,"time":854},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":5,"passed":1,"successRate":1.0,"time":999},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16332},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":342},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":547},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":915},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":117},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":396},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":452},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":34},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":158},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":462},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":89},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":306},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":516},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":3,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":1,"successRate":0.25,"time":749},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4478},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":476},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2744},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":90},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1039},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":556},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":212},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":232},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":140},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":428},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1413},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":118},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":121},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":102},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":3968},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":154},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":903},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":525},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26630},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":290},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":55},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":55},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":162},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":340},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":356},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1126},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":417},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":65},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":304},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":175},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":117},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2192},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":483},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":158},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1899},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":413},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1255},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":3790},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":943},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2613},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":389},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1395},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3598},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":142},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1189},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":86},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":326},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":7506},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":178},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":12927},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":126},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":25},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12281},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":52},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":244},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":303},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":305},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":415},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":3,"successRate":0.75,"time":670},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":158},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":155},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":27374},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":530},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":916},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1702},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":217},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":610},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":180},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":46},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":399},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":66},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":270},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":151},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":956},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":44},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5983},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":463},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5791},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":108},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1063},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1277},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":272},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":402},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":327},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":285},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1701},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":227},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":86},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5193},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":18},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1171},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":489},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":51945},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":179},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":50},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":153},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":201},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":219},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":588},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1700},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":598},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":45},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":291},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":284},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":92},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":104},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1909},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":234},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":91},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1634},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":44},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":271},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3093},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5410},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2424},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4082},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1283},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1951},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3573},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":183},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":624},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":98},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":316},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":8677},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":183},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":50},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":15581},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":137},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10288},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":51},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":10},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":8560},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":484},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":4108},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":85},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":719},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1311},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":271},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":353},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":382},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":587},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1331},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":63},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":200},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":156},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":6110},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1201},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":522},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":29922},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":988},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":847},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1404},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":211},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":567},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":212},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":41},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":109},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":79},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":770},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":119},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":230},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":162},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1353},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":19},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":51020},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":112},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":26},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":60},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":93},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":164},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":555},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1219},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":453},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":48},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":294},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":265},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":174},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2145},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":223},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":138},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1904},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":44},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":752},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2300},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5487},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2130},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4479},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":869},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2073},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5841},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":116},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1072},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":67},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":437},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6482},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":193},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":14366},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":9939},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":51},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15582},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":427},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":415},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":870},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":175},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":818},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":109},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":57},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":58},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":462},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":62},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":204},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":214},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":683},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":22},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4315},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":237},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2388},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":107},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":504},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":482},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":194},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":33},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":120},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":140},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":399},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":977},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":150},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":78},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":3656},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1083},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":466},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26113},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":228},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":67},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":101},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":146},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":635},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":2174},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":645},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":60},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":354},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":212},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":87},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":108},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2053},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":232},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":91},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2035},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":47},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":313},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1209},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3180},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":847},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2176},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":390},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1348},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3059},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":61},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":587},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":144},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":304},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":9411},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":271},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":13867},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":119},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":30},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11129},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":77},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":11},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17927},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":525},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":948},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1103},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":118},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":360},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":201},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":43},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":77},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":74},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":462},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":105},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":257},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":171},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1026},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":20},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4742},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":479},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":3333},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":54},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":593},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":757},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":204},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":343},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":217},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":551},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1344},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":125},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":130},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":5001},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":33},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1006},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":461},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":30695},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":275},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":50},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":346},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":173},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":232},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":350},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1207},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":484},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":334},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":345},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":96},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":155},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2065},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":329},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":159},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2280},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":60},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":361},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1474},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3238},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1042},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3529},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":524},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2593},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3371},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":65},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":698},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":91},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":327},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":8349},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":175},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":15334},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":103},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11104},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":56},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":12},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15083},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":428},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":576},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1048},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":355},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":116},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":139},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":40},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":450},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":69},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":467},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":244},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":828},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4337},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":291},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2581},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":80},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":715},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":580},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":155},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":178},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":200},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":297},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1082},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":100},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":209},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":4251},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":18},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":988},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":371},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25150},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":231},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":65},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":179},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":288},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":443},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1488},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":510},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":61},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":299},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":343},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":90},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":132},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2628},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":366},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":104},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1955},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":160},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1821},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3291},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":939},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3185},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":553},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1527},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3096},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":87},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":655},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":93},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":247},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":8515},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":150},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":13997},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":96},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":22},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11786},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":84},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":11},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":15612},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":359},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":536},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1013},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":119},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":339},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":169},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":28},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":87},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":50},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":391},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":4},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":61},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":317},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":156},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1124},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4068},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":366},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":3172},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":58},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":586},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":618},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":362},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":23},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":161},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":146},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":320},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1203},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":257},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":91},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":4246},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":799},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":377},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":31853},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":114},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":24},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":53},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":111},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":287},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":458},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1472},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":476},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":44},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":362},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":358},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":119},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1993},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":221},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":100},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1830},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":47},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":384},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1482},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3124},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":836},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":3035},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":431},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1507},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3056},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":77},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":563},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":87},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":284},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":7089},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":187},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":14247},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":145},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10425},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":59},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":13},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":16247},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":383},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":493},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1003},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":156},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":421},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":245},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":31},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":55},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":54},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":502},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":137},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":372},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":408},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":901},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":51},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":3,"successRate":0.6,"time":7329},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":445},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2644},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":607},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":586},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":268},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":417},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":353},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":471},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1291},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":29},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":160},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":113},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2282},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":41},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1166},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":410},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27934},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":226},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":79},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":57},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":112},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":246},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":482},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1655},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":614},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":406},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":296},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":86},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1959},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":15,"successRate":0.9375,"time":650},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":96},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1525},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":54},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":381},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1661},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3725},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":834},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2706},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":529},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1618},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3586},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":470},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1242},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":185},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":271},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":9317},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":203},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":5},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":12369},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":174},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":27},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":13970},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":58},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":6},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":7,"passed":0,"successRate":0.0,"time":7570},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":7,"passed":0,"successRate":0.0,"time":8554},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":7,"passed":0,"successRate":0.0,"time":55998},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":7,"passed":0,"successRate":0.0,"time":8357},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":7,"passed":0,"successRate":0.0,"time":8841},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":7,"passed":0,"successRate":0.0,"time":7606},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":7,"passed":1,"successRate":1.0,"time":7950},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":15350},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":324},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":529},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":702},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":166},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":489},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":177},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":120},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":44},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":46},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":479},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":61},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":255},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":219},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":877},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":30},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":3,"successRate":0.6,"time":5521},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":232},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2166},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":115},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":403},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":463},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":118},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":166},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":188},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":381},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":1018},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":57},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":139},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":91},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2979},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":26},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1118},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":728},"title":"","narrative":""},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":32245},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":218},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":49},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":61},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":174},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":474},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":410},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1837},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":523},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":58},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":287},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":269},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":97},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1831},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":15,"successRate":0.9375,"time":256},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":116},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1706},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":42},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":342},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2159},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3259},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":887},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1844},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":420},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1532},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3260},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":137},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":722},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":66},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":325},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":9781},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":195},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":15},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":14932},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":139},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":23},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":14206},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":67},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":858},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2387},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3428},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":325},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":71},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":346},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":549},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1216},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":3104},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":823},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":371},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":363},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":178},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2640},"title":"","narrative":""},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":901},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":458},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":4803},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":32},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26844},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":78},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2248},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":89},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":515},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":400},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":186},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":106},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":332},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":670},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":140},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":79},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2493},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":472},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":28},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":15064},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":531},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1006},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1315},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":154},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":413},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":186},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":27},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":45},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":47},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":566},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":55},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":595},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":121},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":29074},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":167},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1401},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":2325},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1704},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4118},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1071},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2042},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":3866},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":559},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":152},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":223},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":8787},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":214},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":19884},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":154},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":35},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":114},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5267},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":458},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":675},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2988},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":547},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":321},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":209},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":56},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":56},"title":"What it means to be N-Dimensional","narrative":""},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":864},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":3},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":133},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":220},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":227},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":975},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":25},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":31},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":848},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2177},"title":"Cross Device Tensor Slicing","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3625},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":311},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":169},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":366},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":707},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":3609},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1063},"title":"","narrative":""},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":61},"title":"","narrative":""},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":396},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":439},"title":"","narrative":""},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":117},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3629},"title":"","narrative":""},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":230},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":88},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":387},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":35213},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":84},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1268},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":250},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":385},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":124},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":111},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":116},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":209},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":614},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":46},"title":"","narrative":""},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2423},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":30},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":762},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":474},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":22233},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2083},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":566},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":559},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2172},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2481},"title":"","narrative":""},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":58},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":206},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":57},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":182},"title":"","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6492},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":177},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":13650},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":131},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":14696},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":140},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1375},"title":"","narrative":""},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":229},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":418},"title":"","narrative":""},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":1,"successRate":1.0,"time":200},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":13242},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":13249},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":13618},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":13749},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":14020},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":1,"successRate":1.0,"time":13847},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":1,"successRate":1.0,"time":13173},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":12491},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":1,"successRate":1.0,"time":14603},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":68495},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":14131},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":13897},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":14025},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":10443},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":9254},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":10088},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":8854},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":0,"successRate":0.0,"time":9393},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":2,"passed":1,"successRate":1.0,"time":8876},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4902},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":56},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":281},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":109},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":501},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":914},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":429},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":766},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2195},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":191},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":354},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":89},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":71},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":2959},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":535},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":894},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":320},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":24},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1928},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":266},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":47},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":411},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1924},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1414},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":164},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":343},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":461},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4820},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":98},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":110},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":15},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":338},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9249},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1725},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":66},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":237},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":58},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":42},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":196},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":54},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8442},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1018},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":164},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":466},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":58},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":9276},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":749},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":58},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":746},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1905},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":520},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":207},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2303},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":71},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":531},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":874},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6114},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":210},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1523},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":323},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":119},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":450},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":80},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":228},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":47},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10520},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":28203},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":643},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":65},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":161},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4177},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":115},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":553},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1856},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":3544},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":91},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":96},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":161},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":33},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":3,"successRate":0.6,"time":6108},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":113},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":46},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":137},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":1705},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":839},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":298},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":63},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2018},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":228},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":36},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":28},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":4250},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":53},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":761},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":201},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":236},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1410},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":77},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":172},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":311},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":38},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":82},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3073},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":95},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":437},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2152},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":305},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":57},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1734},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":50},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":288},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":975},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3511},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":468},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":780},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":8169},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":211},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":760},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10385},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":955},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":177},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":401},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7628},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":755},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":64},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1151},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2057},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":412},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":351},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":633},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":223},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":290},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":123},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12489},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27578},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":597},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":327},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":48},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":68},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":164},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2582},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":57},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":74},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":237},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4877},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":121},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":172},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":3,"successRate":0.6,"time":892},"title":"","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":326},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":132},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":181},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1105},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1551},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":294},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":773},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":29},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9329},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1662},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":93},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":217},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":33},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":92},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":124},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":150},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":248},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":26},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":4614},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":3203},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":342},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":252},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":709},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":187},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":266},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":112},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":138},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":326},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2926},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1093},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":120},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":311},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":369},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4249},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":64},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1207},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":68},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":5108},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":10},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":558},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1678},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":308},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":54},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1386},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10054},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":926},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":111},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":403},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7530},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":35},"title":"","narrative":""},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2362},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":590},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":741},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":320},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1637},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":155},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":43},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10937},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27190},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":705},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":67},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":140},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4164},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":114},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":6},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":228},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3137},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3129},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3139},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3112},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":1,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":5,"successRate":0.7142857142857143,"time":3158},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3172},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3168},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3113},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3126},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":6,"successRate":0.8571428571428571,"time":3129},"title":"","narrative":""},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":3202},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5169},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":99},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":438},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":6971},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1831},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":111},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":335},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":50},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":161},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":46},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":209},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":262},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1544},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":274},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":267},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1620},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":79},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":131},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":432},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":64},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2609},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":621},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":640},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":256},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1616},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":245},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":39},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":459},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2452},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":882},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":137},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":257},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":346},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4768},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":77},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":16},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":332},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":2,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":3,"successRate":0.6,"time":4976},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":220},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1640},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":191},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":87},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":482},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":136},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":298},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":65},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10643},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9259},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1123},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":154},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":309},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7364},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":721},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":42},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26164},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":704},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":66},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":137},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4337},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":113},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":537},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":779},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":176},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":463},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":270},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\"."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1433},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":166},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":33},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4623},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":66},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":2911},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":336},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4455},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":325},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":370},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1238},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":71},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":282},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":314},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":40},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":511},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1517},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1308},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":110},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":349},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":450},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5361},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":93},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":125},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":405},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10457},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1103},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":75},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":280},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":140},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":44},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4742},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":21},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":354},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1180},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":255},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":29},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1492},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":194},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":679},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10084},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1042},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":131},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":379},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":36},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7532},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":779},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":38},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":628},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2253},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":339},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":249},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":588},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":287},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":354},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":87},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12983},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26497},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":645},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":68},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":147},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4241},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":110},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":529},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":111},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":3136},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":3018},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":3069},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":3080},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":3136},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":3087},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":3097},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":3080},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":3117},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":3089},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":13704},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4790},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":84},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":1427},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10033},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1540},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":71},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":225},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":56},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":172},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":41},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":144},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":330},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2669},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":267},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":535},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1630},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":88},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":190},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":347},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":31},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":458},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2317},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1039},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":196},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":285},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":403},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4968},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":73},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":81},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":366},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":895},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2220},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":409},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":110},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2291},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":50},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":518},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":991},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":806},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":156},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1547},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":182},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":62},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":342},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":92},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":253},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":61},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":15054},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2524},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":544},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":712},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":281},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1916},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":205},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":121},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":31},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10388},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":954},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":202},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":248},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":28},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7143},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":653},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":43},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24717},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":651},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":79},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":143},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4291},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":107},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":594},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":3079},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":3116},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":3112},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":0,"successRate":0.0,"time":3032},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":3063},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4621},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":139},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":1817},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9724},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1454},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":61},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":257},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":44},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":234},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":59},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":447},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2559},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":338},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":142},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2577},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":41},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":394},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1190},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":13},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":475},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2619},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1094},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":209},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":442},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":344},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5313},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":71},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":391},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":170},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":495},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1891},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":435},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":504},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1599},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":133},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":179},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":345},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":33},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":64},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8633},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1073},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":140},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":498},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":66},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8791},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":42},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":784},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":44},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5091},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":243},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1731},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":304},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":112},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":508},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":192},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":305},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":71},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":8},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10834},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26800},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":721},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":166},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4379},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":119},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":597},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":4,"passed":1,"successRate":1.0,"time":3092},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4908},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":193},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":241},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":735},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":424},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":487},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":223},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":100},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1521},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":164},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":175},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":34},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8886},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1408},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":66},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":220},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":46},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":40},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":134},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":49},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1123},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1980},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":467},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":77},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2094},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":44},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":556},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":629},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":150},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":168},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":904},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":402},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":327},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1723},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":72},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":157},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":321},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":21},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":64},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":650},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1797},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1208},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":102},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":250},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":356},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4635},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":77},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":93},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":361},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9477},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1057},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":165},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":379},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7557},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":846},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":67},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5848},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":188},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1364},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":246},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":99},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":433},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":117},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":244},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":70},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10786},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25010},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":675},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":71},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":142},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4288},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":122},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":571},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":2,"errors":1,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":0,"successRate":0.0,"time":5761},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":48},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":4,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":5,"successRate":0.5555555555555556,"time":227},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":264},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1542},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":195},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":55},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1563},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":19},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":446},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":848},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4651},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":59},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":1513},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10205},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1292},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":105},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":141},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":50},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":163},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":55},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":566},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1792},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":765},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":143},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":248},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":618},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5081},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":92},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":139},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":565},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":65},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":230},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3882},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":223},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":619},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1411},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":62},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":180},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":322},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":35},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":78},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9705},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":946},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":251},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":329},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":30},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8145},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":833},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":79},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":7373},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":186},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1374},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":252},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":90},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":418},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":87},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":235},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":78},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":16},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11295},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26938},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":745},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":168},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4382},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":118},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":574},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5214},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":146},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":282},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":784},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":434},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1036},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":196},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":94},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1391},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":219},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":179},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":21},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9345},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1470},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":99},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":186},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":44},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":106},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":53},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":137},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":228},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2178},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":266},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":394},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1583},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":80},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":172},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":353},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":24},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":42},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":807},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1848},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1196},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":112},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":245},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":412},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4487},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":79},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":97},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":430},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9153},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1111},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":146},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":237},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":33},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7658},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":789},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":40},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":7047},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":178},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1423},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":183},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":54},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":469},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":81},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":215},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":58},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10638},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25353},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":675},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":141},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4258},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":110},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":538},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5396},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":83},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":370},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8695},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1585},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":103},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":274},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":55},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":20},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":134},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":47},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":258},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":193},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1467},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":320},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":373},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1689},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":145},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":142},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":344},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":26},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":100},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":652},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2459},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":622},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":225},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1993},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":374},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":801},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3092},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":730},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":807},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":222},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":154},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1878},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":137},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":97},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":665},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2474},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1126},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":110},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":218},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":330},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4716},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":76},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":76},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":17},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":391},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9768},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":937},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":120},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":289},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":38},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7723},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":37},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":751},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":41},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3182},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":200},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1526},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":170},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":143},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":689},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":102},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":231},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":81},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":1},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11064},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":27723},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":707},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":137},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4296},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":108},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":581},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5604},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1269},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":55},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":195},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":31},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":10},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":92},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5250},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":65},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":271},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":3699},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":433},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":732},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":242},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":114},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1768},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":215},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":80},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":14},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1160},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2058},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":427},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":75},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1862},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":36},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":441},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":700},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":782},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1572},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1092},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":86},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":243},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":331},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4415},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":83},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":108},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":312},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6267},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":174},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1344},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":242},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":156},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":572},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":115},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":225},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":57},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10934},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":10157},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":916},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":217},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":297},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7517},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":745},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":49},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24203},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":718},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":150},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4469},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":106},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":539},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1739},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":3891},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":319},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":1,"successRate":0.5,"time":195},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":1,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":0,"successRate":0.0,"time":116},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":3,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":1,"successRate":0.25,"time":128},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5524},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":108},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":603},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":825},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2767},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":347},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":83},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2096},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":33},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":304},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":710},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"","narrative":""},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":155},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":152},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1676},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":338},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":335},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1630},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":87},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":220},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":342},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":20},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":42},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":6485},"title":"Cross Device Tensor Slicing","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1940},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":75},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":217},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":52},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":21},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":176},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":67},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":3299},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":420},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":496},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":279},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":151},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1475},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":155},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":91},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":618},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2193},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1306},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":100},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":284},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":384},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5058},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":69},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":104},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":13},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":336},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4038},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":189},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1511},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":288},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":112},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":423},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":127},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":226},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":73},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":11066},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9319},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1033},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":95},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":322},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":34},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":7810},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":761},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":37},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25606},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":690},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":144},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4460},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":133},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":570},"title":"","narrative":""},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":403},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":611},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":162},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":167},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1659},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":128},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":76},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1314},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4762},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":315},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":230},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":140},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":269},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":105},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5996},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":152},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":52},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":138},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5509},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":67},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":409},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":193},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":306},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1209},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":60},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":118},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":316},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":34},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":33},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":2504},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":34},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":371},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1836},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":205},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":76},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1674},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":28},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":233},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":888},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":11},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":3435},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":326},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":677},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":8200},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":208},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":715},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":682},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2458},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":285},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":313},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":834},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":149},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":374},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":124},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12647},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9412},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":838},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":154},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":347},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8255},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":43},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":755},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":46},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26245},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":16},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":648},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":352},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":46},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":58},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":129},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2709},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":45},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":111},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":12},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":241},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1256},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4953},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":410},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":243},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":139},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":328},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":85},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5350},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":31},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":502},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":212},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":291},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1430},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":52},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":114},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":332},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":28},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":68},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6629},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":180},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":47},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":105},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":3040},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":31},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":348},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1567},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":225},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":51},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1605},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":39},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":286},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":767},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":12},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":859},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":189},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":1427},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":10708},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":308},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":748},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":9255},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":902},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":116},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":461},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":54},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8203},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":39},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":826},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":62},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":467},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2785},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":294},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":384},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":741},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":223},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":362},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":98},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":23},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":13428},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":26073},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":14},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":577},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":374},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":47},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":63},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":146},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2593},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":54},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":56},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":240},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":4470},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":97},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":789},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":170},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":255},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1871},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":250},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":321},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":2145},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":85},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":135},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":308},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":35},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":53},"title":"","narrative":""},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":1,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":4,"successRate":0.8,"time":2399},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"ut.autograd.Autograd_Flags_Explained":{"executedFeatures":["Advanced backpropagation on all AD-Modes ","We can create a shallow copy of a tensor detached from the computation graph."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":2,"passed":9,"successRate":1.0,"time":348},"title":"","narrative":""},"ut.autograd.Autograd_NN_Spec":{"executedFeatures":["Autograd work for simple matrix multiplications.","Autograd works for 2 matrix multiplications in a row.","Autograd works in a simple convolutional dot product and float based feed forward neural network.","Autograd works in a simple convolutional dot product based feed forward neural network.","Autograd works in a simple mat-mul based feed forward neural network."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":628},"title":"Simple Neural Network autograd integration test","narrative":"The integration test below has been implemented by using\n    the following code and the result it produces as reference : \n    https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0 \n    \n    The following seed has been used to assure reproducibility :\n    'torch.manual_seed(503672689411)'"},"ut.autograd.JITProp_Autograd_Tensor_Spec":{"executedFeatures":["Gradient auto-apply kicks in when used AD uses JIT prop","Test JIT propagation variant one.","Test JIT propagation variant two.","Test autograd without JIT and auto apply.","Test in-differential and JIT with auto apply","Test no JIT prop when forward AD","Test no preemptive gradient apply when not requested and auto apply and JIT_prop","Test pending error optimization"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":281},"title":"","narrative":""},"ut.backend.core.Randomization_Spec":{"executedFeatures":["Randomization is in essence the same algorithm as JDKs \"Random\".","The Randomization class can fill various types of arrays with pseudo random numbers.","We can make slices of tensors random."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":166},"title":"","narrative":""},"ut.device.OpenCLDevice_Exception_Spec":{"executedFeatures":["Ad hoc compilation produces expected exceptions when duplication is found.","Ad hoc compilation produces expected exceptions.","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","Trying to restore a tensor which is not on a device raises exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1764},"title":"OpenCLDevice Exception Handling","narrative":"The OpenCLDevice class, one of many implementations of the Device interface, \n    represents physical OpenCL devices.\n    This specification defines how instances of this class deal with exceptional information."},"ut.ndim.NDConfiguration_Spec":{"executedFeatures":["Various NDConfigurations behaviour exactly as their general purpose implementation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":205},"title":"","narrative":""},"ut.tensors.Copy_Spec":{"executedFeatures":["A deep copy of a slice tensor is also a deep copy of the underlying data array.","A deep copy of a tensor is also a deep copy of the underlying data array.","A shallow copy will share the same underlying data as its original tensor.","We can deep copy various types of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":118},"title":"To Copy or Not to Copy","narrative":"In this specification we cover the behaviour of tensors with respect to their copy methods.\n    There are to main ways to copy a tensor: <br>\n    1. .shallowCopy() <br>\n    2. .deepCopy() <br>\n    <br>\n    The first method creates a new tensor with the same underlying data array as the original tensor. <br>\n    The second method on the other hand creates a new tensor with a new data array. <br>\n    <br>\n    The first method is the most efficient, but it is not as safe as the second method. <br>\n    The second method is the most safe, but it is not as efficient. <br>\n    <br>\n    Besides these 2 main requirements, there are als some corner cases with respect to\n    the components of a tensor (like for example its computation graph) which\n    will be covered in this specification as well."},"ut.utility.ListReader_Exception_Spec":{"executedFeatures":["The ListReader will detect inconsistent degrees of nesting in the provided data.","The ListReader will detect inconsistent types in the provided data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":630},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2156},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":531},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":108},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1893},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":48},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":418},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":609},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":7},"title":"","narrative":""},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1026},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1876},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1371},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":152},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":339},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":359},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":4227},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":70},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":78},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":14},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":350},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8193},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":892},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":167},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":532},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":61},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8685},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":41},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":789},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":81},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":5788},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":203},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":1741},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":228},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":128},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":532},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":96},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":208},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":86},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":19},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":10703},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":24655},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":710},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":69},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":146},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":4367},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":110},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":564},"title":"","narrative":""},"ut.backend.core.Backend_Functional_Algorithm_Spec":{"executedFeatures":["A functional algorithm cannot be used if it was not built properly!","A functional algorithm does not accept null as an answer!","A functional algorithm warns us when modified after it has been built!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":1278},"title":"","narrative":""},"ut.calculus.BackendContext_Spec":{"executedFeatures":["BackendContext instances can be created by cloning from Singleton instance.","BackendContext instances return Runner instances for easy visiting with return values.","BackendContext instances return Runner instances for easy visiting."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":5082},"title":"The BackendContext is a cloneable context which can run Tasks.","narrative":"This specification defines the expected behaviour of the backend context\n    which should expose a convenient API to work with.\n    This API should allow for tasks to be running on a given context\n    which is important for testing and modularity not only\n    during library startup but also throughout the runtime."},"ut.device.internal.OpenCL_Data_Spec":{"executedFeatures":["The \"Data\" class can represent various OpenCL data types.","The OpenCLDevice specific Data class represents JVM data for OpenCL."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":372},"title":"","narrative":""},"ut.ndas.NDA_Instantiation_Spec":{"executedFeatures":["A vector can be created from an array of values through the \"of\" method.","ND-arrays can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":154},"title":"ND-Array Instantiation","narrative":"In this specification we cover how ND-arrays can be instantiated."},"ut.ndim.Tensor_Reshape_Spec":{"executedFeatures":["When matrices are transpose, they will change their layout type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":128},"title":"","narrative":""},"ut.optimization.RMSprop_Spec":{"executedFeatures":["RMSprop optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":315},"title":"","narrative":""},"ut.tensors.Tensor_Device_Mock_Spec":{"executedFeatures":["Tensors try to migrate themselves to a device that is being added to them as component.","Tensors try to remove themselves from their device when \"setIsOutsourced(false)\" is being called.","The device of a tensor can be accessed via the \"device()\" method.","When creating slices of tensors then this should trigger a \"parent - child\" relation noticeable to the device!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":87},"title":"","narrative":""},"it.Calculus_Stress_Test":{"executedFeatures":["Activation functions work across types, on large prime sized 1D slices and non sliced 1D tensors.","Activation functions work across types.","Dot operation stress test runs error free and produces expected result","Stress test runs error free and produces expected result","The broadcast operation stress test runs error free and produces expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":6016},"title":"","narrative":""},"ut.autograd.Autograd_Tensor_Spec":{"executedFeatures":["A tensor used as derivative within a computation graph will throw exception when trying to deleting it.","Second-Test \"x-mul\" autograd behaviour. (Not on device)","Test basic autograd behaviour. (Not on device)"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":154},"title":"","narrative":""},"ut.ndim.Tensor_NDConfiguration_Spec":{"executedFeatures":["NDConfiguration instances of tensors have expected state and behaviour.","NDConfiguration instances of tensors have expected state."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":86},"title":"What it means to be N-Dimensional","narrative":""},"ut.tensors.Tensor_Slicing_Spec":{"executedFeatures":["A tensor can be sliced by passing ranges in the form of lists (Groovy ranges).","A tensor can be sliced by passing ranges in the form of primitive arrays.","Normal slicing will try to do autograd.","Slicing is also a Function with autograd support!","The \"at\" method and the \"from\" / \"to\" methods can be mixed when slicing a tensor.","The slice builder also supports slicing with custom step sizes.","We can avoid autograd when slicing by using the \"detached\" instead of the \"get\" method.","We can slice a scalar tensor from a larger tensor of rank 4.","When Slicing only one axis using the SliceBuilder API, the other axes will be sliced implicitly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":9,"totalFeatures":9,"passed":9,"successRate":1.0,"time":151},"title":"Tensors within Tensors","narrative":"ND-Array data structures can be \"sliced\" in the sense\n    that one can create a subset view of the underlying data inside a tensor\n    through a new tensor instance...\n    This can be a tedious and complicated procedure.\n    Therefore a tensor should expose a various user friendly API for slicing which\n    are also fit for various languages.\n    This specification covers these APIs for tensor slicing."},"it.Eleven_Lines_NN_System_Spec":{"executedFeatures":["One can write a simple double based neural network in less than 11 lines of java like code using the \"@\" operator!","One can write a simple float based neural network in less than 11 lines of java like code!","One can write a simple neural network in less than 11 lines of code!","One can write a simple neural network with custom back-prop in 11 lines of code!","The pseudo random number generator works as expected for the weights used in the 11 line NN examples!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":1902},"title":"NN Code Golfing!","narrative":"This system test specification uses the following Numpy\n    code as reference implementation for the equivalent in Neureka\n    or similar implementations and variations.\n    The code below is a simple neural network in only 11 lines of code.\n\n    \u00b4\u00b4\u00b4\n        X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n        y = np.array([[0,1,1,0]]).T\n        W1 = 2*np.random.random((3,4)) - 1\n        W2 = 2*np.random.random((4,1)) - 1\n        for j in xrange(60000):\n            l1 = 1/(1+np.exp(-(np.dot(X,W1))))\n            l2 = 1/(1+np.exp(-(np.dot(l1,W2))))\n            l2_delta = (y - l2)*(l2*(1-l2))\n            l1_delta = l2_delta.dot(W2.T) * (l1 * (1-l1))\n            W2 += l1.T.dot(l2_delta)\n            W1 += X.T.dot(l1_delta)\n    \u00b4\u00b4\u00b4"},"Example_Spec.Example_Spec":{"executedFeatures":["Call me feature not unit test!","I am readable and also best practice!","Numbers to the power of two with a fancy data table!","Should be able to remove from list","iAmNotSoReadable"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":48},"title":"An Introduction to writing Spock Specifications","narrative":"Hello and welcome to the example / template specification of this project.\n    This is a simple introduction as to how to get started writing Spock specifications.\n    \n    Spock works on top of Groovy which is in essence a syntactic super-set of Java.\n    That means that one can write Java code in Groovy, and 99% of the time it will \n    work the exact same way."},"ut.autograd.AD_And_Computation_Graph_Spec":{"executedFeatures":["Payloads and derivatives are null after garbage collection.","Reshaping produces expected computation graph and also works with reverse mode AD."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":271},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Unit_Tests":{"executedFeatures":["GraphNode instantiation works as expected when the context argument is a GraphLock.","GraphNode instantiation works as expected when the context argument is an ExecutionCall."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":2146},"title":"","narrative":""},"ut.backend.Backend_MatMul_Extension_Spec":{"executedFeatures":["GEMM matrix multiplication reference implementation can be set as custom OperationType and works as expected.","Test context mock for opencl reference implementations.","Tile parsing for kernel parameter calculation yields expected tile dimensions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":329},"title":"","narrative":""},"ut.ndim.Tensor_Slice_Reshape_Spec":{"executedFeatures":["A slice of a tensor changes as expected when reshaping it.","Reshaping a slice works as expected.","Two slices of one big tensor perform matrix multiplication flawless."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":32},"title":"","narrative":""},"ut.tensors.Functional_Tensor_Spec":{"executedFeatures":["Tensor initialization lambdas produce expected tensors.","Tensor mapping lambdas produce expected tensors.","The \"map\" method is a shorter convenience method for mapping to the same type.","We can analyse the values of a tensor using various predicate receiving methods"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":1610},"title":"","narrative":""},"ut.tensors.Tensor_Generics_Spec":{"executedFeatures":["1D tensors can be created from primitive arrays.","Anonymous tensor instance has the default datatype class as defined in Neureka settings.","String tensor instance discovers expected class."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":26},"title":"Tensors as Generic Containers","narrative":"Tensors do not just store numeric data.\n    They can hold anything which can be stuffed into a \"Object[]\" array.\n    You could even create a tensor of tensors!"},"ut.tensors.Tensor_IO_Spec":{"executedFeatures":["A tensor produced by a function has expected properties.","A tensor produced by the static \"Tsr.Create.newRandom(shape)\" has expected \"random\" value.","Adding OpenCL device to tensor makes tensor be \"outsourced\" and contain the Device instance as component.","Indexing after reshaping works as expected.","Passing String seed to tensor produces expected values.","Smart tensor constructors yield expected results.","Tensor value type can not be changed by passing float or double arrays to it.","Tensor values can be manipulated via static method calls within the \"Tsr.IO\" class.","Tensors can be instantiated with String seed.","Tensors value type can be changed by calling \"toType(...)\".","The tensor data array can be modified by targeting them with an index.","Vector tensors can be instantiated via factory methods.","We can manipulate the underlying data array of a tensor through the unsafe API.","We can re-populate a tensor of shorts from a single scalar value!","We turn a tensor into a scalar value or string through the \"as\" operator!","When we try to manipulate the underlying data array of a virtual tensor then it will become actual."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":16,"totalFeatures":16,"passed":16,"successRate":1.0,"time":291},"title":"The Tensor state Input and Output Specification","narrative":"Tensors are complicated data structures with a wide range of different possible states.\n    They can host elements of different types residing on many kinds of different devices.\n    Here we want to define some basic stuff about how a tensor can be instantiated\n    and how we can read from and write to the state of a tensor.\n    Here we also specify how a tensor can be converted to another tensor of a different data type!"},"ut.tensors.Tensor_Layout_Spec":{"executedFeatures":["A new transposed version of a given tensor will be returned by the \"T()\" method.","Matrix multiplication works for both column and row major matrices across devices."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":663},"title":"Row or Column Major. Why not both?","narrative":"Although Neureka exposes tensors as row major tensors from \n    a users point of view, it does in fact support both row major and column major \n    based tensor layout under the hood.\n    Here we cover how the layout of tensors can be modified\n    and we ensure the different tensor types still work as expected...\n    (The features in this specification involve mutating tensors, be careful when playing around with this yourself)"},"ut.utility.DataConverter_Spec":{"executedFeatures":["An array of any type of object may be converted to a array of primitives.","The DataConverter can convert the given array data."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":16},"title":"","narrative":""},"it.Cross_Device_Sliced_Tensor_System_Test":{"executedFeatures":["Cross device sliced tensor integration test runs without errors.","Slices can be created using the SliceBuilder."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":5962},"title":"Cross Device Tensor Slicing","narrative":""},"ut.calculus.Calculus_Exception_Spec":{"executedFeatures":["Function throws exception when arity does not match input number.","Function throws exception when not enough inputs provided."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":44},"title":"","narrative":""},"ut.calculus.Tensor_Function_Spec":{"executedFeatures":["Executed tensors are intermediate tensors.","Reshaping on 3D tensors works by instantiate a Function instance built from a String.","Tensor results of various Function instances return expected results.","The \"DimTrim\" operation works forward as well as backward!","The optimization function for the SGD algorithm produces the expected result"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":556},"title":"Applying Functions to Tensors","narrative":"A tensor would be nothing without being able to apply operations on them.\n    However, calling operations manually in order to process your\n    tensors can be a verbose and error prone task.\n    This is where functions come into play.\n    Neureka's functions are composed of operations forming an abstract syntax tree.\n    Passing tensors to a function will route them trough this tree and apply\n    all of the operations on the tensors for you."},"ut.device.Cross_Device_IO_Spec":{"executedFeatures":["We can use the access device API to read from a tensor.","We can use the access device API to write to a tensor"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":180},"title":"Devices manage the states of the tensors they store!","narrative":"Tensors should not manage their states\n    themselves, simply because the type and location\n    of the data is dependent on the device onto which they are stored.\n    This specification tests of various device implementations\n    enable reading to or writing from the tensors they store."},"ut.device.FileDevice_Spec":{"executedFeatures":["A file device stores tensors in idx files by default.","A file device stores tensors in various file formats.","The file device can load known files in a directory."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":204},"title":"FileDevice, Storing Tensors in Files","narrative":"The FileDevice class, one of many implementations of the Device interface, \n    represents a file directory which should be able to store and load tensors as files (idx, jpg, png...)."},"ut.device.internal.OpenCL_GEMM_Unit_Spec":{"executedFeatures":["The GEMM implementation for the OpenCLDevice has realistic behaviour"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":1325},"title":"","narrative":""},"ut.tensors.Tensor_As_Container_Spec":{"executedFeatures":["More tensor operations translate to custom data type \"ComplexNumber\".","Plus operator on String tensors works element-wise.","Tensor operations translate to custom data type \"ComplexNumber\".","We can apply predicates on the values of a tensor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":68},"title":"Why not have a tensor of words?","narrative":"Technically, tensors are merely fancy ND-arrays with some useful mathematical operations\n    applicable to them...\n    Therefore, there is no reason why a tensor would not also be able to store\n    other kinds of objects besides numbers like strings for example.\n    This specification ensures that tensors can hold and index many other things..."},"ut.tensors.Tensor_State_Spec":{"executedFeatures":["A tensor can be instantiated from a target type and nested lists.","Newly instantiated and unmodified scalar tensor has expected state.","Newly instantiated and unmodified vector tensor has expected state.","Numeric tensors as String can be formatted on an entry based level.","Tensor created from shape and datatype has expected state.","Tensors as String can be formatted depending on shape.","Tensors as String can be formatted on an entry based level.","The data and the value of a tensor a 2 different things!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":123},"title":"The Tensor Initialization and State Specification","narrative":"This specification defines the expected states of freshly instantiated\n    and initialized tensors.\n    After a tensor was created successfully we expect it \n    to have certain properties like a shape, rank, type nnd data array\n    among other things."},"ut.utility.Cleaner_Testing":{"executedFeatures":["The default DeviceCleaner works"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":259},"title":"","narrative":""},"ut.utility.ListReader_Spec":{"executedFeatures":["The ListReader can interpret nested lists into a shape list and value list.","The ListReader can interpret nested lists resembling a 3D tensor into a shape list and value list.","The ListReader can interpret nested lists resembling a matrix into a shape list and value list."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":18},"title":"The Internal ListReader turning lists into flat arrays with shape and type data","narrative":"This specification covers an internal class which should not be used\n    outside this library, namely the ListReader class.\n    This class is simply a converter which turns nested lists\n    into flat arrays alongside the type of the elements and the shape of this \"tensor\"."},"ut.utility.Utility_Spec":{"executedFeatures":["Object arrays can be converted to primitive arrays."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":28},"title":"","narrative":""},"st.Benchmark_System_Test":{"executedFeatures":["Tensor can be constructed by passing List instances.","Test benchmark script and simple tensor constructor."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":8245},"title":"","narrative":""},"ut.backend.core.Matrix_Multiplication_Spec":{"executedFeatures":["The internal matrix multiplication test script runs!","The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":1091},"title":"","narrative":""},"ut.calculus.Calculus_Scalar_Spec":{"executedFeatures":["Function \"(I[0]+1/I[0])**-I[0]\" instance returns expected scalar result.","Function \"(cos(I[0]*5)/5+I[0])*(1+sin(I[0])/2)\" instance returns expected scalars.","Function \"1/I[0]\" instance returns expected scalar results.","Function \"I[0]+1/I[0]\" instance returns expected scalar results.","Test scalar results of Function \"sumjs((cos(I[j]*5)/5+I[j])*(1+sin(I[j])/2))\" instance.","Test scalar results of various Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":194},"title":"Functions for Scalars","narrative":"The Function API and it's implementations \n    receive and process arrays of scalars as arguments.\n    Functions don't have to be used alongside tensors / nd-arrays,\n    they can also compute derivatives based on scalar values."},"ut.device.internal.CLFunctionCompiler_Spec":{"executedFeatures":["The CLFunctionCompiler produces an operation which properly integrates to the backend.","The CLFunctionCompiler produces the expected \"ad hoc\" kernel."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":455},"title":"Turning functions into kernels.","narrative":"Neureka parses mathematical expressions into an AST representation\n    hidden behind the Function interface...\n    This feature does not exist without reason, we can use\n    this abstract syntax tree to compile to OpenCL kernels\n    for optimal execution speed!"},"ut.dtype.DataType_Spec":{"executedFeatures":["DataType multi-ton instances behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":52},"title":"","narrative":""},"ut.neureka.Neureka_Spec":{"executedFeatures":["Backend related library objects adhere to the same toString formatting convention!","Every Thread instance has their own Neureka instance.","Neureka class instance has expected behaviour.","Neureka settings class can be locked causing its properties to be immutable.","OpenCL related library objects adhere to the same toString formatting convention!","Various library objects adhere to the same toString formatting convention!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":8417},"title":"The Neureka context can be used and configured as expected.","narrative":"This specification covers the behavior of the Neureka class which\n    exposes a global API for configuring thread local contexts and library settings.\n    The purpose of this is to assert that the API exposed by the Neureka class \n    is both thread local and configurable.\n    This specification also exists to cover standards for the Neureka library in general."},"ut.optimization.Momentum_Spec":{"executedFeatures":["Momentum optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":57},"title":"","narrative":""},"ut.tensors.Tensor_Convolution_Spec":{"executedFeatures":["Autograd works with simple 2D convolution.","Manual convolution produces expected result.","Sime convolution works as expected eith autograd.","Tensors have the correct layout after convolution.","The \"x\" (convolution) operator produces expected results (On the CPU).","Very simple manual convolution produces expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":745},"title":"","narrative":""},"ut.tensors.Tensor_Interop_Spec":{"executedFeatures":["Not all tensor can be converted to images.","Tensor can be converted to buffered images."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":59},"title":"Tensors play well with other data structures!","narrative":"Tensors should have good interoperability with other JDK data structures like images.\n    In this specification we define these interoperability requirements."},"ut.backend.core.Backend_Algorithm_AD_Spec":{"executedFeatures":["Activation implementations behave as expected.","Broadcast implementations behave as expected.","Convolution implementations behave as expected.","Operator implementations behave as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":2508},"title":"","narrative":""},"ut.calculus.Calculus_Function_Spec":{"executedFeatures":["Function implementations ensure that internally created tensors are flagged as \"intermediate\" initially!","Function implementations ensure that outputs which are input members are not flagged as \"intermediate\"!","Function implementations will ensure the \"call\" and \"invoke\" does not return tensors flagged as \"intermediate\".","The library context exposes a set of useful functions."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":373},"title":"Testing Default Methods on Functions","narrative":"This specification tests the default methods on functions\n    through a simple dummy implementation of the Function interface."},"ut.device.Cross_Device_Type_Spec":{"executedFeatures":["Advanced device querying methods query as expected!","Devices store slices which can also be restored.","Devices store tensors which can also be restored.","Execution calls containing null arguments will cause an exception to be thrown in device instances.","Passing a numeric array to a tensor should modify its content!","Querying for Device implementations works as expected.","Tensor data can be fetched from device if the tensor is stored on it...","The simpler device querying methods query as expected!","Virtual tensors stay virtual when outsourced."],"ignoredFeatures":["Devices cannot store slices which parents are not already stored."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":9,"totalFeatures":10,"passed":9,"successRate":1.0,"time":767},"title":"Cross Device-Type Unit Tests","narrative":""},"ut.device.OpenCLDevice_Spec":{"executedFeatures":["Ad hoc compilation produces executable kernel.","Ad hoc compilation works for WIP general purpose matrix multiplication.","Ad hoc compilation works for custom column major based tiled matrix multiplication.","Ad hoc compilation works for custom simple row major based matrix multiplication.","Ad hoc matrix multiplication works for multiple of 16 matrices.","An OpenCLDevice loads tensors in a provided lambda temporarily.","The \"getData()\" method of an outsourced tensor will return null when outsourced.","The \"getValue()\" method of an outsourced tensor will return the expected array type."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":8749},"title":"The OpenCLDevice Specification","narrative":"Tensors need devices for execution!\n    By default we use the CPU as a default device, but sometimes we want to\n    use something more suitable for large amounts of data an a high degree of parallelization.\n    This is were the OpenCLDevice comes into play!\n    It is a Device implementation built on top of the JOCL library, a thin OpenCL API!\n    We expect the OpenCLDevice to stored tensors while still being able to read and write\n    data from and to stored tensors.\n    Also, an OpenCLDevice should allows us to compile OpenCL kernel code on the fly..."},"ut.optimization.ADAM_Spec":{"executedFeatures":["ADAM optimizes according to expected inputs","Equations \"I[0]*I[1]+(1-I[2])*I[3]\" and \"(1-I[0])*I[1]\" used within ADAM return expected results.","Equations used by ADAM return expected result."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":181},"title":"","narrative":""},"ut.utility.FileHandle_Spec":{"executedFeatures":["Fully labeled tenors will be stored with their labels included when saving them as CSV.","Partially labeled tenors will be stored with their labels included when saving them as CSV.","Test reading IDX file format.","Test writing IDX file format.","The FileDevice component \"CSVHead\" can read CSV file formats and load them as tensors.","We can load image files as tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":708},"title":"","narrative":""},"st.Broad_System_Test":{"executedFeatures":["Test integration broadly."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":589},"title":"","narrative":""},"ut.autograd.internal.GraphNode_Instantiation_Exception_Unit_Tests":{"executedFeatures":["GraphNode instantiation throws exception because GraphNode instances of input tensors do not share the same GraphLock.","GraphNode instantiation throws exception because tensors of ExecutionCall do not return GraphNode instances.","GraphNode throws an exception when trying to execute an inline operation on inputs with active autograd.","GraphNode throws an exception when trying to instantiate it with the wrong context.","GraphNode throws exception when payload is null.","GraphNode throws exception when trying to instantiate it with the Function argument being null."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":6,"totalFeatures":6,"passed":6,"successRate":1.0,"time":2575},"title":"","narrative":""},"ut.backend.Backend_Extension_Spec":{"executedFeatures":["Lambda properties of mock implementation interact with FunctionNode as expected.","Mock operation interacts with FunctionNode (AbstractFunction) instance as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":2,"totalFeatures":2,"passed":2,"successRate":1.0,"time":289},"title":"","narrative":""},"ut.backend.Matrix_Multiplication_Spec":{"executedFeatures":["The simple CPU matrix multiplication implementation works as expected."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":374},"title":"Matrix Multiplication","narrative":"The tensor API exposes a useful method for Matrix Multiplication.\n    This specification not only demonstrates how to use this method\n    but also shows how matrix multiplication work \n    for tensors with both row and column major layouts.\n    (typically, column major is faster)"},"ut.device.CPU_Spec":{"executedFeatures":["CPU knows the current number of available processor cores!","The CPU exposes a non null API for executing workloads in parallel.","Thread pool executes given workload in parallel"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":672},"title":"The CPU device, an API for CPU based execution","narrative":"The CPU class, one of many implementations of the Device interface, \n    is simply supposed to be an API for dispatching threaded workloads onto the CPU.\n    Contrary to other types of device, the CPU will host tensor data by default, simply\n    because the tensors will be stored in RAM if no device was specified."},"ut.dtype.NumericType_Spec":{"executedFeatures":["Conversion goes both ways and produces expected numeric values.","NumericType conversion to holder types yields expected results.","NumericType implementations behave as expected.","NumericType implementations return their expected properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":212},"title":"The NumericType and its implementations model their respective numeric data types.","narrative":"This specification covers the behavior of the NumericType interface\n    which is responsible for modelling numeric data types which may or may not be native to the JVM. \n    These implementations however do not model them in the traditional OO style\n    but merely expose useful utility method for converting and representing \n    these numeric data types using JVM types."},"ut.framing.Tensor_Framing_Spec":{"executedFeatures":["Added labels to tensors are accessible through the \"index()\" method.","Rank 2 tensors can be labeled and their labels can be used to extract slices / subsets of tensors.","Rank 3 tensors can be labeled and their labels can be used to extract slices / subsets of tensors."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":329},"title":"Naming Tensors and their Dimensions.","narrative":"A powerful concept in the data science as well as machine learning\n    world is something usually referred to as \"Data Frames\".\n    These are highly flexible 2D data structures\n    used to load and store CSV, CRV, etc... files for \n    data exploration and further processing.\n    Data frames are so powerful because\n    their indices are labeled and therefore human readable.\n    Neureka's tensors are general purpose data containers\n    which may also stored data in 2 dimensions whose\n    indices may also be something other than integers."},"ut.optimization.AdaGrad_Spec":{"executedFeatures":["AdaGrad optimizes according to expected inputs"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":103},"title":"","narrative":""},"ut.optimization.Optimizer_Spec":{"executedFeatures":[],"ignoredFeatures":["Conv dot based feed forward and activation produces expected result."],"stats":{"failures":0,"errors":0,"skipped":1,"totalRuns":0,"totalFeatures":1,"passed":0,"successRate":1.0,"time":2},"title":"","narrative":""},"ut.tensors.exceptions.Tensor_Delete_Exception_Spec":{"executedFeatures":["A deleted tensor will tell you that it has been deleted.","A deleted tensor will throw an exception when accessing its configuration.","A deleted tensor will throw an exception when accessing its data type.","A deleted tensor will throw an exception when accessing its data.","A deleted tensor will throw an exception when modifying its data type.","A deleted tensor will throw an exception when trying to modify its data.","A deleted tensor will throw an exception when trying to set its configuration."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":18},"title":"","narrative":""},"ut.tensors.Tensor_Operation_Spec":{"executedFeatures":["Activation functions work across types on slices and non sliced tensors.","Auto reshaping and broadcasting works and the result can be back propagated.","New method \"asFunction\" of String added at runtime is callable by groovy and also works.","New operator methods added to \"SDK-types\" at runtime are callable by groovy and also work.","Operators \"+,*,**\" produce expected results with gradients which can be accessed via a \"Ig[0]\" Function instance","Overloaded operation methods on tensors produce expected results when called.","Simple slice addition produces expected result.","The \"dot\" operation reshapes and produces valid \"x\" operation result.","The \"matMul\" operation produces the expected result.","The \"random\" function/operation populates tensors randomly.","The values of a randomly populated tensor seems to adhere to a gaussian distribution."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":11,"totalFeatures":11,"passed":11,"successRate":1.0,"time":12021},"title":"Running Tensors through operations","narrative":"This specification covers the interaction \n    between tensors and operations, more specifically it\n    runs tensors through operations and validates that the results are valid."},"it.Cross_Device_Spec":{"executedFeatures":["A gradient of ones can be set by calling the backward method on a tensor sitting on any device.","Convolution can model matrix multiplications across devices.","Mapping tensors works for every device (even if they are not used).","Test cross device system test runs successfully.","Test simple NN implementation with manual backprop"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":25284},"title":"Cross Device Stress Test Specification","narrative":"This specification is pretty much a system test which covers\n    the behavior of the library as a whole across multiple devices!\n    No matter which device is being used for a given stress test, the result should be the same..."},"ut.autograd.Autograd_Explained":{"executedFeatures":["Simple automatic differentiation and propagation."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":17},"title":"Autograd - Automatic Differentiation","narrative":"Central to all neural networks in Neureka is the autograd package.                                      \n    The autograd package provides automatic differentiation for all default operations on Tensors.          \n    Neureka is a define-by-run library, which means that your backpropagation is defined by how             \n    your code is run, and that every single iteration can be different.                                     \n                                                                                                            \n    The class neureka.Tsr is the central class of the main package.                                         \n    If you set its attribute 'rqsGradient' to True, Neureka starts to track all operations on it.           \n    When you finish the forward pass of your network                                                        \n    you can call .backward() and have all the gradients computed                                            \n    and distributed to the tensors requiring them automatically.                                            \n                                                                                                            \n    The gradient for a tensor will be accumulated into a child tensor (component) which                     \n    can be accessed via the '.getGradient()' method.                                                        \n                                                                                                            \n    To stop a tensor from tracking history, you can call '.detach()' to detach it from the                  \n    computation history, and to prevent future computation from being tracked."},"ut.autograd.internal.GraphNode_Tensor_Exception_Unit_Tests":{"executedFeatures":["A tensor cannot be deleted if it is part of a graph and the tensor is used as derivative."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":551},"title":"","narrative":""},"ut.backend.core.Backend_Algorithm_Implementation_Spec":{"executedFeatures":["Activation implementations have expected Executor instances.","CLExecutors of Operator implementations behave as expected.","HostExecutors of Operator implementations behave as expected.","Operator implementations have expected Executor instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":315},"title":"","narrative":""},"ut.calculus.Calculus_Parsing_Spec":{"executedFeatures":["Functions can derive themselves according to the provided index of the input which ought to be derived.","Parsed equations throw expected error messages.","Test parsed equations when building Function instances."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":50},"title":"Parsing Expressions into Functions","narrative":"Neureka uses the 'Function' interface as a representation of a\n    nested structure of operations.\n    This means that a 'Function' is simply an abstract syntax trees made up of other 'Function' implementations\n    which are assembled together by a parser receiving a string expression.\n    In this specification we ensure that function expressions will be properly parsed into\n    'Function' implementations."},"ut.calculus.ConCat_Spec":{"executedFeatures":["We can concatenate 2 float tensors alongside a specified axis!","We can concatenate 2 string tensors alongside a specified axis!","We can concatenate 2 tensors alongside a specified axis!","We can concatenate and then back-propagate 2 simple float tensors alongside a specified axis!","We can concatenate and then back-propagate 3 simple float tensors alongside a specified axis!"],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":66},"title":"Merging Tensors","narrative":"Tensors can not only be sliced, but also merged.\n    This is most easily achieved through the concatenation operation, \n    which stacks 2 tensors alongside a specified axis.\n    This specification not only covers how you can concatenate tensors,\n    but also how this works alongside autograd and non-numeric tensors."},"ut.device.CLFunctionCompiler_Spec":{"executedFeatures":["The OpenCLDevice produces a working optimized Function (internally using the CLFunctionCompiler)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":1,"totalFeatures":1,"passed":1,"successRate":1.0,"time":133},"title":"OpenCLDevice Function Optimization Integration Tests","narrative":""},"ut.device.OpenCL_Spec":{"executedFeatures":["A given OpenCL context can be disposed!","An OpenCLDevice will throw an exception when trying to add a tensor whose \"data parent\" is not outsourced.","First found OpenCLDevice will have realistic numeric properties.","First found OpenCLDevice will have realistic properties inside summary query.","First found OpenCLDevice will have realistic text properties."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":5,"totalFeatures":5,"passed":5,"successRate":1.0,"time":2569},"title":"Working with OpenCL","narrative":"Neureka models the OpenCL API through various types of classes.\n    The most fundamental of these is the OpenCLDevice class which\n    represents a single OpenCL device.\n    Besides that, there is also the OpenCLContext class which\n    represents a OpenCL context, representing OpenCL platforms and multiple devices..."},"ut.tensors.exceptions.Tensor_Exception_Spec":{"executedFeatures":["Building a tensor with \"null\" as shape argument throws an exception.","Building a tensor with 0 shape arguments throws an exception.","Casting a tensor as something unusual will cuas an exception to be thrown.","Out of dimension bound causes descriptive exception!","Passing an invalid key object into the \"getAt\" method causes a descriptive exception.","Passing an invalid object into Tsr constructor causes descriptive exception.","Passing null to various methods of the tensor API will throw exceptions.","Trying to inject an empty tensor into another causes fitting exception."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":8,"totalFeatures":8,"passed":8,"successRate":1.0,"time":43},"title":"Tensors Exception Behavior","narrative":"This specification covers the behavior of the $Tsr class in\n    exceptional scenarios which are contrary to its intended use.\n    The purpose of this is to assert that the $Tsr class will provide\n    useful feedback to a user to explain that a misuse of its API\n    occurred so that the user can correct this misuse."},"ut.tensors.Fluent_Tensor_Creation_Spec":{"executedFeatures":["Initialization lambda based tensors can be created fluently.","Range based tensors can be created fluently.","Scalars can be created fluently.","Seed based tensors can be created fluently.","Tensors can be created fluently.","Value based tensors can be created fluently.","Vectors can be created fluently."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":7,"totalFeatures":7,"passed":7,"successRate":1.0,"time":53},"title":"","narrative":""},"ut.tensors.Tensor_Gradient_Spec":{"executedFeatures":["Gradient of tensor is being applies regardless of the tensor requiring gradient or not","Tensors can have gradients but not require them.","Tensors that have gradients but do not require them still print them."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":3,"totalFeatures":3,"passed":3,"successRate":1.0,"time":9},"title":"Gradients are Tensors which are Components of other Tensors","narrative":"This specification defines the gradient API on tensors.\n    So one ought to be able to check wetter or not a tensor has a gradient attached to it or not.\n    In that case one should be able to get this gradient and then work with\n    it independently of the original tensor to which it belongs to..."},"ut.tensors.Tensor_Version_Spec":{"executedFeatures":["Inline operations cause illegal state exceptions.","Inline operations causes version incrementation.","Non-inline operations do not cause version incrementation.","Storing a tensor on a device should not change the version of a tensor (Even though its data changed technically)."],"ignoredFeatures":[],"stats":{"failures":0,"errors":0,"skipped":0,"totalRuns":4,"totalFeatures":4,"passed":4,"successRate":1.0,"time":244},"title":"Tensor (Data Array) Version","narrative":"There are two fundamental categories of operations\n    which can be applied to tensors : \n    Inline operations and Non-Inline  operations! \n    \n    Inline operations are often times problematic because they produce\n    side effects by changing passed tensors instead of producing new ones... \n    One such bad side effect can easily occur for tensors involved in the\n    autograd system, more specifically: the recorded computation graph. \n    Inline operations can break the mathematically pureness of the back-propagation\n    procedure by for example changing partial derivatives... <br>\n    In order to prevent said errors to occur unnoticed tensors\n    have versions which will increment when the underlying data of the tensor changes. \n    This version will be tracked by the computation graph as well in order to\n    match it with the ones stored inside the tensor. \n    A mismatch would then yield an exception! \n    \n    This specification is responsible for defining the behaviour of this\n    version number with respect to their wrapping tensors as well as computation graph nodes."}}